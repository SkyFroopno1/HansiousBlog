[{"id":"f73a8e23e6f6f669cf99c7dba8fa0722","title":"","content":"一、常用的Linux命令\n\n\n\n\n\n\n\n\nGit Bash 、Git CMD 、Git GUI的区别：\nGit Bash是git的命令行界面，使用的是Linux命令\nGit CMD是git的命令行界面，使用的是Windows的CMD命令\nGit GUI是git的图形画界面，不建议使用\n\n\n\n\n\n\n\n\n\nGit Bash 常用的Linux命令：\n\ncd   改变目录。\ncd ..   回退到上一个目录，直接cd进入默认目录\npwd   显示当前所在的目录路径。\nIs或者ll   列出当前目录中的所有文件，只不过Il(两个ll)列出的内容更为详细。\ntouch   新建一个文件如touch index.js就会在当前目录下新建一个index.js文件。\n rm  删除一个文件, rm index.js就会把index.js文件删除。\nmkdir   新建一个目录,就是新建一个文件夹。\n rm -r   删除一个文件夹, rm -r src删除src目录\nrm -rf / ： 格式化系统，切勿在Linux中尝试！\nmv   移动文件, mv index.html src index.html是我们要移动的文件, src是目标文件夹,当然,这样写夹在同一目录下。\nreset  重新初始化终端/清屏。【不咋用】\n clear  清屏。\nhistory  查看命令历史。\n exit 退出。\n #表示注释\n\n二、Git 的必要配置所有的配置文件都保存在本地:\n1） Git\\etc\\gitconfig : Git安装目录下的gitconfig，系统级\n2）  C:\\Users\\用户名\\gitconfig 只适用于当前登录用户查看\n查看全部配置：\ngit config -l\n\n查看系统配置:\ngit config --system --list\n\n查看当前用户：\ngit config --global --list\n\n\n\n*必做：设置用户名与邮箱：git config --global user.name \"用户名\"\ngit config --global user.email \"邮箱\"\n\n这个邮箱需要保证是真实邮箱，后面有需要\n三、Git的工作原理3.1 配置环境变量将 目录\\Git\\cmd 配置进去\n3.2 Git基本理论(核心)工作区域Git本地有三个工作区域:\n\n工作目录（Working Directory )\n\n暂存区(Stage/Index)\n\n资源库(Repository或Git Directory)\n\n\n如果在加上远程的git仓库(Remote Directory)就可以分为四个工作区域。文件在这四个区域之间的转换关系如下:\n\n\nWorkspace :工作区，就是你平时存放项目代码的地方\nIndex / Stage:暂存区，用于临时存放你的改动，事实上它只是一个文件，保存即将提交到文件列表信息\nRepository:仓库区（或本地仓库），就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本\nRemote :远程仓库，托管代码的服务器，可以简单的认为是你项目组中的一台电脑用于远程数据交换\n\n\n工作流程\n在工作目录中添加、修改文件;\n将需要进行版本管理的文件放入暂存区域;    git add .\n将暂存区域的文件提交到git仓库。    git commit\n\ngit管理的文件有三种状态︰已修改( modified ) ,已暂存( staged ) ,已提交(committed)\n3.3 Git项目搭建3.3.1创建工作目录与常用指令工作目录(WorkSpace)一般就是你希望Git帮助你管理的文件夹，可以是你项目的目录，也可以是一个空目录，建议不要有中文。\n\n3.3.2 本地仓库搭建 及 克隆远程仓库# 初始化方法一，完成后在本目录多了一个 .git隐藏文件\ngit init \n\n# 初始化方法二：克隆远程仓库\ngit clone [URL]\n\n\n\n\n\n四、Git文件操作4.1 文件的状态版本控制就是对文件的版本控制，要对文件进行修改、提交等操作，首先要知道文件当前在什么状态，不然可能会提交了现在还不想提交的文件，或者要提交的文件没提交上。\n\nUntracked:未跟踪,此文件在文件夹中，但并没有加入到git库,不参与版本控制.通过git add状态变为staged\nUnmodify:文件已经入库,未修改,即版本库中的文件快照内容与文件夹中完全一致.这种类型的文件有两种去处,如果它被修改,而变为Modified.如果使用git rm移出版本库,则成为Untracked文件\nModified:文件已修改,仅仅是修改,并没有进行其他的操作.这个文件也有两个去处,通过git add可进入暂存staged状态,使用git checkout则丢弃修改过,返回到_unmodify状态,这个 git checkout即从库中取出文件,覆盖当前修改!\nStaged:暂存状态.执行git commit则将修改同步到库中,这时库中的文件和本地文件又变为一致,文件为unmodify状态.执行git reset HEAD filename取消暂存，文件状态为Modified\n\n查看文件状态：\ngit status\n\n提交到本地仓库：\ngit add . # 提交所有文件到暂存区\ngit commit -m \"\" # 提交到本地仓库，-m是提交附带的信息\n\n\n\n4.2 忽略文件有些时候我们不想把某些文件纳入版本控制中，比如数据库文件，临时文件，设计文件等在主目录下建立”.gitignore“文件，此文件有如下规则∶\n\n忽略文件中的空行或以井号(#）开始的行将会被忽略。\n可以使用Linux通配符。例如∶星号(*)代表任意多个字符，问号( ? )代表一个字符，方括号( [abc])代表可选字符范围，大括号( {string1,string2..…})代表可选的字符串等。\n如果名称的最前面有一个感叹号( !)，表示例外规则，将不被忽略。\n如果名称的最前面是一个路径分隔符(/ )，表示要忽略的文件在此目录下，而子目录中的文件不忽略。\n如果名称的最后面是一个路径分隔符(/)，表示要忽略的是此目录下该名称的子目录，而非文件（默认文件或目录都忽略)。\n\n*.txt # 忽略所有 .txt结尾的文件，这样的话上传就不会被选中！\n!lib.txt # 但lib.txt除外\n/temp  # 仅忽略项目根目录下的TODO文件，不包括其他目录temp\nbuild/ # 忽略build/ 目录下的所有文件\ndoc/*.txt # 会忽略 doc/motes.txt 但不会忽略 doc/server/arch.txt\n\n\n\n五、使用Gitee或者GitHub等远程仓库\n\n\n\n\n\n\n\n\ngithub是有墙的，比较慢，在国内的话，我们一般使用gitee ，公司中有时候会搭建自己的gitlab服务器\n\n登录/注册，完善个人信息\n设置本机绑定SSH公钥，实现免密码登录 【重点】\n\n# 进入 C:\\Users\\用户名\\.ssh\n# 生成SSH公钥\nssh-keygen\n# 默认 rsa加密算法， ssh-keygen -t rsa\n# 完成后生成两个文件：id_rsa 、 id_rsa.pub\n# 其中 .pub是公钥，另一个是私钥\n\n3. 将公钥信息public key添加到 平台账户 中即可\n\n4. 使用平台创建一个自己的仓库\n\n![image-20210502183157859](C:\\Users\\HP\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210502183157859.png)\n\n5. 克隆到本地\n\n\n\n\n\n\n六、Git分支以上都是单人操作，但接下来的多人操作需要分支：\n分支在GIT中相对较难，分支就是科幻电影里面的平行宇宙，如果两个平行宇宙互不干扰，那对现在的你也没啥影响。不过在某个时间点，两个平行宇宙合并了，我们就需要处理一些问题了!\n\n\nGit中常用的分支命令# 列出所有本地分支\ngit branch\n\n# 列出所有的远程分支，并切换到该分支\ngit branch [分支名]\n\n# 合并指定分支到当前分支\ngit merge [branch]\n\n# 删除分支\ngit branch -d [分支名]\n\n# 删除源程分支\ngit push origin --delete [分支名]\ngit branch -dr [remote/branch]\n\n如果同一个文件在合并分支时都被修改了则会引起冲突︰解决的办法是我们可以修改冲突文件后重新提交!选择要保留他的代码还是你的代码!\nmaster主分支应该非常稳定，用来发布新版本，一般情况下不允许在上面工作，工作一般情况下在新建的dev分支上工作，工作完后，比如上要发布，或者说dev分支代码稳定后可以合并到主分支master上来。\nGit分支模型![20180624174835949](D:\\QQ doc\\FileRecv\\20180624174835949.png)\n创建仓库时，在master分支版本号0.1\n拉取分支develop做主开发分支，从develop分支拉取多个feature特征分支\n在feature分支发现的bug就在feature中修改，特征分支开发并bug修复完毕后，合并至develop分支\n随后将develop分支拉取出环境测试分支，即release预发布分支，测试修复问题后，合并至develop分支，同时可以发布面向客户的内容至master分支，此时版本号命名为1.0\n如果发现master分支出现bug，则拉取hotfixes分支进行修复后再发布至master分支，版本号更名为1.1\n过程向后以此类推。\n如果版本号为三位版本号，一般来说第一位的变更意味着大版本的变化，可能不兼容上一版本\n[IDEA集成Git]\n新建项目\n方式一：项目的目录就是本地git仓库的目录\n方式二：把本地仓库的.git等信息拷贝到工程目录即可\n\n\n修改文件，使用IDEA操作git\n添加到暂存区\ngit commit\ngit push\n\n\n\n[问题解决]1. 添加公钥后仍然需要输入帐号密码push问题原因：git clone远程仓库的时候使用的Http\n解决方法：用SSH来git clone远程仓库到本地，就可以解决\n2. git clone只能clone远程库的master分支，无法clone所有分支，解决办法如下：\ngit clone http://myrepo.xxx.com/project/.git ,这样在git_work目录下得到一个project子目录\ncd project\ngit branch -a，列出所有分支名称如下： remotes/origin/dev remotes/origin/release\ngit checkout -b dev origin/dev，作用是checkout远程的dev分支，在本地起名为dev分支，并切换到本地的dev分支\ngit checkout -b release origin/release，作用参见上一步解释\ngit checkout dev，切换回dev分支，并开始开发。\n\n","slug":"Git","date":"2022-05-18T13:09:12.782Z","categories_index":"","tags_index":"","author_index":"Aurora"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new \"My New Post\"\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","slug":"hello-world","date":"2022-05-18T12:20:38.788Z","categories_index":"","tags_index":"","author_index":"Aurora"},{"id":"a87d4bc564771ccc23614a55c07ae175","title":"Java 8 特性及实战","content":"一、JDK 8\nLambda表达式\n强大的Stream API\n便于并行\n最大化减少空指针异常：Optional\nNashorn引擎，允许在JVM上运行JS应用\n\n\n1.1 Lambda表达式\n\n\n\n\n\n\n\n\n例子\n//标准写法\nComparator&lt;Integer> cmp = new Comparator&lt;Integer>() &#123;\n    @Override\n    public int compare(Integer o1, Integer o2) &#123;\n        return Integer.compare(o1,o2);\n    &#125;\n&#125;;\n// Lambda表达式 \nComparator&lt;Integer> cmp1 = (o1, o2) -> Integer.compare(o1,o2);\n// 更简洁的Lambda表达式 \nComparator&lt;Integer> cmp2 = Integer::compare;\n\n\n\n\n\n\n\n\n\n\n本质\n作为函数式接口的实例 (接口实现类对象)\n函数式接口：一个接口中，只声明了一个抽象方法\n\n\n\n\n\n\n\n\n\n格式\n-&gt; Lambda操作符，或箭头操作符，左边是形参列表(其实就是接口中的抽象方法中的形参列表)；右边是Lmbda体，其实就是重写的抽象方法的方法体。\n\n语法格式一：无参，无返回值\nRunnable r = () -> System.out.println(\"无参，无返回值\");\n语法格式二：一个参数，没有返回值\nConsumer&lt;String> con = (String s) -> System.out.println(s);\n语法格式三：数据类型可以省略，因为可由编译器推断出，成为类型推断\nConsumer&lt;String> con =  (s) -> System.out.println(s);\n语法格式四：Lambda若只有一个参数，小括号可以省略\nConsumer&lt;String> con =  s -> System.out.println(s);\n语法格式五：需要两个或以上参数，多条执行语句，并且有返回值\nComparator&lt;Integer> comparator = (x,y)->&#123;\n    System.out.println(\"实现函数式接口方法\");  \n    return Integer.compare(x,y);\n&#125;;\n语法格式六：当Lambda体只有一条语句时，return和大括号可以省略\nComparator&lt;Integer> comparator = (x,y)-> Integer.compare(x,y);\n\n\n\n\n\n\n\n\n\n\n类型推断\n上述 Lambda 表达式中的参数类型都是由编译器推断得出的。Lambda 表达式中无需指定类型，程序依然可以编译，这是因为 javac 根据程序的上下文，在后台推断出了参数的类型。Lambda 表达式的类型依赖于上下文环境，是由编译器推断出来的。\n1.2 函数式接口\n只含有一个抽象方法的接口\n可以通过Lambda表达式创建该接口的对象。若Lambda表达式抛出一个受检异常(即非运行时异常)，那么该异常需要在目标接口的抽象方法上进行声明\n可以在一个接口上使用@FunctionalInterface注解，检查他是否是一个函数式接口。同时Javadoc种也会包含一条声明，说明这个接口是一个函数式接口\n使得Java不但可以支持OOP还可以支持OOF（面向函数编程）\n以前用匿名实现类表示的现在都可以用Lambda表达式来写\n\n\n\n\n\n\n\n\n\n\n\n\nJava内置的四大核心函数式接口\n\n\n\n\n\n\n\n\n\n\n其他接口\n\n1.3 方法引用和构造器引用\n当要传递给Lambda体的操作，已经有实现的方法了，就可以使用方法引用\n可以看做是Lambda表达式深层次的表达。换句话说，方法引用就是Lambda表达式，也就是函数式接口的一个实例，通过方法的名字来指向一个方法。\n实现接口的抽象方法的参数列表和返回值类型，必须与方法引用的方法的参数列表和返回值类型保持一致\n主要有如下三种使用情况\n对象::实例方法名\n类::静态方法名\n类::实例方法名\n\n\n\n\n\n\n\n\n\n\n\n\n方法引用\n\n\n\n\n\n\n\n\n==注意：当函数式接口方法的第一个参数是需要引用方法的调用者，并且第二个参数是需要引用方法的参数(或无参数)时：ClassName::methodName==\n\n\n\n\n\n\n\n\n\n构造器引用\n要求构造器参数列表要与接口中抽象方法的参数列表一致！且方法的返回值即为构造器对应类的对象。\n\n\n以此还有数组引用\n\n\n1.4 Stream API1.4.1 概述\nStream API ( java.util.stream) 把真正的函数式编程风格引入到Java中。这是目前为止对Java类库最好的补充，因为Stream API可以极大提供Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。\nStream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。 使用Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简言之，Stream API 提供了一种高效且易于使用的处理数据的方式\n实际开发中，项目中多数数据源都来自于Mysql，Oracle等。但现在数据源可以更多了，有MongDB，Radis等，而这些NoSQL的数据就需要Java层面去处理。因此需要StreamAPI\n\n\n\n\n\n\n\n\n\n\nStream和Collection集合的区别\n\nCollection是一种静态的内存数据结构，而Stream是有关计算的。\nCollection面向内存，存储在内存中。Stream面向CPU，通过CPU实现计算\n\n\n\n\n\n\n\n\n\n\n是什么\n是数据渠道，用于操作数据源（集合、数组等）所生成的元素序列。集合讲的是数据，Stream讲的是计算！\n注意：\n\nStream自己本身不会存储元素\nStream不会改变源对象。他们会返回一个持有结果的新Stream\nStream操作是延迟执行的。这意味着他们会等到需要结果的时候才执行\n\n\n\n\n\n\n\n\n\n\n使用步骤\n\n创建Stream\n一个数据源(如：集合、数组)，获取一个流\n\n中间操作\n一个中间操作链，对数据源的数据进行处理\n\n终止操作(终端操作)\n一旦执行终止操作，就执行中间操作链，并产生结果。之后不会再被使用\n\n\n\n1.4.2 创建Stream\n\n\n\n\n\n\n\n\n方式一：通过集合\n\ndefault Stream&lt;E&gt; stream()：返回一个顺序流\ndefault Stream&lt;E&gt; parallelStream()：返回一个并行流\n\n\n\n\n\n\n\n\n\n\n方式二：通过数组\n\nstatic&lt;T&gt; Stream&lt;T&gt; stream(T[] array)：返回一个流\n\n\n\n\n\n\n\n\n\n\n方式三：通过Stream的API\n\npublic static&lt;T&gt; Stream&lt;T&gt; of(T... values)：返回一个流\n\n\n\n\n\n\n\n\n\n\n方式四：创建无限流\n\npublic static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f)：迭代\npublic static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s)：生成\n\n并行流就是把一个内容分成多个数据块，并用不同的线程分别处理每个数据块的流。相比较串行的流，并行的流可以很大程度上提高程序的执行效率\nJava 8 中将并行进行了优化，我们可以很容易的对数据进行并行操作。Stream API 可以声明性地通过 parallel() 与 sequential() 在并行流与顺序流之间进行切换.\n1.4.3 中间操作\n\n\n\n\n\n\n\n\n筛选与切片\n\n\n\n\n\n\n\n\n\n\n映射\n\n\n\n\n\n\n\n\n\n\n排序\n\n1.4.4 终止操作\n\n\n\n\n\n\n\n\n匹配与查找\n\n\n\n\n\n\n\n\n\n\n\n规约\n\nmap和reduce的连接通常称为map-reduce模式,因Google用它来进行网络搜索而出名\n\n\n\n\n\n\n\n\n\n收集\n\nCollector 接口中方法的实现决定了如何对流执行收集的操作(如收集到 List、Set、Map)。\nCollectors 实用类提供了很多静态方法，可以方便地创建常见收集器实例\n\n\n1.5 Optional 类Optional 类(java.util.Optional) 是一个容器类，它可以保存类型T的值，代表这个值存在。或者仅仅保存null，表示这个值不存在。原来用 null 表示一个值不存在，现在 Optional 可以更好的表达这个概念。并且可以避免空指针异常。\n==Optional类的Javadoc描述如下：这是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。==\nOptional提供很多有用的方法，这样我们就不用显式进行空值检测。\n\n\n\n\n\n\n\n\n\n创建Optional类对象\n\nOptional.of(T t)：创建一个Optional实例，t必须非空\nOptional.empty()：创建一个空的Optional实例\nOptional.ofNullable(T t)：t可以为null\n\n\n\n\n\n\n\n\n\n\n判断Optional容器种是否包含对象\n\nboolean isPresent()：判断是否包含对象\nvoid ifPresent(Consumer&lt;? super T&gt; consumer)：如果有值，就执行Consumer接口的实现代码，并且该值会作为参数传给它\n\n\n\n\n\n\n\n\n\n\n获取Optional容器的对象\n\nT get()：如果调用对象包含值，返回该值，否则抛异常\nT orElse(T other)：如果有值则将其返回，否则返回指定的other对象\nT orElseGet(Supplier&lt;? extends T&gt; other)：如果有值则直接返回，否则返回由Supplier接口实现提供的对象\nT orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier)：如果有值则将其返回，否则抛出由Supplier接口实现提供的异常\n\n1.6 Collector&lt;T, A, R&gt; // 用于还原操作的输入元素的类型\n&lt;T> the type of input elements to the reduction operation\n// 中间存放数据的容器\n&lt;A> the mutable accumulation type of the reduction operation (often hidden as an implementation detail)\n// 还原操作的结果类型（输出）\n&lt;R> the result type of the reduction operation\n\nCollector通过下面四个方法协同工作以完成汇聚操作：\n\nsupplier： 创建新的结果容器\naccumulator：将输入元素合并到结果容器中\ncombiner：合并两个结果容器(非必然运行 可能在并行流且Collector不具备CONCURRENT  时执行的 ) \nfinisher：将结果容器转换成最终的表示 (非必然运行 中间结果与最终结果类型是否一致决定是否运行，IDENTITY_FINISH用来标志 ) \n\n\n\nCollector 就是归约运算操作的一种抽象 \n\n\n\n\n\n\n\n\n\n理解归约reduce的含义 \n(也就是归纳转换成另外一种形式)\n想要进行归约运算,你先给出一个初始容器,作为中间结果容器                         \n然后再给出迭代运算逻辑 也就是要如何归约 归约的逻辑 就是在这里 结果计算到中间结果容器中                         \n针对于并行计算还需要一个合并的方式 \n\n\n\n\n\n\n\n\n\n常用收集器 \n\ntoList() ：将元素收集到一个 List 中\ntoSet() ：将元素收集到一个 Set 中。             \ntoCollection() ：将元素收集到一个Collection中\ntoMap() ：将元素收集到一个Map中，依据提供的映射函数将元素转换为键值\nsummingInt(ToIntFunction&lt;? super T&gt;) ：给定值序列求和(还有long和double版本)\nreducing(…)：用于归约计算(通常用作下游收集器，比如用于groupingBy 或者 partitioningBy下游)\npartitioningBy(…) ：按照predicate分为两组\ngroupingBy(…)：将元素分组\nmaxBy(Comparator&lt;? super T&gt; comparator)：最大值\nminBy(Comparator&lt;? super T&gt; comparator)：最小值\n**mapping(Function&lt;T,U&gt; , Collector)**：将提供的映射函数应用于每个元素，并使用指定的下游收集器(通常用作下游收集器本身，比如用于groupingBy)进行处理\njoining()：假设元素为String类型，将这些元素联结到一个字符串中(或许使用分隔符、前缀和后缀)\ncounting() ：计算元素数量(通常用作下游收集器)\naveragingInt(ToIntFunction&lt;? super T&gt;) ：平均数(还有long和double版本)\n\n1.7 Function&lt;T, R&gt;\n\n“接收一个参数，返回一个值”\npublic interface Function&lt;T, R> &#123;\n    /**\n    * 将此函数应用于给定参数，真正执行函数接口的方法\n    */\n    R apply(T t);\n    \n    /**\n    * 函数链，before执行的结果做根函数为参数\n    */\n    default &lt;V> Function&lt;V, R> compose(Function&lt;? super V, ? extends T> before)&#123;\n        Objects.requireNonNull(before);\n        return (V v) -> apply(before.apply(v));\n    &#125;\n    \n    /**\n    * 函数链，根函数执行结果作为after的参数\n    */\n    default &lt;V> Function&lt;V, R> compose(Function&lt;? super R, ? extends V> after)&#123;\n        Objects.requireNonNull(after);\n        return (V v) -> apply(after.apply(v));\n    &#125;\n    \n    /**\n    * 返回一个参数作为返回值的函数\n    */\n    static &lt;T> Function&lt;T,T> identity() &#123;\n        return t -> t;\n    &#125;\n&#125;\n\n\n\n\nBiFunction&lt;T,U,R&gt;：代表了一个接受两个输入参数的方法，并且返回一个结果\nDoubleFunction&lt; R&gt;：代表接受一个double值参数的方法，并且返回结果\nDoubleToIntFunction：接受一个double类型输入，返回一个int类型结果。\nDoubleToLongFunction：接受一个double类型输入，返回一个long类型结果\nIntFunction&lt; R&gt;:接受一个int类型输入参数，返回一个结果 。\nIntToDoubleFunction：接受一个int类型输入，返回一个double类型结果 。\nIntToLongFunction：接受一个int类型输入，返回一个long类型结果。\nLongFunction&lt; R&gt;： 接受一个long类型输入参数，返回一个结果。\nLongToDoubleFunction： 接受一个long类型输入，返回一个double类型结果。\nLongToIntFunction：接受一个long类型输入，返回一个int类型结果。\nToDoubleBiFunction&lt;T,U&gt;：接受两个输入参数，返回一个double类型结果\nToDoubleFunction&lt; T&gt;：接受一个输入参数，返回一个double类型结果\nToIntBiFunction&lt;T,U&gt;：接受两个输入参数，返回一个int类型结果。\nToIntFunction&lt; T&gt;：接受一个输入参数，返回一个int类型结果。\nToLongBiFunction&lt;T,U&gt;：接受两个输入参数，返回一个long类型结果。\nToLongFunction&lt; T&gt;：接受一个输入参数，返回一个long类型结果。\n\n二、实战应用2.1 收集器(Collectors)// 获取所有的name转换到List&lt;String>中\nList&lt;String> list =people.stream()\n    .map(Person::getName).collect(Collectors.toList());\n\n// 获取所有的name转换到Set&lt;String>中\nSet&lt;String> set =people.stream()\n  .map(Person::getName).collect(Collectors.toCollection(TreeSet::new));\n\n// 元素转换为String 并且将他们通过\", \" 连接起来\nString joined = things.stream()\n    .map(Object::toString)\n    .collect(Collectors.joining(\", \"));\n\n//计算员工薪水之和\nint total = employees.stream()\n    .collect(Collectors.summingInt(Employee::getSalary)));\n\n// 按照部门对员工进行分组\nMap&lt;Department, List&lt;Employee>> byDept\n    = employees.stream()\n    .collect(Collectors.groupingBy(Employee::getDepartment));\n\n// 计算部门薪资和\nMap&lt;Department, Integer> totalByDept\n    = employees.stream()\n    .collect(Collectors.groupingBy(Employee::getDepartment,\n                                   Collectors.summingInt(Employee::getSalary)));\n\n// 按照成绩是否通过把学生分为两组\nMap&lt;Boolean, List&lt;Student>> passingFailing =\n    students.stream()\n    .collect(Collectors.partitioningBy(s -> s.getGrade() >= PASS_THRESHOLD));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Java 8新版本特性","date":"2022-05-14T16:00:00.000Z","categories_index":"","tags_index":"Java","author_index":"Aurora"},{"id":"0370bef7a3b384109aec67b7f5d61a9d","title":"MySQL高级","content":"零、前言@Author：韩霄杰(Hansious | SkyFroop)\n@Date: 20220209\n@Update: 20220323 | 20220401\n@Description: 本篇专注于MySQL DBMS的分析。若需要强化SQL参考《SQL》笔记。\n[toc]\n一、MySQL架构1.1 MySQL逻辑架构\n1.连接层\n最上层是一些客户端和连接服务，包含本地sock通信和大多数基于客户端/服务端工具实现的类似于tcpip的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。\n⒉服务层\n第二层架构主要完成大多少的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化及部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定查询表的顺序，是否利用索引等，最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存。如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。\n3.引擎层\n存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。后面介绍MyISAM和InnoDB\n4.存储层\n1.2  存储引擎\n\n\n\n\n\n\n\n\n查看本机的存储引擎\n\n\n\n\n\n\n\n\n\n\nMyISAM和InnoDB的区别\n\n二、索引优化2.1 SQL解析\n2.2 索引MySQL官方对索引的定义为:索引(Index）是帮助MySQL高效获取数据的数据结构。数据本身之外，数据库还维护着一个满足特定查找算法的数据结构(B树)，这些数据结构以某种方式指向数据，这样就可以在这些数据结构的基础上实现高级查找算法，这种数据结构就是索引。\n2.2.1 索引数据结构概述==可以得到索引的本质:索引是数据结构。==\n可以简单的理解为“排好序的快速查找数据结构” —&gt; 索引的功能：排序，快速查找\n在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用(指向)数据,这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。\n\n左边是数据表，-共有两列七条记录，最左边的是数据记录的物理地址\n为了加快Col2的查找，可以维护-一个右边所示的二叉查找树，每个节点分别包含索引键值和–个指向对应数据记录物理地止的指针，这样就可以运用二叉查找在一定的复杂度内获取到相应数据，从而快速的检索出符合条件的记录。\n一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上\n\n\n\n\n\n\n\n\n\n我们平常所说的索引，如果没有特别指明，都是指B树(多路搜索树，并不-一定是二叉的)结构组织的索引。其中聚集索引，次要索引，覆盖索引，复合索引，前缀索引，唯一索引默认都是使用B+树索引，统称索引。当然，除了B+树这种类型的索引之外，还有哈稀索引等\n2.2.2 优缺点优点：\n\n提高数据检索的效率，降低数据库的IO成本；\n通过索引对数据进行排序，降低数据排序的成本，降低了CPU的消耗\n\n缺点：\n\n实际上索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录，所以索引列也是要占用空间的\n虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息\n\n索引只是提高效率的一个因素，如果你的MySQL有大数据量的表，就需要花时间研究建立最优秀的索引，\n2.2.3 索引分类和索引结构\n唯一索引：索引列的值必须唯一，但允许有空值\n聚集索引(聚簇索引；主键索引)：数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引。\n一个没加主键的表，它的数据无序的放置在磁盘存储器上\n如果给表上了主键，那么表在磁盘上的存储结构就由整齐排列的结构转变成了树状结构，也就是平衡树结构，换句话说，就是整个表就变成了一个索引，也就是所谓的聚集索引。\n\n\n非聚集索引(常规索引)：该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同，一个表中可以拥有多个非聚集索引。\n\n\n\n\n\n\n\n\n\n\n聚集索引和非聚集索引的主要区别\n\n通过聚集索引可以一次查到需要查找的数据， 而通过非聚集索引第一次只能查到记录对应的主键值 ， 再使用主键的值通过聚集索引查找到需要的数据。\n聚集索引一张表只能有一个，而非聚集索引一张表可以有多个。\n\n\n索引结构：\n\nBtree索引\nHash索引\nFull-test全文索引\nR-Tree索引\n\n\n\n\n\n\n\n\n\n\n 何时创建索引\n\n主键自动建立唯一索引\n频繁作为查询条件的字段应该创建索引\n查询中与其它表关联的字段，外键关系建立索引\nWhere条件里用不到的字段不创建索引\n单键/组合索引的选择问题，\n查询中排序的字段，排序字段若通过索引去访问将大大提高排序速度\n查询中统计或者分组的字段\n\n\n\n\n\n\n\n\n\n\n何时不能创建索引\n\n表记录太少(三百万以下)\n频繁更新的字段不适合创建索引，因为每次更新不单单是更新了记录还会更新索引\n数据重复且分布平均的表字段，因此应该只为最经常查询和最经常排序的数据列建立索引。注意，如果某个数据列包含许多重复的内容，为它建立索引就没有太大的实际效果\n\n​    假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率天约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。​    索引的选择性是指索引列中不同值的数目与表中记录数的比。如果一个表中有2000条记录，表索引列有1980个不同的值，那么这个索引的选择性就是1980/2000=0.99。一个索引的选择性越接近于1，这个索引的效率就越高。\n2.3 性能分析MySQL常见瓶颈：\n\nCPU:CPU在饱和的时候一般发生在数据装入内存或从磁盘上读取数据时候\nIO:磁盘I/O瓶颈发生在装入数据远大于内存容量的时候\n服务器硬件的性能瓶颈:top,free, iostat和vmstat来查看系统的性能状态\n\n2.3.1 Explain使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析查询语句或是表结构的性能瓶颈\n\n\n\n\n\n\n\n\n\n能做什么\n\n表的读取顺序\n数据读取操作的操作类型\n哪些索引可以使用\n哪些索引被实际使用\n表之间的引用\n每张表有多少行被优化器查询\n\n\n\n\n\n\n\n\n\n\n用法\nEXPLAIN + SQL语句\n可以查出如下信息\n\n各字段解释：\n\nid：\n​    id相同：\n\n\n执行顺序由上至下\nid不同：\n\n\n如果是子查询，那么id的序号会递增，id值越大，优先级越高，越先被执行\nid有的相同有的不同\n\n\nid如果相同，可以认为是一组，从上往下顺序执行;在所有组中，id值越大，优先级越高，越先执行\n\nDERIVED = 衍生，derived2就是id为2的那个衍生表\n\n\n\n\nselect_type\n\n\n\nSIMPLE：简单的select查询,查询中不包含子查询或者UNION\nPRIMARY：查询中若包含任何复杂的子部分,最外层查询则被标记为\nSUBQUERY：在SELECT或WHERE列表中包含了子查询\nDERIVED：在FROM列表中包含的子查询被标记为DERIVED(衍生)，MySQL会递归执行这些子查询,把结果放在临时表里。\nUNION：若第二个SELECT出现在UNION之后,则被标记为UNION，若UNION包含在FROM子句的子查询中,外层SELECT将被标记为:DERIVED\nUNION RESULT：从UNION表获取结果的SELECT\n\n\ntable：数据来自于哪张表\n\ntype：显示查询使用了哪种类型\n\n\n从最好到最差：system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null&gt; unique_subquert &gt; index_subquert &gt; range &gt; index &gt; ALL\n==常用：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL，一般来说，得保证查询至少达到range级别，最好ref级别==\nsystem：表只有一行记录（等于系统表)，这是const类型的特列，平时不会出现，这个也可以忽略不计\nconstant：表示通过索引一次就找到了,const用于比较primary key或者unique索引。因为只匹配一行数据，所以很悦如将主键置于where列表中，MySQL就能将该查询转换为一个常量\neq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键或唯一索引扫描\nref：非唯一性索引扫描,返回匹配某个单独值的所有行，本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而，它可能会找到多个符合条件的行，所以他应该属于查找和扫描的混合体\nrange：只检索给定范围的行,使用一个索引来选择行。key 列显示使用了哪个索引一般就是在你的where语句中出现了between、&lt;、&gt;、in等的查询，这种范围扫描索引扫描比全表扫描要好，因为它只需要开始于索引的某一点，而结束语另一点，不用扫描全部\nindex：Full Index Scan，index与ALL区别为index类型只遍历索引树。这通常比ALL快，因为索引文件通常比数据文件小（也就是说虽然all和Index都是读全表，但index是从索引中读取的，而all是从硬盘中读的)\nALL：Full Table Scan，将遍历全表以找到匹配的行\n\n\npossible_keys：显示可能应用在这张表中的索引，一个或多个。查询涉及到的字段上若存在索引，则该索引将被列出，==但不一定被查询实际使用==，即可能使用到的索引\n\nkey：实际使用的索引。如果为NULL，则没有使用索引。查询中若使用了覆盖索引，则该索引和查询的select字段重叠\n\nkey_len：表示索引中使用的字节数，可炬过该列计算查询中使用的索引的长度。在不损失精确性的情况下，长度越短越好；key_len显示的值为索引字段的最大可能长度，==并非实际使用长度==，即key_len是根据表定义计算而得，不是通过表内检索出的\n\nref：显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值\n\nrows：根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数\n\nExtra：包含不适合在其他列中显示但十分重要的额外信息\n\nUsing filesort：说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。MySQL中无法利用索引完成的排序操作称为”文件排序”\n\nUsing temporary：使了用临时表保存中间结果,My5QL在对查询结果排序时使用临时表。常见于排序order by和分组查询group by\n\nUsing index：表示相应的select操作中使用了覆盖索引(Covering Index)，避免访问了表的数据行，效率不错！如果同时出现using where，表明索引被用来执行索引键值的查找;如果没有同时出现using where，表明索引用来读取数据而非执行查找动作。\n\n覆盖索引(索引覆盖)：\n理解方式一:就是select的数据列只用从索引中就能够取得，不必读取数据行，MySQL可以利用索引返回select列表中的字段，而不必根据索引再次读取数据文件,换句话说查询列要被所建的索引覆盖。\n理解方式二:索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据;当能通过读取索引就可以得到想要的数据，那就不需要读取行了。一个索引包含了(或覆盖了)满足查询结果的数据就叫做覆盖索引。\n\n\n\nUsing Where：表明使用了Where过滤\n\nUsing Join buffer：使用了连接缓存\n\nimpossible where：Where子句的值总是false，不能用来获取任何元组\n\nselect tables optimized away：在没有GROUPBY子句的情况下，基于索引优化MIN/MAX操作或者对于MyISAM存储引擎优化COUNT(*)操作,不必等到执行阶段再进行计算,查询执行计划生成的阶段即完成优化。\n\ndistinct：优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作。\n\n\n\n\n\n2.4 单表案例案例查询语句：SELECT id ,author_id FROM article WHERE category_id = 1 AND comments &gt; 1 ORDER BY views DESC LIMIT 1;\n\n\n\n\n\n\n\n\n\n无索引，EXPLAIN：\n\ntype为ALL，全表搜索，Extra中出现Using filesort运行时排序。效率极差\n\n\n\n\n\n\n\n\n\n建立索引：INDEX(category_id , comments , views)\n\ntype从ALL优化到了range，但这样设置索引后仍然在Extra中发现了Using filesort\n如果把comments &gt; 1改为 comments = 1，则Using filesort消失\n==这是因为“大于“属于range，他以及其后的索引则会失效==\n\n\n\n\n\n\n\n\n\n建立索引：INDEX(category_id , views)\n\ntype从range优化为了ref，Using filesort也消失\n2.5 两表案例结论：\n\n左连接时，为右表添加索引。这是由左连接特性决定的。LEFT JOIN条件用于确定如何从右表搜索行,左边一定都有，所以右边是我们的关键点，一定需要建立索引\n右连接时，与左连接情况相反\n\n2.6 三表案例类比两表案例情况\n\n尽可能的减少Join 语句中NestedLoop的循环总次数；“永远用小结果集驱动大的结果集”。\n优先优化NestedLoop的内层循环\n保证Join语句中被驱动表上Join条件字段已经被索引\n当无法保证被驱动表的Join条件字段被索引且内存资源充足的前提下，不要太吝惜JoinBuffer的设置\n\n2.7 索引失效\n尽可能的使用全值匹配\n\n最佳左前缀法则：如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列. (左边大哥不能死，中间兄弟不能断)\n\n不在索引列上做任何操作(计算、函数、(自动or手动)类型转换)，会导致索引失效而转向全表扫描\n\n存储引擎不能使用索引中范围条件右边的列\n\n尽量使用覆盖索引(只访问索引的查询(索引列和查询列一致))，减少SELECT *\n\nMySQL在使用不等于(!= 或者&lt;&gt;)时无法使用索引，会导致全表扫描\n\nis null，is not null 也无法使用索引\n\nlike以通配符开头(‘%abc…’)mysql索引失效会变成全表扫描的操作\n\n\nLike最好是在右边加 百分号(%)\n解决like %字符串% 索引失效的方法：覆盖索引\n\n\n字符串不加单引号索引失效：根本原因是，MySQL底层会隐式的进行类型转换，见第3点\n\n 少用or，用它来连接会索引失效\n\n\n\n\n排序字段已经是一个常量时，order by的索引乱序可以不用using filesort\n\n定值、范围还是排序，一般order by是给个范围group by基本上都需要进行排序，会有临时表产生\n\n\n2.8 一般性建议\n对于单键索引，尽量选择针对当前query过滤性更好的索引、\n在选择组合索引的时候，当前Query中过滤性最好的字段在索引字段顺序中，位置越靠左越好。\n在选择组合索引的时候，尽量选择可以能够包含当前query中的where字句中更多字段的索引\n尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的\n\n三、查询截取优化3.1 查询优化3.1.1 永远小表驱动大表即小的数据集驱动大的数据集\n原理：\n\nEXISTS：SELECT … FROM table WHERE EXISTS (subquery)\n该语法可以理解为：将主查询的数据，放到子查询中做条件验证，根据验证结果(TRUE或FALSE)来决定主查询的数据结果是否得以保留。\n\n\n提示：\nEXISTS(subquery)只返回TRUE或FALSE，因此子查询中的SELECT *也可以是SELECT 1或其他(SELECT ‘X’)，官方说法是实际执行时会忽略SELECT清单，因此没有区别\nEXISTS 子查询的实际执行过程可能经过了优化而不是我们理解上的逐条对比，如果担心效率问题，可进行实际校验以确定是否有效率问题\nEXISTS子查询往往也可以用条件表达式、其他子查询或者JOIN来题代，何种最优需要具体问题具体分析。\n\n\n\n\n当B表的数据集必须小于A表的数据集时，用in优于exists\n\n当A表的数据集必须小于B表的数据集时，用exists优于in\nA表和B表的id都应当建立索引\n3.1.2 Order By优化\n\n\n\n\n\n\n\n\n1、 ORDER BY 子句，尽量使用Index方式排序，避免使用FileSort方式排序\n\n默认是升序排序。如果出现ASC则FileSort\n均为DESC则不会导致索引失效\n\n\nOrder By覆盖索引，且最左法则，则Index。\nORDER BY语句使用索引最左前列\n使用where子句与Order BY子句条件列组合满足索引最左前列\n\n\n\nMySQL支持二种方式的排序,FileSort和lIndex，Index效率高.它指MySQL扫描索引本身完成排序。FileSort方式效率较低。\n\n\n\n\n\n\n\n\n\n2、 尽可能在索引列上完成排序操作，遵照索引建的最佳左前缀\n\n\n\n\n\n\n\n\n\n3、 如果不在索引列上，FileSort有两种算法：MySQL就要启动双路排序和单路排序\n\n双路排序：MySQL 4.1 之前使用的是双路排序，字面意思就是两次扫描磁盘，最终得到数据，读取行指针和order by列，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出\n\n取一批数据，要对磁盘进行了两次扫描，众所周知，TO是很耗时的，所以在mysq/4.1之后，出现了第二种改进的算法，就是单路排序。\n\n单路排序：从磁盘读取查询需要的所有列，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出，它的效率更快一些，避免了第二次读取数据。并且把随机I0变成了顺序I0,但是它会使用更多的空间，因为它把每一行都保存在内存中了。\n\n单路总体而言好于双路，但是使用单路会出现一个问题：\n在sort_buffer中，方法B比方法A要多占用很多空间，因为方法B是把所有字段都取出,所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取sort_buffer容量大小的数据，进行排序（创建tmp文件，多路合并），排完再取取sort_buffer容量大小,再排…..从而多次IO。\n本来想省一次I/O操作,反而导致了大量的I/O操作，反而得不偿失。\n\n\n\n\n\n\n\n\n\n优化策略\n\n增大sort_buffer_size参数的设置\n不管用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程的\n\n\n增大max_length_for_sort_data参数的设置\n提高这个参数，会增加用改进算法的概率。但是如果设的太高，数据总容量超出sort_buffer_size的概率就增大，明显症状是高的磁盘I/O活动和低的处理器使用率.\n\n\nOrder by时select*是一个大忌只Query需要的字段，这点非常重要。在这里的影响是:\n当Query的字段大小总和小于max_length_for_sort_data而且排序字段不是TEXTIBLOB类型时，会用改进后的算法一一单路排序，否则用老算法——多路排序。\n两种算法的数据都有可能超出sort_buffer的容量，超出之后，会创建tmp文件进行合并排序，导致多次IO，但是用单路排序算法的风险会更大一些,所以要提高sort buffer size。\n\n\n\n3.1.3 Group By优化趋同于Order By的优化\n\ngroup by实质是先排序后进行分组,遵照索引建的最佳左前缀\n当无法使用索引列，增大max_length_for_sort_data参数的设置+增大sort_buffer_size参数的设置\nwhere高于having，能写在where限定的条件就不要去having限定了。\n\n3.2 慢查询日志\n\n\n\n\n\n\n\n\n是什么\n\nMySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。\n具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。long_query_time的默认值为10，意思是运行10秒以上的语句。\n\n由他来查看哪些SQL超出了我们的最大忍耐时间值，比如一条sql执行超过5秒钟，我们就算慢SQL，希望能收集超过5秒的sql，结合explain进行全面分析。\n\n\n\n\n\n\n\n\n\n使用方式\n默认情况下，MySQL数据库没有开启慢查询日志，需要我们手动来设置这个参数。如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会带来一定的性能影响。慢查询日志支持将日志记录写入文件。\n# 默认\nSHOW VARIABLES LIKE '%slow_query_log%';\n# 开启,如果MySQL重启，则会自动关闭\nSET GLOBAL slow_query_log = 1;\n# 永久生效需要修改配置文件：my.cnf\n# 查看阙值\nSHOW VARIABLES LIKE 'long_query_time%';\n# 设置慢的阙值时间：\nSET GLOBAL long_query_time = s;\n-- 修改后看不到值被修改\n-- 需要重新连接或新开一个会话才能看到修改值。\n-- SHOW VARIABLES LIKE 'long_query_time%\";\n-- show global variables like 'long_query_time';\n\n\n关于慢查询的参数slow_query_log_file，它指定慢查询日志文件的存放路径，系统默认会给一个缺省的文件host_name-slow.log\n假如运行时间正好等于long_query_time的情况，并不会被记录下来。在mysql源码里是判断大于long_query_time，而非大于等于。\n\n\n\n\n\n\n\n\n\n\n日志分析工具 mysqldumpslow\n在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具mysqldumpslow。\n\n\n\n\n\n\n3.3 批量数据脚本案例案例：查入1000w\n\n\n\n\n\n\n\n\n\n1、建库建表\n\n\n\n\n\n\n\n\n\n\n\n\n\n2、设置参数log_bin_trust_function_creators\n\n\n\n\n\n\n\n\n\n\n3、创建函数，保证每条数据不同\n\n\n\n\n\n\n\n\n\n\n 4、创建存储过程\n\n\n\n\n\n\n\n\n\n\n\n5、 调用存储过程\n\n3.4 Show Profile是mysql提供可以用来分析当前会话中语句执行的资源消耗情况。可以用于sQL的调优的测量\n默认情况下，参数处于关闭状态，并保存最近15次的运行结果\n分析步骤：\n\n是否支持，查看当前MySQL版本是否支持\nSHOW VARIABLES LIKE 'profiling';\n开启功能，默认是关闭的。\nSET profiling=on;\n运行SQL\n\n查看结果\nshow profiles;\n诊断SQL\nShow Profile cpu,block io for query (Show profiles中问题SQL数字号码，即Query_ID)\n\n\n\n\n\n\n\n\n\n\n\n\n\n应该要注意的点\n\nconverting HEAP to MyISAM查询结果太大，内存都不够用了往磁盘上搬了。\nCreating tmp table创建临时表\n拷贝数据到临时表\n用完再删除\n\n\nCopying to tmp table on disk把内存中临时表复制到磁盘，危险！！\nlocked\n\n3.5 全局查询日志==只能用于测试环境，切不可用于线上环境==\n启用方式：\n\n\n\n四、锁机制锁是计算机协调多个进程或线程并发访问某一资源的机制。\n在数据库中，除传统的计算资源（如CPU、RAM、V/O等）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。\n4.0 SELECT … FOR UPDATEFOR UPDATE 仅适用于InnoDB，且必须在事务区块(BEGIN/COMMIT)中才能生效。\n使用select…for update会把数据给锁住，MySQL InnoDB默认Row-Level Lock，所以只有「明确」地指定主键，MySQL 才会执行Row lock (只锁住被选取的数据) ，否则MySQL 将会执行Table Lock (将整个数据表单给锁住)。\n\n例1: (明确指定主键，并且有此数据，row lock)\n\nSELECT * FROM products WHERE id=’3’ FOR UPDATE;\n\n例2: (明确指定主键，若查无此数据，无lock)\n\nSELECT * FROM products WHERE id=’-1’ FOR UPDATE;\n\n例2: (无主键，table lock)\n\nSELECT * FROM products WHERE name=’Hansious’ FOR UPDATE;\n\n例3: (主键不明确，table lock)\n\nSELECT * FROM products WHERE id&lt;&gt;’3’ FOR UPDATE;\n\n例4: (主键不明确，table lock)\n\nSELECT * FROM products WHERE id LIKE ‘3’ FOR UPDATE;\n4.1 锁的分类更详细可参考[附录D](#附录D LBCC)\n\n\n\n\n\n\n\n\n\n从对数据操作的类型(读/写)分\n\n读锁(共享锁)：针对同一份数据，多个读操作可以同时进行而不会相互影响\n写锁(排他锁)：当前写操作没有完成前，他会阻断其他写锁和读锁\n\n\n\n\n\n\n\n\n\n\n从对数据操作的粒度分\n\n表锁\n行锁\n\n4.2 表锁(偏读)特点：\n\n偏向MyISAM存储引擎，开销小，加锁快;\n无死锁;\n锁定粒度大，发生锁冲突的概率最高,并发度最低\n\n\n\n\n\n\n\n\n\n\n加锁\n# 查看锁\nShow open tables;\n# 加读锁\nlock table [表名] read;\n# 加写锁\nlock table [表名] write;\n# 释放锁\nunlock tables;\n\n\n施加读锁时\n自己可以读自己的锁定表，别人可以读自己的加锁表\n自己和别人都 不可以修改加锁表，自己修改会报错，别人修改会堵塞\n自己不可以读自己的非加锁表，别人可以读自己的非加锁表\n\n\n施加写锁时\n自己可以读自己的锁定表；别人不可以读自己的加锁表，会阻塞\n自己可以修改加锁表；别人不可以修改加锁表\n自己不可以读自己的非加锁表；别人可以读自己的非加锁表\n\n\n\n\n对MyISAM表的读操作〈加读锁)，不会阻塞其他进程对同一表的读请求，但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其它进程的写操作。\n对MyISAM表的写操作（加写锁)，会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其它进程的读写操作。\n\n\n\n\n\n\n\n\n\n\n锁分析\nSHOW Status like 'table%'\n\n这里有两个状态变量记录MySQL内部表级锁定的情况，两个变量说明如下:\n\nTable_locks_immediate:产生表级锁定的次数，表示可以立即获取锁的查询次数，每立即获取锁值加1 ;\nTable_locks waited:出现表级锁定争用而发生等待的次数(不能立即获取锁的次数，每等待一次锁值加1)，此值高则说明存在着较严重的表级锁争用情况;\n\n此外，Myisam的读写锁调度是写优先，这也是myisam不适合做写为主表的引擎。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞\n4.3 行锁(偏写)特点：\n\n偏向InnoDB存储引擎，开销大，加锁慢;\n会出现死锁;\n锁定粒度最小，发生锁冲突的概率最低,并发度也最高。\n\nInnoDB与MyISAM的最大不同有两点:一是支持事务（TRANSACTION)﹔二是采用了行级锁\n\n\n\n\n\n\n\n\n\n误操作导致无索引行锁升级为表锁\n索引失效，如varchar必须加单引号等\n\n\n\n\n\n\n\n\n\n间隙锁的危害\n当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁;对于键值在条件范围内但并不存在的记录，叫做“间隙”，\nInnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁(Next-Key锁)。\n\n\n没有a = 2，产生间隙\nINSERT INTO test_innodb_lock values(2,'2000');\n\nsession1 commit之前，session2会被阻塞\n\n因为Query执行过程中通过过范围查找的话，他会锁定整个范围内所有的索引键值，即使这个键值并不存在。\n间隙锁有一个比较致命的弱点，就是当锁定一个范围键值之后，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。在某些场景下这可能会对性能造成很大的危害\n\n\n\n\n\n\n\n\n\n\n如何锁定一行\n\n\n\n\n\n\n\n\n\n\n锁分析\nShow status like 'innodb_row_lock%';\n\n\nlnnodb_row_lock_current_waits:当前正在等待锁定的数量;\nInnodb_row_lock_time:从系统启动到现在锁定总时间长度;\nInnodb_row_lock_time_avg:每次等待所花平均时间;\nInnodb_row_lock_time_max:从系统启动到现在等待最常的一次所花的时间;\nInnodb_row_lock_waits:系统启动后到现在总共等待的次数;\n\n对于这5个状态变量，比较重要的主要是:\n\nInnodb_row_lock_time_avg (等待平均时长)，\nInnodb_row_lock_waits(等待总次数)\nInnodb_row_lock_time(等待总时长)\n\n这三项。尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。\n4.4 页锁特点：\n\n开销和加锁时间界于表锁和行锁之间;\n会出现死锁;\n锁定粒度界于表锁和行锁之间，并发度一般\n\n4.5 锁优化建议\n尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁。\n合理设计索引，尽量缩小锁的范围\n尽可能较少检索条件，避免间隙锁\n尽量控制事务大小,减少锁定资源量和时间长度\n尽可能低级别事务隔离\n\n五、主从复制5.1 原理slave会从master读取binlog来进行数据同步\n\n\nmaster将改变记录到二进制日志(binary log)。这些记录过程叫做二进制日志事件，binary log events;\nslave将master的binary log events拷贝到它的中继日志(relay log）；\nslave重做中继日志中的事件，将改变应用到自己的数据库中。MySQL复制是异步的且串行化的\n\n\n\n\n\n\n\n\n\n\n复制的基本原则\n\n每个slave只有一个master\n每个slave只能有一个唯一的服务器ID\n每个master可以有多个salve\n\n\n\n\n\n\n\n\n\n\n复制的最大问题\n延时\n5.2 一主一从配置案例\nMySQL版本尽量一致，版本号前两段必须相同。同时互相必须能ping通\n\n主机修改配置文件\n\n必须: 主服务器唯一ID：server-id=1\n必须：启用二进制日志：log-bin = 本地路径/mysqlbin\n启用错误日志：log-err = 本地路径/mysqlerr\n根目录：basedir = “本地路径”\n临时目录：tmpdir = “本地路径”\n数据目录：datadir = “本地路径/Data”\nread-only = 0 ,代表主机读写都可以\n设置不要复制的数据库：binlog-ignore-db = 数据库名列表\n设置需要复制的数据库：binlog-do-db = 数据库名列表\n\n\n从机修改配置文件\n\n必须: 从机服务器唯一ID ：service-id = 2\n[建议] 启用二进制日志\n\n\n重启主从机MySQL数据库\n\n主从机关闭防火墙\n\n在主机上建立账户并授权slave\n\n```sqlGRANT REPLICATION SLAVE ON . TO ‘账户名‘@’从机数据库IP’ IDENTIFIED BY ‘账户密码’;flush privileges; # 刷新SHOW master status; # 查询Master的状态，并记录下File和Position的值！\n\n   2. 操作完上述步骤后，不再操作主服务器MySQL，防止主服务器状态值变化\n\n7. 在**从机**上配置需要复制的主机\n\n   1. &#96;&#96;&#96;sql\n      # 登录主机账户\n      CHANGE MASTER TO MASTER_HOST&#x3D;&#39;主机IP&#39;,\n      MASTER_USER&#x3D;&#39;账户名&#39;,\n      MASTER_PASSWORD&#x3D;&#39;账户密码&#39;,\n      MASTER_LOG_FILE&#x3D;&#39;6.1步骤中记录下的File值&#39;,\n      MASTER_LOG_POS&#x3D;&#39;6.1步骤中记录下的Position值&#39;;\n      # 启动从服务器复制功能\n      START SLAVE;\n使用Show slave status\\G命令，若Slave_IO_Running:Yes且Slave_SQL_Running:Yes则主从配置成功\n\n\n\n停止从服务复制功能：stop slave;\n\n\n7.2 中可能出现某一项不为Yes，解决方法：\n\n附录A MySQL文件结构\n\n\n二进制日志log-bin :主从复制\n错误日志log-error：默认是关闭的,记录严玉的警告和告误信息，每次启动和关闭的详细信息等。\n查询日志log：默认关闭，记录查询的sql语句，如果开启会减低mysql的整体性能，因为记录日志也是需要消耗系统资源的\n数据文件\n位置：\nwindows data日录下可以祧选很多库\nlinux 默认路径:fvarflibfmysql\n\n\nfrm文件 ：存放表结构\nmyd文件：存放表数据\nmyi文件：存放表索引\n\n\n配置文件\nwindows：my.ini文件\nLinux：/etc/my.cnf文件\n\n\n\n附录B MySQL的隔离级别==在读懂本案例的前提是，你已经明白了事务是什么，下述的问题都出现在一个事务当中，而不是某次操作==\n\n读未提交（READ UNCOMMITTED）\n读提交 （READ COMMITTED）\n可重复读 （REPEATABLE READ）\n串行化 （SERIALIZABLE）\n\n从上往下，隔离强度逐渐增强，性能逐渐变差。采用哪种隔离级别要根据系统需求权衡决定，其中，可重复读是 MySQL 的默认级别。\n\n\n只有串行化的隔离级别解决了全部这 3 个问题，其他的 3 个隔离级别都有缺陷。\n1、读未提交MySQL 事务隔离其实是依靠锁来实现的，加锁自然会带来性能的损失。而读未提交隔离级别是不加锁的，所以它的性能是最好的，没有加锁、解锁带来的性能开销。但有利就有弊，这基本上就相当于裸奔啊，所以它连脏读的问题都没办法解决。\n任何事务对数据的修改都会第一时间暴露给其他事务，即使事务还没有提交。\n读未提交，其实就是可以读到其他事务未提交的数据，但没有办法保证你读到的数据最终一定是提交后的数据，如果中间发生回滚，那就会出现脏数据问题，读未提交没办法解决脏数据问题。\n2、读提交既然读未提交没办法解决脏数据问题，那么就有了读提交。读提交就是一个事务只能读到其他事务已经提交过的数据，也就是其他事务调用 commit 命令之后的数据。\n读提交事务隔离级别是大多数流行数据库的默认事务隔离界别，比如 Oracle，但是不是 MySQL 的默认隔离界别。\n\n\n每个 select 语句都有自己的一份快照，而不是一个事务一份，所以在不同的时刻，查询出来的数据可能是不一致的。即本案例事务B中，前两个 SELECT 和第三个的查询结果是不一致的\n读提交解决了脏读的问题，但是无法做到可重复读，也没办法解决幻读。\n3、可重复读可重复是对比不可重复而言的，不可重复读是指同一事物不同时刻读到的数据值可能不一致。而可重复读是指，事务不会读到其他事务对已有数据的修改，即使其他事务已提交，也就是说，事务开始时读到的已有数据是什么，在事务提交前的任意时刻，这些数据的值都是一样的。但是，对于其他事务新插入的数据是可以读到的，这也就引发了幻读问题\n\n\n可重复读做到了，这只是针对已有行的更改操作有效，但是对于新插入的行记录，幻读就产生了\n\n\n在事务A commit之前，第二个SELECT读到的数据比第一次SELECT多一条数据！\n4、串行化串行化是4种事务隔离级别中隔离效果最好的，解决了脏读、可重复读、幻读的问题，但是效果最差，它将事务的执行变为顺序执行，与其他三个隔离级别相比，它就相当于单线程，后一个事务的执行必须等待前一个事务结束。效率极低\n附录C MVCC==为了解决不可重复读，或者为了实现可重复读，MySQL 采用了 MVCC (Multi-Version Concrrent Control多版本并发控制) 的方式。==\n它的实现原理主要是版本链，undo日志 ，Read View\n我们在数据库表中看到的一行记录可能实际上有多个版本，每个版本的记录除了有数据本身外，还要有一个表示版本的字段，记为row trx_id，而这个字段就是使其产生的事务的 id，事务 ID 记为 transaction id，它在事务开始的时候向事务系统申请，按时间先后顺序递增。\n\n6字节的事务ID(DB_TRX_ID)字段：用来标识最近一次对本行记录做修改(insert|update)的事务的标识符，即最后一次修改(insert|update)本行记录的事务id。至于delete操作，在innodb看来也不过是一次update操作，更新行中的一个特殊位将行表示为deleted，并非真正删除。\n7字节的回滚指针(DB_ROLL_PTR)字段：指写入回滚段(rollback segment)的 undo log record (撤销日志记录记录)。如果一行记录被更新, 则 undo log record 包含 ‘重建该行记录被更新之前内容’ 所必须的信息。\n6字节的DB_ROW_ID字段：包含一个随着新行插入而单调递增的行ID，当由innodb自动产生聚集索引时，聚集索引会包括这个行ID的值，否则这个行ID不会出现在任何索引中。\n\n\n\n快照，学名叫做一致性视图，这也是可重复读和不可重复读的关键，可重复读是在事务开始的时候生成一个当前事务全局性的快照，而读提交则是每次执行语句的时候都重新生成一次快照。\n对于一个快照来说，它能够读到那些版本数据，要遵循以下规则：\n\n当前事务内的更新，可以读到；\n版本未提交，不能读到；\n版本已提交，但是却在快照创建后提交的，不能读到；\n版本已提交，且是在快照创建前提交的，可以读到；\n\n\n快照读：读取的是快照版本，也就是历史版本。简单的select操作(不包括 select … lock in share mode, select … for update)\n\n当前读：读取的是最新版本。UPDATE、DELETE、INSERT、SELECT …  LOCK IN SHARE MODE、SELECT … FOR UPDATE是当前读。\n\n\n附录D LBCC与附录C相对，LBCC (Lock-Based Concrrent Control)是基于锁的并发控制。如果仅仅是基于锁来实现事务隔离，一个事务读取的时候不允许其他时候修改，那 就意味着不支持并发的读写操作，而我们的大多数应用都是读多写少的，这样会极大地 影响操作数据的效率。\n\n锁定力度：表锁 &gt; 行锁\n加锁效率：表锁 &gt; 行锁\n冲突概率：表锁 &gt; 行锁\n并发性能：表锁 &lt; 行锁\n\ninnodb的行锁是通过给索引项加锁实现的,这就意味着只有通过索引条件检索数据时,innodb才使用行锁,否则使用表锁。\n\n\n\n\n\n\n\n\n\n官方八锁\n\n乐观锁、互斥锁(共享和独占/排他 锁)(Shard and Exclusive Locks, 行级锁)\n意向锁(Intention Locks, 表级锁)\n记录锁/行锁 (Record Locks, 行级锁)\n间隙锁 (Gap Locks, 行级锁)\n临键锁 (Next-key Locksl, 行级锁) (Record Locks + Gap Locks )\n插入意向锁(Insert Intention Locks, 行级锁)\n自增锁(AUTO-INC Locks, 表级锁)\n空间索引的谓词锁 \n\nD1 Shard And Exclusive Locks行级锁定,其中有两种类型的锁：共享锁、独占锁\n\n共享(S)锁允许持有锁的事务读取一行。\n排他(X)锁允许持有锁的事务更新或删除行\n\n如果事务T1在行r上持有共享(S)锁，则来自某些不同事务T2的对行r的锁请求将按以下方式处理：\n\nT2对S锁定的请求可以立即获得批准。结果，T1和T2都在r上保持S锁定。\nT2对X锁定的请求无法立即获得批准。\n\n如果事务T1在行r上拥有排他(X)锁，则不能立即批准来自某个不同事务T2的对r上任一类型的锁的请求。相反，事务T2必须 await 事务T1释放对行r的锁定。\nD2 Intention LocksInnoDB 支持多重粒度，它允许行锁和表锁并存,意向锁是一种不与行级锁冲突表级锁。例如，LOCK TABLES ... WRITE 之类的语句在指定的table上具有排他锁。为了使在多个粒度级别上的锁定切实可行，InnoDB使用意向锁(Intention Locks)。==意向锁是 table 级锁==，指示事务稍后对 table 中的行需要哪种类型的锁(共享锁或排他锁)。\n加意向锁的目的是为了表明某个事务正在锁定一行或者将要锁定一行。\n当一个事务在需要获取资源的锁定时，如果该资源已经被排他锁占用，则数据库会自动给该事务申请一个该表的意向锁。如果自己需要一个共享锁定，就申请一个意向共享锁。如果需要的是某行（或者某些行）的排他锁定，则申请一个意向排他锁。\n有两种类型的意图锁：\n\n意向共享锁(IS) 表示事务打算对table中的各个行设置共享锁\n-- 事务要获取某些行的 S 锁，必须先获得表的 IS 锁。\nSELECT column FROM table ... LOCK IN SHARE MODE; \n意向排他锁(IX) 表示事务打算对table中的各个行设置排他锁\n-- 事务要获取某些行的 X 锁，必须先获得表的 IX 锁。\nSELECT column FROM table ... FOR UPDATE; \n\n意向锁是有数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享 / 排他锁之前，InooDB 会先获取该数据行所在在数据表的对应意向锁。\n意向锁定协议：\n\n在事务可以获取 table 中某行的共享锁之前，它必须首先获取该 table 中的IS锁或更强的锁。\n在事务可以获取 table 中某行的排它锁之前，它必须首先获取该 table 中的IX锁。\n\n\n\n\n\n\n\n\n\n\n兼容性\n\n\n\n\nX\nIX\nS\nIS\n\n\n\nX\n冲突\n冲突\n冲突\n冲突\n\n\nIX\n冲突\n兼容\n冲突\n兼容\n\n\nS\n冲突\n冲突\n兼容\n兼容\n\n\nIS\n冲突\n兼容\n兼容\n兼容\n\n\n\n\n\n\n\n\n\n\n\n举例\n事务A锁住表中的一行(写锁)\n事务B锁住整个表(写锁)\n\n出现的问题：事务A既然锁住了某一行，其他事务就不可能修改这一行。这与”事务B锁住整个表就能修改表中的任意一行“形成了冲突。所以，没有意向锁的时候，行锁与表锁共存就会存在问题！！\n\n使用意向锁后，事务A在申请行锁（写锁）之前，数据库会自动先给事务A申请表的意向排他锁。当事务B去申请表的写锁时就会失败，因为表上有意向排他锁之后事务B申请表的写锁时会被阻塞。\n事务 A 获取了某一行的排他锁，并未提交：\nSELECT * FROM users WHERE id = 6 FOR UPDATE;\n\n此时 users 表存在两把锁：\n\nusers 表上的意向排他锁\nid 为 6 的数据行上的排他锁。\n\n事务 B 想要获取 users 表的共享锁(表级)：\nLOCK TABLES users READ;\n\n此时事务 B 检测事务 A 持有 users 表的意向排他锁，就可以得知事务 A 必然持有该表中某些数据行的排他锁，那么事务 B 对 users 表的加锁请求就会被排斥（阻塞），而无需去检测表中的每一行数据是否存在排他锁。\n最后事务 C 也想获取 users 表中某一行的排他锁：\nSELECT * FROM users WHERE id = 5 FOR UPDATE;\n\n\n事务 C 申请 users 表的意向排他锁。\n事务 C 检测到事务 A 持有 users 表的意向排他锁。\n因为意向锁之间并不互斥，所以事务 C 获取到了 users 表的意向排他锁。\n因为id 为 5 的数据行上不存在任何排他锁，最终事务 C 成功获取到了该数据行上的排他锁。\n\nD3 Record Locks==InnoDB三种行锁算法之一==。单个行记录上的锁。\n记录锁是对索引记录的锁定（行锁）。例如，SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE;阻止任何其他事务插入，更新或删除t.c1的值为10的行。\n记录锁始终锁定索引记录，即使没有定义索引的 table 也是如此。在这种情况下，InnoDB将创建一个隐藏的聚集索引，并将该索引用于记录锁定。\nD4 Gap Locks==InnoDB三种行锁算法之一==。目的是为了防止同一事务的两次当前读，出现幻读的情况。\n间隙锁是对索引记录之间的间隙的锁定，或者是对第一个或最后一个索引记录之前的间隙的锁定。例如，SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE;防止其他事务将15的值插入到t.c1列中，无论该列中是否已经有这样的值，因为该范围中所有现有值之间的间隙都被锁定。\n间隙可能跨越单个索引值，多个索引值，甚至为空。间隙锁是性能和并发性之间权衡的一部分，并且在某些事务隔离级别而非其他级别中使用。\n值得注意的是：对于使用唯一索引来锁定唯一行来锁定行的语句，不需要间隙锁定。例如，如果id列具有唯一索引，则以下语句仅使用具有id值 100 的行的索引记录锁，其他会话是否在前面的间隙中插入行都没有关系：\nSELECT * FROM child WHERE id = 100;\n\n如果id未构建索引或索引不唯一，则该语句会锁定前面的间隙。即锁定(- ∞ ，100]的行\n如果将事务隔离级别更改为READ COMMITTED，将禁用间隙锁定进行搜索和索引扫描，并且仅将其用于外键约束检查和重复键检查。\nD5 Next-Key Locks==InnoDB三种行锁算法之一==。对于行的查询，都是采用该方法，innodb默认的锁就是Next-Key locks。主要目的是解决幻读的问题。\n临键锁 是索引记录上的记录锁(行锁)和索引记录之前的间隙上的间隙锁定的组和 \n\nD6 Insert Intention Locks插入意图锁是一种在行插入之前通过 INSERT 操作设置的间隙锁定。此锁发出插入意图的 signal 是，如果多个事务未插入间隙中的相同位置，则无需 await 彼此插入的多个事务。假设有索引记录，其值分别为 4 和 7.单独的事务分别尝试插入值 5 和 6，在获得插入行的排他锁之前，每个事务都使用插入意图锁来锁定 4 和 7 之间的间隙，但不要互相阻塞，因为行是无冲突的。\n插入意向锁本质上可以看成是一个Gap Lock\n\n普通的Gap Lock 不允许 在 （上一条记录，本记录） 区间范围内插入数据。\n插入意向锁Gap Lock 允许 在 （上一条记录，本记录） 区间范围内插入数据。\n\n插入意向锁的作用是为了提高并发插入的性能， 多个事务 同时写入 不同数据 至同一索引范围（区间）内，并不需要等待其他事务完成，不会发生锁等待。\n\n\n\n\n\n\n\n\n\n举例\n\n\n\n\n\n\n\n\n\nTime\n会话A\n会话B\n\n\n1\nbegin\nbegin\n\n\n2\nselect * from a where a&lt;=13 for update\n\n\n\n3\n\ninsert into a values (12)– waiting…… （被阻塞了，在这里等待）\n\n\n4\ncommit\n\n\n\n5\n\n输出：Query OK, 1 row affected前提条件：insert操作的锁没有超时\n\n\n此时事务B插入成功但是还未commit，再执行show engine innodb status\\G语句，会有以下输出：\n---TRANSACTION 4425, ACTIVE 26 sec\n2 lock struct(s), heap size 1136, 1 row lock(s), undo log entries 1\nMySQL thread id 3, OS thread handle 140018685810432, query id 247 localhost root\nTABLE LOCK table `test`.`a` trx id 4425 lock mode IX\n\nRECORD LOCKS space id 37 page no 3 n bits 72 index PRIMARY of table `test`.`a` trx id 4425 lock_mode X locks gap before rec insert intention\n\nRecord lock, heap no 4 PHYSICAL RECORD: n_fields 3; compact format; info bits 0\n 0: len 4; hex 8000000d; asc     ;;\n 1: len 6; hex 000000001140; asc      @;;\n 2: len 7; hex b400000128011c; asc     (  ;;\n\n从上面的输出可以看到在记录13 上面加了一把插入意图锁（lock_mode X locks gap before rec insert intention）。\n获得插入意图锁之后，我们就可以在11-13之间并发插入记录，而不需要一个事物等待另一事物，当所有相关的插入的事物都提交后， 13上的插入意向锁 便会释放。\n假如没有插入意向锁，而是用普通的间隙锁。插入数据时会获取这条记录所在区间的间隙锁及这条记录的排它锁，其他事务是不可能在这个区间内插入数据的，因为当前事务已经获取了这个区间内的间隙锁，其他事务无法获取对应记录的排它锁，只能等待其他事务完成；\n用插入意向锁后，数据库设计插入意向锁与排它锁不互斥。多个事务既可以获取对应区间的插入意向锁也可以获取对应记录的排它锁，各个事务互不影响，不需要等待其他事务完成后才能进行插入。\nD7 AUTO-INC Locks在InnoDB中，每个含有自增列的表都有一个自增长计数器。当对含有自增长计数器的表进行插入时，首先会执行select max(auto_inc_col) from t for update来得到计数器的值，然后再将这个值加1赋予自增长列。我们将这种方式称之为AUTO_INC Lock\nAUTO-INC锁是一种特殊的 table 级锁，由事务插入具有AUTO_INCREMENT列的 table 中获得。在最简单的情况下，如果一个事务正在向 table 中插入值，那么任何其他事务都必须 await 自己在该 table 中进行插入，以便第一个事务插入的行接收连续的主键值。\n从MySQL 5.1.22开始，InnoDB中提供了一种轻量级互斥量的自增长实现机制，同时InnoDB存储引擎提供了一个参数innodb_autoinc_lock_mode来控制自增长的模式，进而提高自增长值插入的性能。innodb_autoinc_lock_mode和插入类型有关\ninnodb_autoinc_lock_mode值:\ninnodb_autoinc_lock_mode = 0 传统模式，所有的插入语句在开始的时候都需要先获取自增锁，语句结束之后才释放自增自增锁，最安全但并发性最差。\ninnodb_autoinc_lock_mode = 1 连续模式，InnoDB 中默认的方式，该模式对于可预测插入行数的插入进行了优化，一次可以批量生成连续的值。\ninnodb_autoinc_lock_mode = 2 交错模式，在这种锁定模式下，没有使用表级的自增锁，因此它的速度是最快的。但是该模式下并不能保证生成的值是连续，因此在主从复制或数据恢复的时候，主键可能与之前产生的不一致。\n插入类型：\n\n“INSERT-like” statements\n泛指所有的插入语句, 它包括 “simple-inserts”, “bulk-inserts”, 和 “mixed-mode inserts”.\n\n“Simple inserts”\n插入的记录行数是确定的：比如：insert into values，replace但是不包括： INSERT … ON DUPLICATE KEY UPDATE.\n\n“Bulk inserts”\n插入的记录行数不能马上确定的，比如： INSERT … SELECT, REPLACE … SELECT, and LOAD DATA\n\n“Mixed-mode inserts”\n这些都是simple-insert，但是部分auto increment值给定或者不给定. 例子如下(where c1 is an AUTO_INCREMENT column of table t1):\nINSERT INTO t1 (c1,c2) VALUES (1,'a'), (NULL,'b'), (5,'c'), (NULL,'d');\n\n另外一种 “mixed-mode insert” 就是 INSERT ... ON DUPLICATE KEY UPDATE\n\n\nD8 Predicate Locks for Spatial Indexes在多维空间数据中，没有绝对排序的概念，因此之前引入的间隙锁机制不能有效的处理空间数据的数据隔离。为此 InnoDB 中引入了空间索引谓词锁的机制，空间索引采用的是R-Tree 数据结构实现，空间索引包含了最小矩形边界的数据(MBR)，因此 InnoDB 可以通过在 MBR 上加谓词锁来保证一致性读。\n\n\n\n\n\n\n\n\n\n优化空间分析\n对于MyISAM和InnoDBtable，可以使用SPATIAL索引优化包含空间数据的列中的搜索操作。最典型的操作是：\n\n点查询，搜索包含给定点的所有对象\n区域查询搜索与给定区域重叠的所有对象\n\nMySQL 对空间列上的SPATIAL索引使用具有二次分裂的 R 树。使用几何的最小边界矩形(MBR)构建SPATIAL索引。对于大多数几何图形，MBR 是围绕几何图形的最小矩形。对于水平或垂直线串，MBR 是退化为线串的矩形。对于一个点，MBR 是退化为该点的矩形。\n也可以在空间列上创建普通索引。在非SPATIAL索引中，必须为除POINT列之外的任何空间列声明前缀。\n附录E B树、B+树和B*树传统用来搜索的平衡二叉树有很多，如 AVL 树，红黑树等。这些树在一般情况下查询性能非常好，但当数据非常大的时候它们就无能为力了。原因是当数据量非常大时，内存不够用，大部分数据只能存放在磁盘上，只有需要的数据才加载到内存中。一般而言内存访问的时间约为 50 ns，而磁盘在 10 ms 左右。速度相差了近 5 个数量级，磁盘读取时间远远超过了数据在内存中比较的时间。这说明程序大部分时间会阻塞在磁盘 IO 上。减少磁盘 IO 次数，像 AVL 树，红黑树这类平衡二叉树从设计上无法“迎合”磁盘。\nB的全程是Blance，平衡的意思\n\nB/B- 树：多路搜索树，每个节点存储M/2到M个关键字，非叶子节点存储指向关键字范围的子节点；所有关键字在整棵树中出现，并且只出现一次，非叶子节点可以命中。每个节点只存储一个关键字，等于则命中，小于走左节点，大于走右节点\nB+树：在B-树的基础上，为叶子节点增加链表指针，所有关键字都在叶子节点中出现，非叶子节点作为叶子节点的索引；B+树总时到叶子节点才命中。\nB*树：在B+树的基础上，为非叶子节点也增加链表指针，将节点的最低利用率从1/2提高到2/3\n\nB树模拟\nE1 B/B- 树B类树是平衡树，每个结点到叶子结点的高度都是相同，这也保证了每个查询是稳定的，查询的时间复杂度是log2(n)；其次是构造一个多阶B类树，然后在尽量多的在结点上存储相关的信息，保证层数尽量的少，以便后面我们可以更快的找到信息；总结：利用平衡树的优势加快查询的稳定性和速度。\n一个 m 阶的B树满足以下条件：\n\n定义任意非叶子结点最多只有M个儿子；且M&gt;2；\n根结点的儿子数为[2, M]；\n除根结点以外的非叶子结点的儿子数为[M/2, M]；\n每个结点存放至少M/2-1（取上整）和至多M-1个关键字；（至少2个关键字）\n非叶子结点的关键字个数=指向儿子的指针个数-1；\n非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]；\n非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树；\n所有叶子结点位于同一层；\n\n\n\n\n\n\n\n\n\n\n插入流程\n\n如果该结点的关键字个数没有到达m-1个，那么直接插入即可；\n如果该结点的关键字个数已经到达了m-1个，那么根据B树的性质显然无法满足，需要将其进行分裂。\n分裂的规则是该结点分成两半，将中间的关键字进行提升，加入到父亲结点中，但是这又可能存在父亲结点也满员的情况，则不得不向上进行回溯，甚至是要对根结点进行分裂，那么整棵树都加了一层。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n删除流程\n同样的，我们需要先通过搜索找到相应的值，存在则进行删除，需要考虑删除以后的情况，\n\n如果该结点拥有关键字数量仍然满足B树性质，则不做任何处理；\n如果该结点在删除关键字以后不满足B树的性质（关键字没有到达ceil(m/2)-1的数量），则需要向兄弟结点借关键字，这有分为兄弟结点的关键字数量是否足够的情况。\n如果兄弟结点的关键字足够借给该结点，则过程为将父亲结点的关键字下移，兄弟结点的关键字上移；\n如果兄弟结点的关键字在借出去以后也无法满足情况，即之前兄弟结点的关键字的数量为ceil(m/2)-1，借的一方的关键字数量为ceil(m/2)-2的情况，那么我们可以将该结点合并到兄弟结点中，合并之后的子结点数量少了一个，则需要将父亲结点的关键字下放，如果父亲结点不满足性质，则向上回溯；\n\n\n其余情况参照BST中的删除。\n\n\n\n索引的效率依赖与磁盘 IO 的次数，快速索引需要有效的减少磁盘 IO 次数，如何快速索引呢？索引的原理其实是不断的缩小查找范围，就如我们平时用字典查单词一样，先找首字母缩小范围，再第二个字母等等。平衡二叉树是每次将范围分割为两个区间。为了更快，B-树每次将范围分割为多个区间，区间越多，定位数据越快越精确。那么如果节点为区间范围，每个节点就较大了。\n\n\n\n\nB-树的搜索，从根结点开始，对结点内的关键字（有序）序列进行二分查找，如果命中则结束，否则进入查询关键字所属范围的儿子结点；重复，直到所对应的儿子指针为空，或已经是叶子结点；\nB-树的特性：\n\n关键字集合分布在整棵树中；\n任何一个关键字出现且只出现在一个结点中；\n搜索有可能在非叶子结点结束；\n其搜索性能等价于在关键字全集内做一次二分查找；\n自动层次控制；\n\nE2 B+ 树B+树是B-树的变体，也是一种多路搜索树。\n其定义基本与B-树同，除了：\n\n非叶子结点的子树指针与关键字个数相同；\n非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）；\n为所有叶子结点增加一个链指针；\n所有关键字都在叶子结点出现；\n\n\n\n因为内节点并不存储 data，所以一般B+树的叶节点和内节点大小不同，而B-树的每个节点大小一般是相同的，为一页。\n\n\n\n\n\n\n\n\n\nB+ 树和 B- 树的区别\n\nB+树内节点不存储数据，所有 data 存储在叶节点导致查询时间复杂度固定为 log n。而B-树查询时间复杂度不固定，与 key 在树中的位置有关，最好为O(1)\n如下：B-树/B+树查询节点 key 为 50 的 data\n\nB-树\n\nB+树\n\nkey 为 50 的节点就在第一层，B-树只需要一次磁盘 IO 即可完成查找。由于B+树所有的 data 域都在根节点，所以查询 key 为 50的节点必须从根节点索引到叶节点，时间复杂度固定为 O(log n)。\n\n\n\nB+树叶节点两两相连可大大增加区间访问性，可使用在范围查询等，而B-树每个节点 key 和 data 在一起，则无法区间查找。\n\n根据空间局部性原理：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。\n\nB+树可以很好的利用局部性原理，若我们访问节点 key为 50，则 key 为 55、60、62 的节点将来也可能被访问，我们可以利用磁盘预读原理提前将这些数据读入内存，减少了磁盘 IO 的次数。\n\n\n\nB+树更适合外部存储。由于内节点无 data 域，每个节点能索引的范围更大更精确\n由于B-树节点内部每个 key 都带着 data 域，而B+树节点只存储 key 的副本，真实的 key 和 data 域都在叶子节点存储。磁盘是分 block 的，一次磁盘 IO 会读取若干个 block，具体和操作系统有关，那么由于磁盘 IO 数据大小是固定的，在一次 IO 中，单个元素越小，量就越大。这就意味着B+树单次磁盘 IO 的信息量大于B-树，从这点来看B+树相对B-树磁盘 IO 次数少。\n\n\nE3 B * 树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针；\nB*树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针；\n所以，B*树分配新结点的概率比B+树要低，空间使用率更高；\nE4 MySQL为什么使用B-Tree（B+Tree）上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构。这需要结合计算机组成原理相关知识讨论B - /+Tree作为索引的理论基础。\n一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。\n\n\n\n\n\n\n\n\n\n存储数据最小单元\n在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）他的最小单元是块，一个块的大小是4k\nInnoDB存储引擎也有自己的最小储存单元——页（Page），一个页的大小是16K(可以通过参数innodb_page_size设置)。\n\n假设一行数据的大小是1k，那么一个页可以存放16行这样的数据。\n\n\n\n\n\n\n\n\n\n主存存取原理\n目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。\n\n\n从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。\n主存的存取过程如下：\n\n当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。\n写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。\n这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。\n\n\n\n\n\n\n\n\n\n\n磁盘存取原理\n索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。\n一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。\n盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。\n当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。\n\n\n\n\n\n\n\n\n\n局部性原理和磁盘预读\n由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：\n\n当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。\n\n由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。\n预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。\n所以IO一次就是读一页的大小\n附录F MySQL的在可重复读级别下幻读的解决方案==解决办法：MySQL的间隙锁==\n在数据库中会为索引维护一套B+树，用来快速定位行记录。B+索引树是有序的，所以会把这张表的索引分割成几个区间。\n\n\n\n\n\n\n\n\n\n\n\n案例\n\n\n本案例中，在事务A提交之前，事务B的插入操作只能等待，这就是间隙锁起得作用。\n当事务A执行update user set name=&#39;风筝2号’ where age = 10; 的时候，由于条件 where age = 10 ，数据库不仅在 age =10 的行上添加了行锁，而且在这条记录的两边，也就是(负无穷,10]、(10,30]这两个区间加了间隙锁，从而导致事务B插入操作无法完成，只能等待事务A提交。\n不仅插入 age = 10 的记录需要等待事务A提交，age&lt;10、10&lt;age&lt;30 的记录页无法完成，而大于等于30的记录则不受影响，这足以解决幻读问题了。\n值得注意的是：这是有索引的情况，如果 age 不是索引列，那么数据库会为整个表加上间隙锁。所以，如果是没有索引的话，不管 age 是否大于等于30，都要等待事务A提交才可以成功插入。\n附录G MySQL中的日志\nundo log：是为回滚而用，具体内容就是copy事务前的数据库内容（行）到undo buffer，在适合的时间把undo buffer中的内容刷新到磁盘。\nundo buffer与redo buffer一样，也是环形缓冲，但当缓冲满的时候，undo buffer中的内容会也会被刷新到磁盘；\n与redo log不同的是，磁盘上不存在单独的undo log文件，所有的undo log均存放在主ibd数据文件中（表空间），即使客户端设置了每表一个数据文件也是如此。\n\n\nredo log：确保事务的持久性。redo日志记录事务执行后的状态，用来恢复未写入data file的已成功事务更新的数据。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。\n物理格式的日志，记录的是物理数据页面的修改的信息，其redo log是顺序写入redo log file的物理文件中去的。事务开始之后就产生redo log，redo log的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo log文件中。\n\n\nbin log：用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。用于数据库的基于时间点的还原。\n逻辑格式的日志，可以简单认为就是执行过的事务中的sql语句。而且还包括了执行的sql语句（增删改）反向的信息，也就意味着delete对应着delete本身和其反向的insert；update对应着update执行前后的版本的信息；insert对应着delete和insert本身的信息。\n\n\ngeneral query log：普通查询日志。记录了服务器接收到的每一个查询或是命令，无论这些查询或是命令是否正确甚至是否包含语法错误，general log 都会将其记录下来 ，记录的格式为 {Time ，Id ，Command，Argument }。Mysql默认是把General log关闭的。\n错误日志：记录着mysqld启动和停止,以及服务器在运行过程中发生的错误的相关信息。在默认情况下，系统记录错误日志的功能是关闭的，错误信息被输出到标准错误输出。\n慢查询日志：记录执行时间过长和没有使用索引的查询语句，报错select、update、delete以及insert语句，慢日志只会记录执行成功的语句。(在前面索引优化部分有讲解)\n\n附录H 聚集索引和非聚集索引解析聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。\nH1 聚集索引聚集索引表记录的排列顺序和索引的排列顺序一致(即索引的物理地址顺序和主键的逻辑顺序一致)，所以查询效率快，只要找到第一个索引值记录，其余就连续性的记录在物理也一样连续存放。聚集索引对应的缺点就是修改慢，因为为了保证表中记录的物理和索引顺序一致，在记录插入的时候，会对数据页重新排序。因为在物理内存中的顺序只能有一种，所以聚集索引在一个表中只能有一个。\nH2 非聚集索引非聚集索引制定了表中记录的逻辑顺序，但是记录的物理和索引不一定一致（在逻辑上数据是按顺序排存放的，但是物理上在真实的存储器中是散列存放的），两种索引都采用B+树结构，非聚集索引的叶子层并不和实际数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针方式。非聚集索引层次多，不会造成数据重排。所以如果表的读操作远远多于写操作，那么就可以使用非聚集索引。\nH3","slug":"MySQL高级","date":"2022-04-25T16:00:00.000Z","categories_index":"","tags_index":"DBA","author_index":"Aurora"},{"id":"982a6bb2fed072725a80b0baababe5b6","title":"Java开发手册","content":"零、前言@author：韩霄杰(hanxj_a)\n@date: 2022.02.20\n@update:2022.03.14 / 2022.03.15 / \n\nNPE即 NullPointerException\nJSL即《The Java Language Specification》\nOOM即OutOfMemoryError\nCME即ConcurrentModificationException\nJMM即Java Memory Model\nJVM即Java Virtual Machine\nJ.U.C即java.util.concurrent.* 包\n\n目录\n\n[toc]\n一、三目运算符导致NPE的问题1.1 NPE触发Integer a = 1;\nInteger b = 2;\nInteger c = null;\nBoolean flag = false;\n// a*b的结果是int类型，那么c会强制拆箱成int型，而c是null，因此会抛出NPE\nInteger res = (flag ? a*b : c);\n\n\n\n1.2 基础知识\n三目运算符\n自动拆箱、自动装箱\n\n1.2.1  三目运算符运算规则三目运算符形式：\n&lt;表达式1&gt; ？ &lt;表达式2&gt; : &lt;表达式3&gt;\n通过?、:组合的形式得到一个条件表达式。其中?运算符的含义是：先求表达式 1 的值，\n\n若为真，执行并返回 表达式 2  的结果；\n若为假，则执行并返回 表达式 3 的结果。\n\n值得注意的是：一个条件表达式从不会既计算 &lt; 表达式 2&gt;，又计算 &lt; 表达式\n3&gt;。条件运算符是右结合的，也就是说，从右向左分组计算。例如，a?b:c?d:e 将按\na?b:（c?d:e）执行\n1.2.2 自动拆箱和自动装箱自动装箱和自动拆箱是在Java SE5中，为方便基本数据类型和包装类之间的转换引入的功能。\nInteger a = 10; // 自动装箱\nint b = a; \t\t// 自动拆箱\n\n自动装箱都是通过包装类的 valueOf() 方法来实现的\n自动拆箱都是通过包装类对象的 xxxValue() 来实现的（如 booleanValue()、longValue() 等）。\n1.3 原理刨析1.3.1 问题复现boolean flag = true; \t\t// 设置成 true，保证条件表达式的表达式二一定可以执行\nboolean simpleBoolean = false; // 定义一个基本数据类型的 boolean 变量\nBoolean nullBoolean = null\t;// 定义一个包装类对象类型的 Boolean 变量，值为 null\nboolean x = flag ? nullBoolean : simpleBoolean; // 使用三目运算符并给 x 变量赋值\n\n对上述代码反编译后，可以得到：\nboolean flag = true;\t\t\nboolean simpleBoolean = false;\nBoolean nullBoolean = null;\nboolean x = flag ? nullBoolean.booleanValue() : simpleBoolean;\t// 注意点\n\n反编译后的代码，最后一行进行了一次自动拆箱，而nullBollean是null，null调用booleanValue()方法导致NPE。\n需要注意的是：根据三目运算符运算规则，NPE产生，和flag 以及 nullBoolean的位置有关，若flag = false;那当nullBoolean在:后时，才会导致NPE。\n1.3.2 原理分析根据《JLS》-15.25 相关介绍：\n\n当三目运算符&lt;表达式2&gt;和&lt;表达式3&gt;操作数的类型相同(如都是包装类)时，则三目运算符表达式的结果和这两位操作数的类型相同。\n当三目运算符&lt;表达式2&gt;和&lt;表达式3&gt;操作数中存在一个基本数据类型，那么该表达式的结果的类型要求是基本类型。如果不符合预期，那么编译器就会自动拆箱。\n\n1.4 开发场景拓展假设下述场景：\nMap&lt;String,Boolean> map = new HashMap&lt;String, Boolean>();\nBoolean b = (map!=null ? map.get(“test”) : false);\n\n结果：在小于 JDK 1.8 的版本中执行的结果是 NPE，在 JDK 1.8 及以后的版本中执行结果是 null。\n\n\n\n\n\n\n\n\n\nJDK 8 新规范\n依据JDK8 后的《JLS》：(JDK8前没有这些规定)\n\n如果表达式的第二个和第三个操作数都是布尔表达式，那么该条件表达式就是布尔表达式\n如果表达式的第二个和第三个操作数都是数字型表达式，那么该条件表达式就是数字型表达式\n除了以上两种以外的表达式就是引用表达式\n\n因为第二个操作数为map.get(&quot;test&quot;)，虽然Map在定义时规定了其值类型为Boolean，但是在编译过程泛型会被擦除，因此其结果就是Object，即引用表达式。\n\n如果引用条件表达式出现在赋值上下文或调用上下文中，那么条件表达式就是合成表达式\n\n因为Boolean b = (map!=null ? map.get(&quot;test&quot;) : false)就时一个赋值上下文，所以map!=null ? map.get(&quot;test&quot;) : false是合成表达式\n\n合成的引用条件表达式的类型与其目标类型相同\n\n因此该表达式的第二个操作数和第三个操作数的结果应该都是 Boolean 类型，所以JDK 8以后的编译过程中，把他们都转化成Boolean，即：\nBoolean b = maps == null ? Boolean.valueOf(false) : (Boolean)maps.get(\"test\");\n\n\n\n\n\n\n\n\n\n\n会出现空指针异常的原因\n对上述代码反编译后：\nHashMap hashmap = new HashMap();\nBoolean boolean1 = Boolean.valueOf(hashmap == null ? false : ( (Boolean)hashmap.get(\"test\") ).booleanValue() );\n\n问题出现在：((Boolean)hashmap.get(&quot;test&quot;)).booleanValue()的执行过程中：\nhashmap.get(\"test\") -> null; \n(Boolean) null -> null;\nnull.booleanValue() -> 报错NPE\n\n\n\n\n\n\n\n\n\n\n解决办法\nMap&lt;String,Boolean> map = new HashMap&lt;>();\nBoolean b = (map!=null ? map.get(\"test\") : Boolean.FALSE );\n\n统一为包装类，map.get(“test”)就不会因此调用booleanValue()进行拆箱了。\n二、初始化HashMap容量的建议2.1 基础知识参考《集合》中HashMap的扩容原理。\nHashMap 类中有以下主要成员变量：\n\ntransient int size;\n记录了 Map 中 KV 对的个数\n\n\nloadFactor\n装载印子，用来衡量 HashMap 满的程度。loadFactor 的默认值为 0.75f（static final float DEFAULT_LOAD_FACTOR = 0.75f;）\n\n\nint threshold;\n临界值，当实际 KV 个数超过 threshold 时，HashMap 会将容量扩容，threshold ＝容量 * 加载因子\n\n\n除了以上这些重要成员变量外，HashMap 中还有一个概念：capacity\n容量，如果不指定，默认容量是 16(static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;)\n\n\n\n2.2 建议在已知 HashMap 中将要存放的 KV 个数的时候，设置一个合理的初始化容量可以有效的提高性能。\n默认情况下，当我们设置 HashMap的初始化容量时，实际上 HashMap 会采用第一个大于该数值的 2 的幂作为初始化容量。如我们 new HashMap&lt;String, String&gt;(1)实际容量为2。\n在 Jdk 1.7 和 Jdk 1.8 中，HashMap 初始化这个容量的时机不同。jdk1.8中，在调用 HashMap 的构造函数定义 HashMap 的时候，就会进行容量的设定。而在 Jdk 1.7 中，要等到第一次 put 操作时才进行这一操作。但计算初始化容量的算法基本相同\n\n\n\n\n\n\n\n\n\n初始化大小建议\n依据JDK 8 中putAll方法中的算法实现： (int) ((float) expectedSize / 0.75F + 1.0F)。我们可以认为，当我们明确知道 HashMap 中元素的个数的时候，把默认容量设置成 expectedSize / 0.75F + 1.0F 是一个在性能上相对好的选择，但是，同时也会牺牲些内存\n值得注意的是：本操作是用内存换性能的做法，真正使用时需要考虑内存影响。\n三、禁止使用Executors创建线程池的原因3.1 基础知识\n线程池\n阻塞队列\n\n3.1.1 线程池线程池技术：《深入源码分析Java线程池的实现原理》\n3.1.2 阻塞队列BlockingQueue主要有两种实现：\n\nArrayBlockingQueue\n是一个用数组实现的有界阻塞队列，必须设置容量\n\n\nLinkedBlockingQueue\n是一个用链表实现的有界阻塞队列，容量可以选择进行设置，不设置的话将会是一个无边界的阻塞队列，最大长度为Integer.MAX_VALUE。\n\n\n\n3.2 Executors存在的问题\nFixedThreadPool和SingleThreadPool：允许的请求队列长度为Integer.MAX_VALUE，可能会堆积大量请求，从而导致OOM\nCachedThreadPool和ScheduledThreadPool：允许的创建线程数量为Integer.MAX_VALUE，可能会创建大量的线程，从而导致OOM\n\npublic class ExecutorsDemo &#123;\n    private static ExecutorService executor = Executors.newFixedThreadPool(15);\n    public static void main(String[] args) &#123;\n        for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123;\n            executor.execute(new SubThread());\n        &#125;\n    &#125;\n&#125;\nclass SubThread implements Runnable &#123;\n    @Override\n    public void run() &#123;\n        try &#123;\n            Thread.sleep(10000);\n        &#125; catch (InterruptedException e) &#123;\n            //do nothing\n        &#125;\n    &#125;\n&#125;\n\n通过JVM参数：-Xmx8m -Xms8m运行，就会抛出OOM，抛错行数为5行executor.execute(new SubThread())处。\n3.3 Executors存在缺陷的原因通过报错信息，最直接导致OOM的是LinkedBlockingQueue.offer()方法。\n底层实现：\npublic static ExecutorService newFixedThreadPool(int nThreads) &#123;\n    return new ThreadPoolExecutor(nThreads, nThreads,\n                                  0L, TimeUnit.MILLISECONDS,\n                                  new LinkedBlockingQueue&lt;Runnable>());\n\n由LinkedBlockingQueue的实现可知，这里未设置阻塞队列容量，因此将会是一个无边界的阻塞队列，最大长度为Integer.MAX_VALUE。\n3.4 正确创建线程池\n\n\n\n\n\n\n\n\n方式一：\n避免使用Executors创建线程池，主要是避免使用其中的默认实现。那么可以自己直接调用ThreadPoolExecutor的构造函数自己创建线程池，只需要给BlockQueue指定容量就可以规避了。\nprivate static ExecutorService executor = \n    new ThreadPoolExecutor(10, 10,\n                           60L, TimeUnit.SECONDS,\n                           new ArrayBlockingQueue(10));\n\n此时只要提交的线程数超过当前可用线程数，就会抛出RejectedExecutionException异常。\n\n\n\n\n\n\n\n\n\n方式二：(更推荐)\n使用ThreadFactoryBuilder创建线程池\npublic class ExecutorsDemo &#123;\n    private static ThreadFactory namedThreadFactory = new \n        ThreadFactoryBuilder().setNameFormat(\"demo-pool-%d\").build();\n    private static ExecutorService pool =\n        new ThreadPoolExecutor(5, 200,\n                               0L, TimeUnit.MILLISECONDS,\n                               new LinkedBlockingQueue&lt;Runnable>(1024), namedThreadFactory, new \n                               ThreadPoolExecutor.\n                               AbortPolicy());\n    public static void main(String[] args) &#123;\n        for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123;\n            pool.execute(new SubThread());\n        &#125;\n    &#125;\n&#125;\n\n这种方法不仅可以避免 OOM 的问题，还可以自定义线程名称，更加便于出错时溯源。\n四、谨慎使用ArrayList中的subList()方法4.1 基础知识4.1.1 subList()方法subList() 是 List 接口中定义的一个方法，该方法主要用于返回一个集合中的一段，可以理解为截取一个集合中的部分元素，他的返回值也是一个 List。\n如果将subList的返回值强转成ArrayList或其他List实现类，则会抛出ClassCastException异常。\n4.1.2 视图subList()方法返回的是一个视图，没有新建一个ArrayList，而是返回了一个ArrayList的内部类。这个内部类(SubList)即ArrayList的一个视图。\n当通过set方法修改subList中某个元素的值的时候，源List中对应的元素的值也发生了改变；同理，对源List中的某个元素进行修改，那么subList中对应的值也会发生改变\n4.2 底层原理subList()的底层实现：\npublic List&lt;E> subList(int fromIndex, int toIndex) &#123;\n    subListRangeCheck(fromIndex, toIndex, size);\n    return new SubList(this, 0, fromIndex, toIndex);\n&#125;\n\n这个方法返回了一个SubList，这个类是ArrayList中的一个内部类\nSubList中单独定义了set、get、size、add、remove方法。调用subList()方法时会调用SubList的构造器创建一个SubList：\nSubList(AbstractList&lt;E> parent,\n        int offset, int fromIndex, int toIndex) &#123;\n    this.parent = parent;\n    this.parentOffset = fromIndex;\n    this.offset = offset + fromIndex;\n    this.size = toIndex - fromIndex;\n    this.modCount = ArrayList.this.modCount;\n&#125;\n\n构造函数中把原来的 List 以及该 List 中的部分属性直接赋值给自己的一些属性。\n==SubList只是ArrayList的内部类，没有继承关系，无法直接进行强制类型转换。==\n五、String字符串的“+”操作原理5.1 基础知识5.1.1 String的不可变性不可变类的实例一旦创建，其成员变量的值就不能被修改。这样设计有很多好处，比如可以缓存 hashcode、使用更加便利以及更加安全等。\n5.1.2 StringBufferJava 中除了定义了一个可以用来定义字符串常量的 String 类以外，还提供了可以用来定义字符串变量的 StringBuffer 类，它的对象是可以扩充和修改的。\nStringBuffer wechat = new StringBuffer(\"hanxj_a\");\nString introduce = \" intro \";\nStringBuffer res = wechat.append(\",\").append(introduce);\n\n5.1.3 StringBuilder除了 StringBuffer 以外，还有一个类 StringBuilder 也可以使用，其用法和 StringBuffer 类似。如：\nStringBuilder wechat = new StringBuilder(\"hanxj_a\");\nString introduce = \" intro \";\nStringBuilder res = wechat.append(\",\").append(introduce);\n\n5.1.4 StringUtils.join()apache.commons 中提供的 StringUtils 类，其中的 join 方法可以拼接字符串。\nString wechat = \"hanxj_a\";\nString introduce = \" intro \";\nSystem.out.println(StringUtils.join(wechat, \",\", introduce));\n\n5.2 ‘+’的拼接原理反编译结果：\nString wechat = \"hanxj_a\";\nString introduce = \" intro \";\nString hollis = (new StringBuilder()).append(wechat).append(\",\").\n    append(introduce).toString();\n\n字符串常量在拼接过程中，是将 String 转成了 StringBuilder 后，使用其 append 方法进行处理的。\n从性能来看：StringBuilder&lt;StringBuffer&lt;concat&lt;+&lt;StringUtils.join\n六、foreach中remove/add引起的CME问题6.1 基础知识6.1.1 foreach循环也叫增强for\nfor( 元素类型 t 元素变量 x : 遍历对象 obj)&#123; \n    \n&#125;\n\n原理：\nIterator iterator = variable.iterator();\ndo\n&#123;\n    if(!iterator.hasNext())\n        break;\n    String variable = (String)iterator.next();\n    // 引用了 x 的 java 语句 ; \n&#125; while(true);\n\n6.1.2 fail-fast机制《fail-fast机制》\nJava的集合类运用fail-fast机制进行设计，默认指的是Java集合的一种错误检测机制。当多个线程对部分集合进行结构上的改变的操作时，有可能会产生fail-fast机制，这个时候就会抛出CME。\n值得注意的是：很多时候代码并没有在多线程环境中执行，但依然会抛出CME。\n\n在Java中， 如果在foreach 循环里对某些集合元素进行元素的 remove/add 操作的时候，就会触发fail-fast机制\n\n6.2 原理刨析\nmodCount 是 ArrayList 中的一个成员变量。它表示该集合实际被修改的次数。\nexpectedModCount 是 ArrayList 中的一个内部类——Itr 中的成员变量。expectedModCount表示这个迭代器期望该集合被修改的次数。其值是在ArrayList.iterator 方法被调用的时候初始化的。只有通过迭代器对集合进行操作，该值才会改变。\nItr 是一个 Iterator 的实现，使用 ArrayList.iterator 方法可以获取到的迭代器就是 Itr 类的实例\n\n6.2.1 remove/add的工作机制以remove为例\nprivate void fastRemove(int index)&#123;\n    modCount ++;\n    int numMoved = size - index - 1;\n    if(numMoved > 0)\n        System.arraycopy(elementData,index+1,elementData,index,numMoved);\n    elementData[--size] = null; // clear to let GC do its work\n&#125;\n\n它只修改了 modCount，并没有对 expectedModCount 做任何操作\n6.2.2 原理分析通过异常堆栈可以发现，异常发生在Iterator.next()处，Iterator.next调用了Iterator.checkForComodification()方法。\nfinal void checkForComodification() &#123;\n    if (modCount != expectedModCount)\n        throw new ConcurrentModificationException();\n&#125;\n\n当modCount != expectedModCount就会抛出CME。\n由6.2.1可以知道，modCount在自增，而expectedModCount没有操作，导致该语句被触发\n七、日志框架的使用探究不能直接使用Log4j、Logback中API的原因\n\n结论：为了解耦\n建议：使用例如 Log4j + SLF4J的组合进行日志输出\n7.1 常用日志框架7.1.1 j.u.lj.u.l 是 JDK 1.4 引入的 java.util.logging 包的简称。Java Logging API 提供了七个日志级别用来控制输出。这七个级别分别是：SEVERE、WARNING、INFO、CONFIG、FINE、FINER、FINEST。\n7.1.2 Log4j\n可以控制日志信息输送的目的地是控制台、文件、GUI 组件，甚至是套接口服务器、NT 的事件记录器、UNIX Syslog 守护进程等；\n控制日志的输出格式\n通过定义每一条日志信息的级别，能够更加细致地控制日志的生成过程\n可以通过一个配置文件来灵活地进行配置，而不需要修改应用的代码。\n\nLog4j的七种日志级别：OFF、FATAL、ERROR、WARN、INFO、DEBUG和 TRACE。\n7.1.3 LogBacklogback 当前分成三个模块：logback-core,logback- classic 和 logback-access。logback-core 是其它两个模块的基础模块。logback-classic 是 Log4j 的一个改良版本。ogback-classic 完整实现 SLF4J API，可以很方便地更换成其它日记系统如 Log4j 或 j.u.l。logback-access 访问模块与Servlet 容器集成提供通过 Http 来访问日记的功能。\n7.1.4 Log4j2与Log4j已经完全不同\n7.2 门面模式 (外观模式)核心为：外部与一个子系统的通信必须通过一个统一的外观对象进行，使得子系统更易于使用。\n\n\n\n\n\n\n7.3 日志门面日志门面，是门面模式的一个典型的应用。是为了解决：“每一种日志框架都有自己单独的 API，要使用对应的框架就要使用其对应的 API，这就大大的增加应用程序代码对于日志框架的耦合性。” 这一问题，即解耦\n在日志框架和应用程序之间架设一个沟通的桥梁，对于应用程序来说，无论底层的日志框架如何变，都不需要有任何感知。只要门面服务做的足够好，随意换另外一个日志框架，应用程序不需要修改任意一行代码。\n7.4 常用日志门面7.4.1 SLF4JJava 简易日志门面（Simple Logging Facade for Java，缩写 SLF4J），是一套包装 Logging 框架的界面程式，以外观模式实现。可以在软件部署的时候决定要使用的 Logging 框架，目前主要支持的有 Java Logging API、Log4j 及 logback等。\n\n\n\n\n\n\n\n\n\nLog4j 和 SLF4J对比\n\nLog4j 提供 TRACE, DEBUG, INFO, WARN, ERROR 及 FATAL 六种纪录等级。SLF4J认为ERROR与FATAL没有本质区别，因此只有五种：TRACE, DEBUG, INFO, WARN, ERROR\n logger.error(exception)操作中，Log4j会去把 exception.tostring。真正的写法应该是logger(message.exception); SLF4J中不会使得程序员书写不合适的写法。\nLog4j 间接的在鼓励程序员使用 string 相加的写法(参考五、String字符串的“+”操作原理,存在性能问题)，而 SLF4J 就不会有这个问题，可以使用 logger.error(“{} is+serviceid”,serviceid);\n SLF4J 可以方便的使用其提供的各种集体的实现的 jar\n提供字串内容替换的功能，会比较有效率\nSLF4J 只支持 MDC，不支持 NDC\n\n7.4.2 commons-loggingcommons-logging 和 SLF4J 的功能是类似的。\n是一个基于 Java 的日志记录实用程序，是用于日志记录和其他工具包的编程模型。它通过其他一些工具提供 API，日志实现和包装器实现。\n八、SimpleDateFormat不能被定义成static的原因\n8.1 基础知识8.1.1 SimpleDateFormat用法用于 格式化（日期 -&gt; 文本）、解析（文本 -&gt; 日期）和规范化。使得可以选择任何用户定义的日期 - 时间格式的模式。\n日期格式化时必须使用y表示年，而不能用Y，原因详见：十二、日期格式化时必须使用y表示年，不用Y的原因 。\n使用 SimpleDateFormat 的 format 方法，将一个 Date 类型转化成 String 类型，并且可以指定输出格式：\n//Date 转 String\nDate date = new Date();\nSimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\nString dateStr = sdf.format(date);\nSystem.out.println(dateStr);\n\n使用 SimpleDateFormat 的 parse 方法，将一个 String 类型转化成 Date 类型\n//String 转 Date\nSystem.out.println(sdf.parse(dateStr));\n\n8.1.2 日期和时间模式表达方法使用 SimpleDateFormat 的时候，需要通过字母来描述时间元素，并组装成想要的日期和时间模式。常用的时间元素和字母的对应表如下：\n\n==模式字母通常是重复的，其数量确定其精确表示==\n8.1.3 输出不同时区的时间默认情况下，如果不指明，在创建日期的时候，会使用当前计算机所在的时区作为默认时区。\n使用SimpleDateFormat实现输出指定时区的时间：\nSimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\nsdf.setTimeZone(TimeZone.getTimeZone(\"America/Los_Angeles\"));\nSystem.out.println(sdf.format(Calendar.getInstance().getTime()));\n\n8.1.4 参考资料《线程池的创建》\n《CountDownLatch详解》\n8.2 问题触发以下代码使用线程池执行时间输出：\npublic class Main&#123;\n\t/* 定义一个全局SimpleDateFormat */\n    private static SimpleDateFormat simpleDateformat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n    /* 使用ThreadFactoryBuilder定义一个线程池 */\n    private static ThreadFactory namedThreadFactory = new ThreadFactoryBuilder()\n        .setNameFormat(\"demo-pool-%d\")\n        .build();\n    private static ExecutorService pool = new ThreadPoolExecutor(5,200,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue&lt;Runnable>(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy() );\n    \n    /* 定义一个CountDownLatch，保证所有子线程执行完之后主线程再执行 */\n    private static CountDownLatch countDownLatch = new CountDownLatch(100);\n    \n    public static void main(String[] args)&#123;\n        // 定义一个线程安全的HashSet\n        Set&lt;String> dates = Collections.synchronizedSet(new HashSet&lt;String>());\n        for(int i = 0; i &lt; 100; i ++)&#123;\n            // 获取当前时间\n            Calendar calendar = Calendar.getInstance();\n            int finalI = i;\n            pool.execute(()->&#123;\n                // 时间增加\n                calendar.add(Calendar.DATE, finalI);\n                // 通过simpleDateFormat把时间转化成字符串\n                String dateString = simpleDateFormat.format(calendar.getTime());\n                // 把字符串放入Set中\n                dates.add(dateString);\n                // countDown\n                countDownLatch.countDown();\n                &#125;);\n        &#125;\n        // 阻塞,直到countDown数量为0\n        countDownLatch.await();\n        // 输出 去重后 的时间个数\n        System.out.println(dates.size());\n    &#125;\n&#125;\n//================================\n// result &lt; 100\n\n代码概述：循环一百次，每次循环的时候都在当前时间基础上增加一个天数（这个天数随着循环次数而变化），然后把所有日期放入一个线程安全的、带有去重功能的 Set 中，然后输出 Set 中元素个数。\n SimpleDateFormat 作为一个非线程安全的类，被当做了共享变量在多个线程中进行使用，出现了线程安全问题。\n8.3 原理刨析8.3.1 线程不安全的原因\nSimpleDateFormat 中的 format 方法在执行过程中，会使用一个成员变量calendar 来保存时间。这就是问题的关键。\n由于我们在声明 SimpleDateFormat 的时候，使用的是 static 定义的。那么这 个 SimpleDateFormat 就 是 一 个 共 享 变 量， 随 之，SimpleDateFormat 中 的calendar 也就可以被多个线程访问到。\n假设线程 1 刚刚执行完 calendar.setTime 把时间设置成 2018-11-11，还没等执行完，线程 2 又执行了 calendar.setTime 把时间改成了 2018-12-12。这时候线程 1 继续往下执行，拿到的 calendar.getTime 得到的时间就是线程 2 改过之后的\n需要注意的是：除了 format 方法以外，SimpleDateFormat 的 parse 方法也有同样的问题\n8.3.2 解决方案\n使用局部变量\nfor(int i = 0; i &lt; 100; i ++)&#123;\n    // 获取当前时间\n    Calendar calendar = Calendar.getInstance();\n    int finalI = i;\n    pool.execute(()->&#123;\n       // SimpleDateFormat 声明为 局部变量\n        SimpleDateFormat simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        // 时间增加\n        calendar.add(Calendar.DATE, finalI);\n        // 通过simpleDateFormat把时间转换成字符串\n        String dateString = simpleDateFormat.format(calendar.getTime());\n        // 把字符串放入Set中\n        dates.add(dateString);\n        // countDown\n        countDownLatch.countDown();\n    &#125;);\n&#125;\n加同步锁\nfor (int i = 0; i &lt; 100; i++) &#123;\n    // 获取当前时间\n    Calendar calendar = Calendar.getInstance();\n    int finalI = i;\n    pool.execute(() -> &#123;\n        // 加锁\n        synchronized (simpleDateFormat) &#123;\n            // 时间增加\n            calendar.add(Calendar.DATE, finalI);\n            // 通过 simpleDateFormat 把时间转换成字符串\n            String dateString = simpleDateFormat.format(calendar.getTime());\n            // 把字符串放入 Set 中\n            dates.add(dateString);\n            //countDown\n            countDownLatch.countDown();\n        &#125;\n    &#125;);\n&#125;\n\n优化：可以把锁的粒度再设置的小一点，可以只对 simpleDateFormat.format 这一行加锁\n\n使用ThreadLocal\n/* 使用Thread Local定义一个全局的simpleDateFormat */\nprivate static ThreadLocal&lt;SimpleDateFormat> simpleDateFormatThreadLocal = new ThreadLocal&lt;SimpleDateFormat>()&#123;\n    @Override\n    protected SimpleDateFormat initialValue()&#123;\n        return new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n    &#125;\n&#125;;\n\n// 使用\nString dateString = simpleDateFormatThreadLocal\n    .get()\t// 得到全局的simpleDateFormat\n    .format(calendar.getTime()); // 调用全局simpleDateFormat的format方法\n使用DateTimeFormatter 代替SimpleDateFormat (JDK 8 +)\n// 解析日期\nString dateStr= \"2016 年 10 月 25 日 \";\nDateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy 年 MM 月 dd 日 \");\nLocalDate date= LocalDate.parse(dateStr, formatter);\n// 日期转换为字符串\nLocalDateTime now = LocalDateTime.now();\nDateTimeFormatter format = DateTimeFormatter.ofPattern(\"yyyy 年 MM 月 dd 日 hh:mm a\");\nString nowStr = now .format(format);\n\nSystem.out.println(nowStr);\n\n九、禁止使用isXxx作为变量名的原因\n9.1 使用场景关于这个”本次请求是否成功”的字段的定义，一般情况下，可以有以下四种方式来定义一个布尔类型的成员变量：\nboolean success;\nboolean isSuccess;\nBoolean success;\nBoolean isSuccess;\n\n以上三种命名方式，在IDEA的POJO中自动构建的getter/setter 分别如下：\n// 方式一：boolean success\nclass Model1 &#123;\n    private boolean success;\n    public boolean isSuccess()&#123;\t\t// 相当于getter\n        return success;\n    &#125;\n    public void setSuccess(boolean success)&#123;\n        this.success = success;\n    &#125;\n&#125;\n// 方式二：boolean isSuccess\nclass Model2 &#123;\n    private boolean isSuccess;\n    public boolean isSuccess() &#123;\t// 相当于getter\n        return isSuccess;\n    &#125;\n    public void setSuccess(boolean success)&#123;\n        isSuccess = success;\n    &#125;\n&#125;\n// 方式三：Boolean success\nclass Model3 &#123;\n    private Boolean success;\n    public Boolean getSuccess()&#123;\n        return success;\n    &#125;\n    public void setSuccess(Boolean success)&#123;\n        this.success = success;\n    &#125;\n&#125;\n//方式四：Boolean isSuccess\nclass Model4 &#123;\n    private Boolean isSuccess;\n    public Boolean getSuccess()&#123;\n        return isSuccess;\n    &#125;\n    public void setSuccess(Boolean success)&#123;\n        isSuccess = success;\n    &#125;\n&#125;\n\n简单来说：\n\n基本类型自动生成的 getter 和 setter 方法，名称都是 isXXX() 和 setXXX()形式的。\n包 装 类 型 自 动 生 成 的 getter 和 setter 方 法， 名 称 都 是 getXXX() 和setXXX() 形式的。\n\n9.2 原理剖析9.2.1 Java Bean中关于setter/getter的规范Java Bean 规范文档：JavaBeans(TM) Specification 规定，如果是普通的参数 propertyName，要以以下方式定义其 setter/getter：\npublic &lt;PropertyType> get&lt;PropertyName>();\npublic void set&lt;PropertyName>(&lt;PropertyType> a);\n\n但是，布尔类型的变量 propertyName 则是单独定义的：\npublic boolean is&lt;PropertyName>();\npublic void set&lt;PropertyName>(boolean m);\n\n\n\n因此根据Java Bean规范，Model2(boolean isSuccess)中变量名为isSuccess，如果严格按照规范定义，那么getter应命名为isIsSuccess()。但是多数IDE会默认生成isSuccess\n9.2.2 序列化带来的影响这里以JSON序列化为例：\npublic class BooleanMainTest &#123;\n    \n    public static void main(String[] args) throws IOException &#123;\n        // 创建Model2实体\n        Model2 model2 = new Model2();\n        model2.setSuccess(true);\n        \n        // 使用fastJson(1.2.16)序列化model2成字符串\n        JSON.toJSONString(model2);\n        \n        // 使用Gson(2.8.5)序列化model2 成字符串\n        gson.toJson(model2);\n        \n        //使用jackson(2.9.7)序列化model2成字符串\n        ObjectMapper om = new ObjectMapper();\n        om.writeValueAsString(model2);\n    &#125;\n&#125;\n\nclass Model2 implements Serializable &#123;\n    private static final long serialVersionUID = 1836697963736227954L;\n    private boolean isSuccess;\n    public boolean isSuccess() &#123;\n        return isSuccess;\n    &#125;\n    public void setSuccess(boolean success) &#123;\n        isSuccess = success;\n    &#125;\n    public String getTest()&#123;\n        return \"booleanTest\";\n    &#125;\n&#125;\n\n如果输出上述的三种序列化后的字符串：\nSerializable Result With fastjson :&#123;\"test\":\"booleanTest\",\"success\":true&#125;\nSerializable Result With Gson :&#123;\"isSuccess\":true&#125;\nSerializable Result With jackson :&#123;\"success\":true,\"test\":\"booleanTest\"&#125;\n\n可以得出结论：fastjson 和 jackson 在把对象序列化成 json 字符串的时候，是通过反射遍历出该类中的所有 getter 方法，得到getTest和isSuccess，然后根据JavaBeans规则，认为这是两个属性test和success的值。直接序列化成json:{“test”:”booleanTest”,”success”:true}。 Gson通过反射遍历该类中的所有属性，并把其值序列化成 json:{“isSuccess”:true}。\n若不考虑getTest，那么结果应该是：\nSerializable Result With fastjson :&#123;\"success\":true&#125;\nSerializable Result With Gson :&#123;\"isSuccess\":true&#125;\nSerializable Result With jackson :&#123;\"success\":true&#125;\n\n不同的序列化框架得到的 json 内容并不相同，如果使用 fastjson 进行序列化，再使用 Gson 反序列化：\npublic class BooleanMainTest &#123;\n    public static void main(String[] args) throws IOException &#123;\n        Model2 model2 = new Model2();\n        Model2.setSuccess(true);\n        Gson gson = new Gson();\n        System.out.println( gson.fromJson( JSON.toJSONString(model2), Model2.class ) );\n    &#125;\n&#125;\nclass Model2 implements Serializable &#123;\n    private static final long serialVersionUID = 1836697963736227954L;\n    private boolean isSuccess;\n    public boolean isSuccess() &#123;\n        return isSuccess;\n    &#125;\n    public void setSuccess(boolean success) &#123;\n        isSuccess = success;\n    &#125;\n    @Override\n    public String toString() &#123;\n        return new StringJoiner(\", \", Model2.class.getSimpleName() + \"[\",\"]\")\n            .add(\"isSuccess=\" + isSuccess)\n            .toString();\n    &#125;\n&#125;\n// ================================\n// Model2[isSuccess=false]\n\n我们setSuccces为true，而最终得到的结果是false，这是因为：\nJSON 框架通过扫描所有的 getter后发现有一个 isSuccess 方法，然后根据 JavaBeans 的规范，解析出变量名为success，把 model 对象序列化城字符串后内容为 &#123;&quot;success&quot;:true&#125;\n根据 {“success”:true} 这个 json 串，Gson 框架在通过解析后，通过反射寻找 Model 类中的 success 属性，但是 Model 类中只有 isSuccess 属性，所以，最终反序列化后的 Model 类的对象中，isSuccess 则会使用默认值 false。\n9.2.3 在POJO中Boolean和boolean的使用经过 &lt;&lt;9.2.2 序列化带来的影响&gt;&gt; 的解释，排除错误项后，还剩：\nboolean success;\nBoolean success;\n\n结论：定义一个成员变量时，使用包装类型更好。\npublic class BooleanMainTest &#123;\n    public static void main(String[] args) &#123;\n        Model model = new Model();\n        System.out.println(\"default model: \" + model);\n    &#125;\n&#125;\nclass Model &#123;\n    private Boolean success;\n    private boolean failure;\n    @Override\n    public String toString()&#123;\n        return new StringJoiner(\", \" , Model.class.getSimpleName() + \"[\", \"]\")\n            .add(\"success=\" + success)\n            .add(\"failure=\" + failure)\n            .toString();\n    &#125;\n&#125;\n// =============================================\n// default model : Model[success=null, failure=false]\n\n当我们没有设置 Model 对象的字段的值的时候，Boolean 类型的变量会设置默认值为 null，而 boolean 类型的变量会设置默认值为 false。（即对象的默认值是 null，boolean 基本数据类型的默认值是 false）\n\n\n\n\n\n\n\n\n\n\n举例\n扣费系统：扣费时需要从外部的定价系统中读取一个费率的值，我们预期该接口的返回值中会包含一个浮点型的费率字段。当我们取到这个值得时候就使用公式：金额 * 费率 = 费用 进行计算，计算结果进行划扣。\n如果由于计费系统异常，他可能会返回个默认值，如果这个字段是 Double 类型的话，该默认值为 null，如果该字段是 double 类型的话，该默认值为 0.0。\n如果扣费系统对于该费率返回值没做特殊处理的话，拿到 null 值进行计算会直接报错，阻断程序。拿到 0.0 可能就直接进行计算，得出接口为 0 后进行扣费了。\n如果是基本类型，这种异常情况就无法被感知！！\n9.3 拓展：由JavaBean规范引起的异常JavaBean的规范中明确提到：如果第一个字母是小写，第二个字母大写的情况(如：eId, eName…)，在生成setter/getter的时候直接在前面加上set/get，比如eId的setter/getter是seteId()/geteId()，所以 eId在注入的时候会寻找seteId()方法，而不是setEId()。\n但是Lombok等插件自动生成的get/set是setEId()和getEId()，因此注入时会找不到seteId()方法\n解决方案：\n\n在属性上面加注解@JsonProperty(value = “eId”)\n不使用lombok, 手动写setter -&gt; seteId()\n\n十、禁止修改serialVersionUID值的原因\n10.1 基础知识参考资料：\n\n《序列化与反序列化》\n《深入分析Java序列化与反序列化》\n《单例与序列化》\n\n10.1.1 Serializable类通过实现 java.io.Serializable 接口以启用其序列化功能。未实现此接口的类将无法进行序列化或反序列化。\n Serializable 接口没有方法或字段，仅用于标识可序列化的语义。但是，如果一个类没有实现这个接口，想要被序列化的话，就会抛出 java.io.NotSerializableException 异常：\n// 在执行序列化时会执行：\nif (obj instanceof String) &#123;\n    writeString((String) obj, unshared);\n&#125; else if (c1.isArray())&#123;\n    writeArray(obj, desc, unshared);\n&#125; else if (obj instanceof Enum)&#123;\n    writeEnum((Enum&lt;?>) obj, desc, unshared);\n&#125; else if (obj instanceof Serializable)&#123;\n    writeOrdinaryObject(obj, desc, unshared);\n&#125; else &#123;\n    if (extendedDebugInfo) &#123;\n        throw new NotSerializableException(c1.getName() + \"\\n\" + debugInfoStack.toString());\n    &#125;else &#123;\n        throw new NotSerializableException(c1.getName());\n    &#125;\n&#125;\n\n在进行序列化操作时，会判断要被序列化的类是否是 Enum、Array 和 Serializable 类型，如果都不是则直接抛出 NotSerializableException。\n10.1.2 ExternalizableExternalizable 继承自 Serializable，该接口中定义了两个抽象方法：writeExternal() 与 readExternal()\n当使用 Externalizable 接口来进行序列化与反序列化的时候需要开发人员重写 writeExternal() 与 readExternal() 方法。否则所有变量的值都会变成默认值。\n10.1.3 transienttransient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。\n10.1.4 自定义序列化策略​    在序列化过程中，如果被序列化的类中定义了writeObject 和readObject方法，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化。\n​    如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。\n​    用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程，比如可以在序列化的过程中动态改变序列化的数值。所以，对于一些特殊字段需要定义序列化的策略的时候，可以考虑使用 transient 修饰，并自己重写 writeObject 和 readObject 方法。(ArrayList中就有这样的实现)\n10.1.5 serialVersionUID==序列化是将对象的状态信息转换为可存储或传输的形式的过程。== 可以在JVM 停机的情况下也能把对象保存下来。\n虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的一点是两个类的序列化 ID 是否一致，这个所谓的序列化 ID，就是我们在代码中定义的 serialVersionUID。\n\n\n\n\n\n\n\n\n\n举例：serialVersionUID变化后会发生什么\npublic class SerializableDemo &#123;\n    public static void main(String[] args) &#123;\n        // 初始化Object\n        User1 user = new User1();\n        user.setName(\"Han Xiaojie\");\n        // 序列化到文件\n        ObjectOutputStream oos = null;\n        try &#123;\n            oos = new ObjectOutputStream(new FileOutputStream(\"tempFile\"));\n            oos.writeObject(user);\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            IOUtils.closeQuietly(oos);\n        &#125;\n    &#125;\n&#125;\nclass User1 implements Serializable &#123;\n    private static final long serialVersionUID = 1L;\n    private String name;\n    \n    public String getName() &#123;return name;&#125;\n    public void setName(String name) &#123;this.name = name;&#125;\n&#125;\n\n存入文件后，修改User1类，将serialVersionUID修改为2L\nclass User1 implements Serializable &#123;\n    private static final long serialVersionUID = 2L;\n    private String name;\n    \n    public String getName() &#123;return name;&#125;\n    public void setName(String name) &#123; this.name = name;&#125;\n&#125;\n\n反序列化\npublic class SerializableDemo2 &#123;\n    public static void main(String[] args) &#123;\n        // 从File中读数据\n        File file = new File(\"tempFile\");\n        ObjectInputStream ois = null;\n        try &#123;\n            ois = new ObjectInputStream(new FileInputStream(file));\n            User1 newUser = (User1) ios.readObject();\n            System.out.println(newUser);\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            IOUtils.closeQuietly(ois);\n            try &#123;\n                FileUtils.forceDelete(file);\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n执行结果：\njava.io.InvalidClassException: com.hollis.User1; local class incompatible: stream classdesc \nserialVersionUID = 1, local class serialVersionUID = 2\n\n\n\n\n\n\n\n\n\n\n举例：不设置serialVersionUID\n根据上各例子的demo代码，修改User1：\nclass User1 implements Serializable &#123;\n    private String name;\n    \n    public String getName() &#123;return name;&#125;\n    public void setName(String name) &#123;this.name = name;&#125;\n&#125;\n\n然后修改User1类，向其中增加一个属性：\nclass User1 implements Serializable &#123;\n    private String name;\n    private int age;\n    \n    public String getName() &#123;return name;&#125;\n    public void setName(String name) &#123;this.name = name;&#125;\n    public int getAge() &#123;return age;&#125;\n    public void setAge(int age) &#123;this.age = age;&#125;\n&#125;\n\n反序列化结果：\njava.io.InvalidClassException: com.hollis.User1; \nlocal class incompatible: stream classdesc serialVersionUID = -2986778152837257883, local class serialVersionUID = 7961728318907695402\n\n从本例中可以看出，系统自己添加了一个serialVersionUID。\nserialVersionUID的两种显式生成方式：\n\n默认的 1L，比如：private static final long serialVersionUID = 1L;\n根据类名、接口名、成员方法及属性等来生成一个 64 位的哈希字段，如：private static final long serialVersionUID = xxxxL; ([本方法可以借助IDE实现](#10.3 IDEA提示设置))\n\n==总结来说，serialVersionUID 其实是验证版本一致性的==\n10.2 原理剖析反序列化的调用链：\nObjectInputStream.readObject -&gt; readObject0 -&gt; readOrdinaryObject -&gt; readClassDesc -&gt; readNonProxyDesc -&gt; ObjectStreamClass.initNonProxy\n在initNonProxy中，关键代码：\n\n在反序列化过程中，对 serialVersionUID 做了比较，如果发现不相等，则直接抛出异常\n深入至 getSerialVersionUID() 方法：\npublic long getSerialVersionUID() &#123;\n    // REMIND: synchronize instead of relying on volatile?\n    if (suid == null) &#123;\n        suid = AccessController.doPrivileged(\n        \tnew PrivilegeAction&lt;Long>() &#123;\n                public Long run() &#123;\n                    return computeDefaultSUID(c1);\n                &#125;\n            &#125;\n        );\n    &#125;\n    return suid.longValue();\n&#125;\n\n在没有定义 serialVersionUID 的时候，会调用 computeDefaultSUID 方法，生成一个默认的 serialVersionUID。\n这里也解释了  [&lt;&lt;10.1.5 serialVersionUID&gt;&gt;](#10.1.5 serialVersionUID) 中两个例子的现象\n10.3 IDEA的提示设置\n\nFile -&gt; setting 中搜索serialVersionUID\n勾选Serializable class without serialVersionUID项。\n\n十一、禁止使用Apache Beanutils进行属性copy的原因\n11.1 常用工具类\nSpring BeanUtils\nCglib BeanCopier\nApache BeanUtils\nApache PropertyUtils\nDozer\n\n11.2 性能对比POJO：\npublic class PersonDO &#123;\n    private Integer id;\n    private String name;\n    private Integer age;\n    private Date birthday;\n    // 省略 setter/getter\n&#125;\n// ===================================\npublic class PersonDTO &#123;\n    private String name;\n    private Integer age;\n    private Date birthday;\n&#125;\n\n\n\n\n\n\n\n\n\n\n使用Spring BeanUtils进行属性拷贝\nprivate void mappingBySpringBeanUtils(PersonDO personDO, int times) &#123;\n    // stopwatch 用于记录代码执行时间\n    StopWatch stopwatch = new StopWatch();\n    stopwatch.start();\n    \n    for(int i = 0; i &lt; times; i++) &#123;\n        PersonDTO personDTO = new PersonDTO();\n        org.springframework.beans.BeanUtils.copyProperties(personDO, personDTO);\n    &#125;\n    stopwatch.stop();\n    System.out.println(\"mappingBySpringBeanUtils cost :\" + stopwatch.getTotalTimeMillis());\n&#125;\n\n\n\n\n\n\n\n\n\n\n使用Cglib BeanCopier进行属性拷贝\nprivate void mappingByCglibBeanCopier (PersonDO personDO, int time) &#123;\n    StopWatch stopwatch = new StopWatch();\n    stopwatch.start();\n    \n    for(int i = 0; i &lt; times; i++)&#123;\n        PersonDTO personDTO = new PersonDTO();\n        BeanCopier copier = BeanCopier.create(PersonDO.class, PersonDTO.class, false);\n        copier.copy(personDO, personDTO, null);\n    &#125;\n    stopwatch.stop();\n    System.out.println(\"mappingByCglibBeanCopier cost :\" + stopwatch.getTotalTimeMillis());\n&#125;\n\n\n\n\n\n\n\n\n\n\n使用Apache BeanUtils进行属性拷贝\nprivate void mappingByApachePropertyUtils(PersonDO personDO, int times) throws InvocationTargetException, IllegalAccessException,NoSuchMethodException &#123;\n    StopWatch stopwatch = new StopWatch();\n    stopwatch.start();\n    for(int i = 0; i &lt; times; i++)&#123;\n        PersonDTO personDTO = new PersonDTO();\n        BeanUtils.copyProperties(personDTO, personDO);\n    &#125;\n    stopwatch.stop();\n    System.out.println(\"mappingByApacheBeanUtils cost :\" + stopwatch.getTotalTimeMillis());\n&#125;\n\n\n\n\n\n\n\n\n\n\n使用Apache PropertyUtils进行属性拷贝\nprivate void mappingByApachePropertyUtils(PersonDO personDO, int times) throws nvocationTargetException, IllegalAccessException,NoSuchMethodException &#123;\n    StopWatch stopwatch = new StopWatch();\n    stopwatch.start();\n    for(int i = 0; i &lt; times; i++)&#123;\n        PersonDTO personDTO = new PersonDTO();\n        PropertyUtils.copyProperties(personDTO, personDO);\n    &#125;\n    stopwatch.stop();\n    System.out.println(\"mappingByApachePropertyUtils cost :\" + stopwatch.getTotalTimeMillis());\n&#125;\n\n\n\n性能测试代码：\npublic static void main(String[] args)\n    throws InvocationTargetException, IllegalAccessException, \nNoSuchMethodException &#123;\n    PersonDO personDO = new PersonDO();\n    personDO.setName(\"Hollis\");\n    personDO.setAge(26);\n    personDO.setBirthday(new Date());\n    personDO.setId(1);\n    MapperTest mapperTest = new MapperTest();\n    mapperTest.mappingBySpringBeanUtils(personDO, 100);\n    mapperTest.mappingBySpringBeanUtils(personDO, 1000);\n    mapperTest.mappingBySpringBeanUtils(personDO, 10000);\n    mapperTest.mappingBySpringBeanUtils(personDO, 100000);\n    mapperTest.mappingBySpringBeanUtils(personDO, 1000000);\n    mapperTest.mappingByCglibBeanCopier(personDO, 100);\n    mapperTest.mappingByCglibBeanCopier(personDO, 1000);\n    mapperTest.mappingByCglibBeanCopier(personDO, 10000);\n    mapperTest.mappingByCglibBeanCopier(personDO, 100000);\n    mapperTest.mappingByCglibBeanCopier(personDO, 1000000);\n    mapperTest.mappingByApachePropertyUtils(personDO, 100);\n    mapperTest.mappingByApachePropertyUtils(personDO, 1000);\n    mapperTest.mappingByApachePropertyUtils(personDO, 10000);\n    mapperTest.mappingByApachePropertyUtils(personDO, 100000);\n    mapperTest.mappingByApachePropertyUtils(personDO, 1000000);\n    mapperTest.mappingByApacheBeanUtils(personDO, 100);\n    mapperTest.mappingByApacheBeanUtils(personDO, 1000);\n    mapperTest.mappingByApacheBeanUtils(personDO, 10000);\n    mapperTest.mappingByApacheBeanUtils(personDO, 100000);\n    mapperTest.mappingByApacheBeanUtils(personDO, 1000000);\n&#125;\n\n运行结果：\n\n折线图：\n\n在性能方面，Spring BeanUtils 和 Cglib BeanCopier 表现比较不错，\nApache PropertyUtils、Apache BeanUtils 以及 Dozer则表现的很不好\n​        所以考虑性能的话，不要选择 Apache PropertyUtils、Apache BeanUtils 以及 Dozer 等工具类。Apache BeanUtils 力求做得完美 , 在代码中增加了非常多的校验、兼容、日志打印等代码，过度的包装导致性能下降严重。\n十二、日期格式化时必须使用y表示年，不用Y的原因\nSimpleDateFormat的用法可见：[8.1.1 SimpleDateFormat用法](#8.1.1 SimpleDateFormat用法)\n12.1 ISO 8601因为不同人对于日期和时间的表示方法有不同的理解，于是制定了一个国际规范：ISO 8601。\n​        国际标准化组织的国际标准 ISO 8601 是日期和时间的表示方法，全称为《数据存储和交换形式·信息交换·日期和时间的表示方法》\n在 ISO 8601 中。对于一年的第一个日历星期有以下四种等效说法：\n\n本年度第一个星期四所在的星期\n1 月 4 日所在的星期\n本年度第一个至少有 4 天在同一星期内的星期\n星期一在去年 12 月 29 日至今年 1 月 4 日以内的星期\n\n根据这个标准，我们可以推算出：\n2020 年第一周：2019.12.29-2020.1.4\n所以， 根 据 ISO 8601 标 准，2019 年 12 月 29 日、2019 年 12 月 30 日、2019 年 12 月 31 日这两天，其实不属于 2019 年的最后一周，而是属于 2020 年的第一周\n12.2 JDK 针对ISO 8601提供的支持​        我们希望输入一个日期，然后程序告诉我们，根据 ISO 8601 中关于日历日期的定义，这个日期到底属于哪一年。比如输入 2019-12-20，他显示 2019；而输入 2019-12-30 的时候，他显示 2020。\n​        为了提供这样的数据，Java 7 引入了「YYYY」作为一个新的日期模式来作为标识。使用「YYYY」作为标识，再通过 SimpleDateFormat 就可以得到一个日期所属的周属于哪一年了。\npublic class WeekYearTest &#123;\n    public static void main(String[] args) &#123;\n        SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd\");\n        SimpleDateFormat sdf1 = new SimpleDateFormat(\"YYYY\");\n        System.out.println(sdf1.format(sdf.parse(\"2019-12-01\")));\n        System.out.println(sdf1.format(sdf.parse(\"2019-12-30\")));\n        System.out.println(sdf1.format(sdf.parse(\"2020-01-01\")));\n    &#125;\n&#125;\n// =============================\n// 2019\n// 2020\n// 2020\n\n因为有这样的情况，所以在日常开发的时候，如果把 y 写成了 Y，那就可能导致日期输出的结果不符合我们的预期。\n当我们要表示日期的时候，==一定要使用 yyyy-MM-dd 而不是 YYYY-MM-dd==，这两者的返回结果大多数情况下都一样，但是极端情况就会有问题了。\n十三、MySQL中count(列名)、count(常量)和count(*)的区别\n13.1 COUNT\nCOUNT(expr) ，返回 SELECT 语句检索的行中 expr 的值不为 NULL 的数量。结果是一个 BIGINT 值。\n如果查询结果没有命中任何记录，则返回 0。\n值得注意的是，COUNT(*) 的统计结果中，会包含值为 NULL 的行数。\n\n13.2 count(列名)、count(常量)和count(*)之间的区别列名、常量 和 * 这三个条件中，\n常量 是一个固定值，肯定不为 NULL，\n* 可以理解为查询整行，所以肯定也不为 NULL，\n列名 可能为NULL\n所以， COUNT( 常量 ) 和 COUNT(*) 表示的是直接查询符合条件的数据库表的行数。而 COUNT( 列名 ) 表示的是查询符合条件的列的值不为 NULL 的行数。\n13.3 COUNT(*) 优化​    优化与SQL引擎有关，MySQL中常用InnoDB和MyISAM。COUNT(*)的优化主要和“MyISAM 不支持事务，MyISAM 中的锁是表级锁；而InnoDB 支持事务，并且支持行级锁。”有关\n​    因为 MyISAM 的锁是表级锁，所以同一张表上面的操作需要串行进行，所以，MyISAM 做了一个简单的优化，那就是它可以把表的总行数单独记录下来，如果从一张表中使用 COUNT(*) 进行查询的时候，可以直接返回这个记录下来的数值就可以了，但前提是不能有 where 条件。\nMyISAM 数据库是表级锁，不会有并发的数据库行数修改，查询得到的行数是准确的。\n​    在 InnoDB 中，使用COUNT(*)要不可避免的扫表，在扫表的过程中做了一些优化：MySQL 会优先选择最小的非聚簇索引来扫表。所以，当我们建表的时候，除了主键索引以外，创建一个非主键索引还是有必要的。\nInnoDB中的优化前提是查询语句中不包含WHERE或GROUP BY等条件\n13.4 COUNT(*) 和COUNT(1)==对于 COUNT(1)和 COUNT(*)，MySQL 的优化是完全一样的!!==\n但是更建议使用COUNT(*)\nCOUNT(*)是 SQL92 定义的标准统计行数的语法\n13.5 COUNT( 字段 )工作原理：全表扫描，然后判断指定字段的值是不是为 NULL，不为 NULL 则累加。\n相比 COUNT( * )，COUNT( 字段 ) 多了一个步骤就是判断所查询的字段是否为NULL，所以他的性能要比 COUNT( * ) 慢。\n13.6 总结结论：建议使用COUNT(*)\n​    因为 COUNT(*) 是 SQL92 定义的标准统计行数的语法，所以 MySQL 对他进行了很多优化\n​    在 InnoDB 中 COUNT(*) 和 COUNT(1) 实现上没有区别，而且效率一样，但是COUNT( 字段 ) 需要进行字段的非 NULL 判断，所以效率会低一些。\n十四、Java值传递的误区和探索==首先讲结论：Java只有值传递，没有引用传递==\n声明：三个误区均为错误结论。本章就是要解释三个误区为什么错，错在哪里\n\n误区一：值传递和和引用传递，区分的条件是传递的内容，如果是个值，就是值传递；如果是个引用就是引用传递。\n误区二：Java存在引用传递。\n误区三：传递的参数如果是普通类型，那就是值传递；如果是对象，那就是引用传递。\n\n14.1 值传递和引用传递当我们调用一个有参函数的时候，会把实际参数(实参)传递给形式参数(形参)。但是，在程序语言中，这个传递过程中传递的两种情况，即值传递和引用传递。定义如下：\n\n值传递(pass by value)：指在调用函数时将实际参数复制一份传递到函数中，这样在函数中如果对参数进行修改，将不会影响到实际参数。\n引用传递(pass by reference)：指在调用函数时将实际参数的地址直接传递到函数中，那么在函数中对参数所进行的修改，将影响到实际参数。\n\n值传递和引用传递的根本区别是：值传递会创建副本，引用传递不创建副本\npublic class Test &#123;\n    public static void main(String[] args) &#123;\n        int i = 10;\n        pass(i);\n        System.out.println(\"In Main,i is \" + i);\n    &#125;\n    static void pass(int j) &#123;\n        j = 20;\n        System.out.println(\"In Pass,j is \" + j);\n    &#125;\n&#125;\n// ========================OUTPUT=============================\n// In Pass,j is 20\n// In Main,i is 10\n\n由上述实例，我们可以看出，当形参为基本类型时，Java的方法是值传递。\n(本实例仅展示值传递和引用传递的区别，不能因当前实例妄下结论：Java的方法均是值传递。但在下文会去证实这个结论)\n14.2 问题复现实例：\npublic class Test &#123;\n    public static void main(String[] args) &#123;\n        User han = new User();\n        han.setName(\"Han Xiaojie\");\n        han.setAge(22);\n        pass(han);\n        System.out.println(\"In Main,user is \" + user);\n    &#125;\n\n    static void pass(User user) &#123;\n        user.setName(\"SkyFroop\");\n        System.out.println(\"In Pass,user is \" + user);\n    &#125;\n&#125;\n\nclass User &#123;\n    private String name;\n    private Integer age;\n    /* 省略Setter/Getter和toString() */\n&#125;\n// ========================OUTPUT=============================\n// In Pass,user is User&#123;name='SkyFroop', age=22&#125;\n// In Main,user is User&#123;name='SkyFroop', age=22&#125;\n\n为什么这里的实参发生变化了？\n那是不是可以得出 “传递的参数如果是普通类型，那就是值传递；如果是对象，那就是引用传递”的结论呢？\n再看下面的例子：\npublic class Test &#123;\n    public static void main(String[] args) &#123;\n        String name = \"Han Xiaojie\"\n        pass(user);\n        System.out.println(\"In Main,name is \" + user);\n    &#125;\n\n    static void pass(User name) &#123;\n        name = \"SkyFroop\"\n        System.out.println(\"In Pass,name is \" + user);\n    &#125;\n&#125;\n// ========================OUTPUT=============================\n// In Pass,name is SkyFroop\n// In Main,name is Han Xiaojie\n\n同样传递了一个对象，但是原始参数的值并没有被修改。\n14.3 Java的值传递上面，我们举了三个例子，表现的结果却不一样，上面的概念没有错，只是代码展示了三种值传递的情况。\n回到14.2的第一个例子，从JMM层面来看：\n\n\n在参数传递过程中，实际参数的地址0x123456 拷贝给了形参user，只是，在这个方法中，并没有对形参本身进行修改，而是修改的形参持有的地址中存储的内容。也就是修改的并不是0x123456这个值，而是修改的这个引用下的成员变量的引用。因此这里仍然是值传递，只不过这个值，是一个地址。但由于这个地址是拷贝给了形参，因此仍然是值传递。\n如何看出来这里是 拷贝 给了形参呢？再看下面这个例子：\npublic class Test &#123;\n    public static void main(String[] args) &#123;\n        User han = new User();\n        han.setName(\"Han Xiaojie\");\n        han.setAge(22);\n        pass(han);\n        System.out.println(\"In Main,user is \" + user);\n    &#125;\n\n    static void pass(User user) &#123;\n        user = new User();\n        user.setName(\"SkyFroop\");\n        System.out.println(\"In Pass,user is \" + user);\n    &#125;\n&#125;\n\nclass User &#123;\n    private String name;\n    private Integer age;\n    /* 省略Setter/Getter和toString() */\n&#125;\n// ========================OUTPUT=============================\n// In Pass,user is User&#123;name='SkyFroop', age=22&#125;\n// In Main,user is User&#123;name='Han Xiaojie', age=22&#125;\n\n这里的流程如下：\n\nnew User()并没有对0x123456进行修改，而是又复制了一个 副本，开辟了一个新的空间。\n因此可以完全得出结论：值传递和引用传递的区别并不是传递的内容。而是实参到底有没有被复制一份给形参。在判断实参内容有没有受影响的时候，要看传的的是什么，如果你传递的是个地址，那么就看这个地址的变化会不会有影响，而不是看地址指向的对象的变化。\n14.4  拓展关于String，String是一个不可变类，从String的方法可以看到，所有对String的操作，都是在CopyOf一个新的String对象，如果尝试在pass中修改name，其实间接的创建了一个新的String对象，因此不会改变实参的值。\n","slug":"Java开发手册","date":"2022-02-19T16:00:00.000Z","categories_index":"","tags_index":"Java","author_index":"Aurora"},{"id":"637365ccb3eb125c455d5b7283ecb983","title":"Nginx简单应用及配置模板","content":"Nginx概述Nginx (engine x)是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务\nNginx常用命令cd /usr/local/nginx/sbin/\n./nginx #启动\n./nginx -s stop   # 停止\n./nginx -s quit   # 安全退出\n./nginx -s reload # 重新加载配置文件\nps auxlgrep nginx # 查看nginx进程\n\n\n\nNginx语法匹配规则语法规则： location [=||*|^~] /uri/ { … }\n\n= 开头表示精确匹配\n^~ 开头表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。\n~ 开头表示区分大小写的正则匹配\n~* 开头表示不区分大小写的正则匹配\n!~ 和 !~*分别为区分大小写不匹配及不区分大小写不匹配 的正则\n/ 通用匹配，任何请求都会匹配到。\n\n多个location配置的情况下匹配顺序为（参考资料而来，还未实际验证，试试就知道了，不必拘泥，仅供参考）：首先匹配 =，其次匹配^~, 其次是按文件中顺序的正则匹配，最后是交给 / 通用匹配。当有匹配成功时候，停止匹配，按当前匹配规则处理请求。\n例子，有如下匹配规则：\nlocation = / &#123;    # 精确匹配，必须是127.0.0.1/\n\n#规则A\n\n&#125;\n\nlocation = /login &#123;    # 精确匹配，必须是127.0.0.1/login\n\n#规则B\n\n&#125;\n\nlocation ^~ /static/ &#123;    # 非精确匹配，并且不区分大小写，比如127.0.0.1/static/js.\n\n#规则C\n\n&#125;\n\nlocation ~ \\.(gif|jpg|png|js|css)$ &#123;    # 区分大小写，以gif,jpg,js结尾\n\n#规则D\n\n&#125;\n\nlocation ~* \\.png$ &#123;     # 不区分大小写，匹配.png结尾的\n\n#规则E\n\n&#125;\n\nlocation !~ \\.xhtml$ &#123;   # 区分大小写，匹配不已.xhtml结尾的\n\n#规则F\n\n&#125;\n\nlocation !~* \\.xhtml$ &#123;\n\n#规则G\n\n&#125;\n\nlocation / &#123;  # 什么都可以\n\n#规则H\n\n&#125;\n\n那么产生的效果如下：\n\n访问根目录/， 比如http://localhost/ 将匹配规则A\n访问 http://localhost/login 将匹配规则B，http://localhost/register 则匹配规则H\n访问 http://localhost/static/a.html 将匹配规则C\n访问 http://localhost/a.gif, http://localhost/b.jpg 将匹配规则D和规则E，但是规则D顺序优先，规则E不起作用， 而 http://localhost/static/c.png 则优先匹配到 规则C\n访问 http://localhost/a.PNG 则匹配规则E， 而不会匹配规则D，因为规则E不区分大小写。\n访问 http://localhost/a.xhtml 不会匹配规则F和规则G，http://localhost/a.XHTML不会匹配规则G，因为不区分大小写。规则F，规则G属于排除法，符合匹配规则但是不会匹配到，所以想想看实际应用中哪里会用到。\n访问 http://localhost/category/id/1111 则最终匹配到规则H，因为以上规则都不匹配，这个时候应该是nginx转发请求给后端应用服务器，比如FastCGI（php），tomcat（jsp），nginx作为方向代理服务器存在。\n\n==注意:如果uri包含正则表达式，则必须要有~ 或者 ~*标识。==\n\n\n\n\n\n\n\n\n\n实际中常用\n#这里是直接转发给后端应用服务器了，也可以是一个静态首页\n\n# 第一个必选规则\n\nlocation = / &#123;\n\nproxy_pass http://tomcat:8080/index\n\n&#125;\n\n# 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项\n\n# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用\n\nlocation ^~ /static/ &#123;\n\nroot /webroot/static/;\n\n&#125;\n\nlocation ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ &#123;\n\nroot /webroot/res/;\n\n&#125;\n\n#第三个规则就是通用规则，用来转发动态请求到后端应用服务器\n\n#非静态文件请求就默认是动态请求，自己根据实际把握\n\n#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了\n\nlocation / &#123;\n\nproxy_pass http://tomcat:8080/\n\n&#125;\n\n#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。\n\n#这里是直接转发给后端应用服务器了，也可以是一个静态首页\n\n# 第一个必选规则\n\nlocation = / &#123;\n\nproxy_pass http://tomcat:8080/index\n\n&#125;\n\n# 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项\n\n# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用\n\nlocation ^~ /static/ &#123;\n\nroot /webroot/static/;\n\n&#125;\n\nlocation ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ &#123;\n\nroot /webroot/res/;\n\n&#125;\n\n#第三个规则就是通用规则，用来转发动态请求到后端应用服务器\n\n#非静态文件请求就默认是动态请求，自己根据实际把握\n\n#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了\n\nlocation / &#123;\n\nproxy_pass http://tomcat:8080/\n\n&#125;\n\n\n\n\n\n\n反向代理配置配置文件结构：\nhttp&#123;\n\t(http 配置)\n\t\n\tserver&#123;\n\t\tlisten  80;\n\t\tserver_name localhost;\n\t\t# 代理\n\t\tlocation / &#123;\n\t\t\t# xxxx\n\t\t\troot \n\t\t\tindex\n\t\t\tproxy_pass #代理服务\n\t\t&#125;\n\t\t\n\t\tlocation / &#123;\n\t\t\t# xxxx\n\t\t&#125;\n\t&#125;\n\t\n\tserver&#123;\n\t\tlisten  443;\n\t\tserver_name localhost;\n\t\t# 代理\n\t&#125;\n\t(......)\n&#125;\n\n\n\n\n\n负载均衡配置http&#123;\n\t(http 配置)\n\t\n\tupstream name&#123;\n\t\t# 负载均衡策略\n\t\t[轮询(default) | weight | ip_hash | fair]\n\t\t# 负载均衡服务器列表\n\t\tserver IP:Port weight = x;\n\t\t...\n\t&#125;\n\t\n\tserver&#123;\n\t\tlisten  80;\n\t\tserver_name localhost;\n\t\t# 代理\n\t\tlocation / &#123;\n\t\t\t# xxxx\n\t\t\troot \n\t\t\tindex\n\t\t\t# 代理服务\n\t\t\tproxy_pass http://[upstream起的name];\n\t\t&#125;\n\t\t\n\t&#125;\n\t\n&#125;\n\n\n\n\n分配策略：\n\n轮询(默认)：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除\nweight：weight代表权重，默认值为1，权重越高，被分配的客户端越多\nip_hash：每个请求按访问ip的hash结果分配，这样每个访客固定访问同一台后端服务器\nfair(第三方)：按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n\n动静分离配置Nginx动静分离简单来说就是把动态跟静态请求分开,不能理解成只是单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用Nginw.处理静态页面，Tomcat处理动态页面。动静分离从目前实现角度来讲大致分为两种：\n\n一种是纯粹把静态文件独立成单独的域名,放在独立的服务器上,也是目前主流推崇的方案;\n另外一种方法就是动态跟静态文件混合在一起发布，通过nginx来分开。\n\n通过location指定不同的后缀名实现不同的请求转发。通过expires参数设置，可以使浏览器缓存过期时间，减少与服务器之前的请求和流量。具体Expires定义:是给一个资源设定一个过期时间,也就是说无需去服务端验证,直接通过浏览器自身确认是否过期即可，所以不会产生额外的流量。此种方法非常适合不经常变动的资源。(如果经常更新的文件，不建议使用Expires来缓存)，我这里设置3d，表示在这3天之内访问这个URL，发送一个请求，比对服务器该文件最后更新时间没有变化，则不会从服务器抓取，返回状态码304，如果有修改，则直接从服务器重新下载，返回状态码200。\nhttp&#123;\n\t(http 配置)\n\t\n\tserver&#123;\n\t\tlisten  80;\n\t\tserver_name localhost;\n\t\t# 代理\n\t\tlocation /规则1 &#123;\n\t\t\t# xxxx\n\t\t\troot \n\t\t\tindex\n\t\t\t# 代理服务\n\t\t\tproxy_pass http://[upstream起的name];\n\t\t&#125;\n\t\t\n\t\tlocation /规则2 &#123;\n\t\t\t# xxxx\n\t\t\troot \n\t\t\tindex\n\t\t\t# 代理服务\n\t\t\tproxy_pass http://[upstream起的name];\n\t\t&#125;\n\t\t\n\t&#125;\n\t\n&#125;\n\n\n\n高可用集群\n\n需要两台Nginx服务器\n\n需要Keepalived\n\n```bashyum install keepalived -y\n\n- 在etc里面生成目录keepalived，有文件keepalived.conf\n\n  &#96;&#96;&#96;shell\n  # 全局配置\n  glocal_defs &#123;\n  \tnotification_email&#123;\n  \t\tacassen@firewall.loc\n  \t\tfailover@firewall.loc\n  \t\tsysadmin@firewall.loc\n  \t&#125;\n  \tnotification_email_from Alexandre.Cassen@firewall.loc\n  \tsmtp_server 主机IP\n  \tsmtp_counnect_timeout 30 \n  \trouter_id 主机名 # 访问到主机 &#x2F;etc&#x2F;hosts中编辑\n  &#125;\n  \n  # 检测脚本配置\n  vrrp_script chk_http_port &#123;\n  \tscript &quot;&#x2F;usr&#x2F;local&#x2F;src&#x2F;nginx_check.sh&quot;\n  \tinterval 2 # 检测脚本执行的间隔\n  \tweight 2 # 权重，设置当前服务器的一个权重\n  &#125;\n  \n  # 虚拟IP的配置\n  vrrp_instance VI_1&#123;\n  \tstate MASTER # 备份服务器上讲MASTER换成BACKUP\n  \tinterface ens33 # 网卡\n  \tvirtual_router_id 51 # 主备机的virtual_router id 必须相同\n  \tpriority 100 # 主备机取不同的优先级，主机值较大，备份机较小\n  \tadvert_int 1 # 时间间隔\n  \tauthtication &#123;\n  \t\tauth_type PASS\n  \t\tauth_pass 1111\n  \t&#125;\n  \tviretual_ipaddress &#123;\n  \t\t# VRRP H虚拟地址\n  \t&#125;\n  &#125;\n检测脚本nginx_check.sh\n# !/bin/bash\nA = `ps -C nginx -no-header |wc -l`\nif [$A -eq 0]; then\n\t/usr/local/nginx/sbin/nginx\n\tsleep 2\n\tif [`ps -C nginx --no-header |wc -l` -eq 0]; then\n\t\tkillall keepalived\n    fi\nfi\n\n\n需要虚拟ip\n\n\nNginx原理\n两个进程：worker&amp;master\n\n多个worker使用的是争抢机制\n\n\n\n\n\n\n\n\n\n一个 master 和多个 woker 有好处\n\n可以使用 nginx –s reload 热部署，利用 nginx 进行热部署操作\n每个 woker 是独立的进程，如果有其中的一个 woker 出现问题，其他 woker 独立的，继续进行争抢，实现请求过程，不会造成服务中断\n\n\n\n\n\n\n\n\n\n\n设置多少个 woker 合适\nNginx和Redis类似都采用了IO多路复用机制，每个worker都是一个独立的进程，但每个进程里面只有一个主线程，通过异步非阻塞的方式来处理请求。每个worker的线程可以把一个CPU的i性能发挥到极致。所以worker数和服务器的CPU数相等是最合适的。设少了会浪费CPU，多了会导致CPU频繁的上下文切换\n\n\n\n\n\n\n\n\n\n连接数 worker_connection\n这个值表示每个worker进程所能建立连接的最大值，所以，一个Nginx能建立的最大连接数，应该是worker_connections * worker_processes。这里所说的是最大连接数。\n\n对于HTTP请求本地资源来说，能够支持的最大并发数量是 worker_connections * worker_processses，\n如果是支持http1.1 的浏览器每次访问要占两个连接，所以普通的静态访问最大并发数是: worker_connections * worker_processes / 2\n如果是HTTP作为反向代理来说，最大并发数量应该是worker_connection * worker_processes / 4\n\n因此作为反向代理服务器，每个并发会与客户端的连接和与后端服务的连接，会占用两个连接\n[解决办法]阿里云服务器80端口被占用解决方案fuser -k 80/tcp命令停止阿里云的占用进程\n","slug":"Nginx简单应用","date":"2021-04-30T16:00:00.000Z","categories_index":"","tags_index":"运维","author_index":"Aurora"},{"id":"65246f4baaabadb30d023b3b91b63fa2","title":"Linux快速上手","content":"Linux基础\n\nKali Linux ： 安全渗透测试使用\n开机登录开机会启动许多程序。它们在Windows叫做”服务” ( service )，在Linux就叫做”守护进程” ( daemon ) 。开机成功后，它会显示一个文本登录界面，这个界面就是我们经常看到的登录界面，在这个登录界面中会提示用户输入用户名，而用户输入的用户将作为参数传给login程序来验证用户的身份，密码是不显示的，输完回车即可！一般来说，用户的登录方式有三种:\n\n命令行登录\nssh登录\n图形界面登录\n\n最高权限账户为root，可以操作一切!\n关机sync # 将数据由内存同步到硬盘\nshutdown # 关机\nshutdown -h 10 # 10分钟后关机\nshutdown -h 20:25 # 系统会在今天的20:25关机\nshutdown -h +10 #\nshutdown -r now # 立马重启\nreboot # 同上\nhalt # 关闭系统\n\n\n\n系统的目录结构==一切皆文件==\n根目录: /所有的文件都在这个目录下\n\n\n/bin : Binary的缩写，这个目录存放的是最常用的命令\n/boot : 这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件及镜像文件(不要动)\n/dev : dev时Device(设备)的缩写，存放的时Linux的外部设备，再Linux中访问设备的方式和访问文件的方式是相同的\n/etc : 用来存放所有的系统管理所需要的配置文件和子目录\n/home : 用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录是以用户的帐号命名的\n/lib : 存放着系统最近本的动态连接共享库，起作用类似于Windows里的DLL文件(不要动)\n/lost+found : 一般情况下是孔的，当系统非法关机后，这里会存放一些文件(不要动)\n/media : Linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux会把识别的设备挂载到这个目录下\n/mnt : 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载到/mnt/上，然后进入该目录就可以查看光驱里的内容了\n/opt : 给主机额外安装软件所摆放的目录，默认是空的\n/proc : 虚拟目录，是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息\n/root : 系统管理员，也称作超级权限这的用户主目录\n/sbin  : s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序\n/srv : 该目录存放一些服务启动之后需要提取的数据\n/sys : 是Linux2.6内核的很大的变化，目录下安装了2.5内核新出现的一个文件系统sysfs\n/tmp : 用来存放一些临时文件(用完即丢的文件可以放在这个目录下)\n/usr : ==非常重要的目录==，用户的很多应用程序和文件都放在这个目录下，类似于Windos下的program Files目录\n/usr/bin : 系统用户使用的应用程序\n/usr/sbin : 超级用户使用的比较高级的管理程序和系统守护程序\n/usr/src : 内核源代码默认的放置目录\n/var : 存放着不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下，包括各种日志文件\n/run : 是一个临时文件系统，存储系统启动以来的信息，当系统重启时，这个目录下的文件应该被删掉或清除。\n/www : 存放服务器网站相关的资源，比如环境、网站的项目\n\n\n\n\n\n\n\n\n\n\n基本属性\nLinux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件)的权限做了不同的规定。\n在Linux中我们可以使用ll或者ls -l命令来显示一个文件的属性以及文件所属的用户和组，如︰\n实例中，boot文件的第一个属性用”d”表示。”d””在Linux中代表该文件是一个目录文件。在Linux中第一个字符代表这个文件是目录、文件或链接文件等等:\n\n当为[ d]则是目录\n当为[-]则是文件;\n若是[Ⅰ]则表示为链接文档(link file ) ;\n若是[ b ]则表示为装置文件里面的可供储存的接口设备（可随机存取装置);\n若是[ c]则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。\n\n接下来的字符中，以三个为一组，且均为『rwx』的三个参数的组合。其中，[ r ]代表可读(read)、[ w ]代表可写(write)、[ x]代表可执行(execute)。要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现减号[ - ]而已。每个文件的属性由左边第一部分的10个字符来确定（如下图）∶\n\n\n\n\n\n\n\n\n\n\n修改文件属性\n\n更改文件属组\n\nchgrp [-R] 数组名 文件名\n\n\n更改文件属主，也可以同时更改文件属组\n\nchown [-R] 属主名 文件名\nchown [-R] 属主名: 属组名 文件名\n\n\n更改文件的九个属性\n\nchmod [-R] xyz 文件或目录\n\nLinux文件属性有两种设置方法，一种是数字，一种是符号。Linux文件的基本权限就有九个，分别是owner/group/others三种身份各有自己的read/write/execute权限。先复习一下刚刚上面提到的数据∶文件的权限字符为︰『-wxrwXrwx』，这九个权限是三个三个一组的!其中，我们可以使用数字来代表各个权限，各权限的分数对照表如下:\nr:4     w:2   x:1\n可读可写不可执行   rw-   6\n可读可写可执行     rwx   7\nchomd 777 就代表所有用户都赋予可读可写可执行\n\n\n\n文件内容查看\ncat 由第一行开始显示文件内容\n\ntac 从最后一行开始显示，(冷知识 : tac是cat的倒着写)\n\nnl 显示的时候，顺便输出行号\n\nmore 一页一页的显示文件内容(空格表示翻页，enter表示向下看一行，:f可以查看行号)\n\nless 与 more类似，但是比more更好的是可以往前翻页(上下键代表上下翻动页面，退出使用q命令，/ 可以向下查询，?向上查询 n继续查询下一个,N查询上一个)\n\nhead 只看头几行 head -n 行数\n\ntail只看尾部几行 tail -n 行数\n\n\nifconfig 查看网络配置\nVim编辑器所有的Unix Like系统都会内建vi文书编辑器，其他的文书编辑器则不一定会存在。连vim的官方网站(http://www.vim.org)自己也说vim是一个程序开发工具而不是文字处理软件。\nvim键盘图:\n \n\n\n\n\n\n\n\n\n\n三种使用模式\n基本上vivim 共分为三种模式，分别是命令模式(Command mode )，输入模式( lnsert mode)和底线命令模式( Lastline mode )。这三种模式的作用分别是:命令模式用户刚刚启动vi/vim，便进入了命令模式。此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。以下是常用的几个命令∶\n\ni : 切换到输入模式，以输入字符。\nx : 删除当前光标所在处的字符。\n:  :切换到底线命令模式，以在最底一行输入命令。如果是编辑模式需要先退出编辑模式。若想要编辑文本∶启动Vim，进入了命令模式，按下i，切换到输入模式。命令模式只有一些最基本的命令，因此仍要依靠底线命令模式输入更多命令。\n\n输入模式\n在命令模式下按下i就进入了输入模式\n在输入模式中：\n\n字符按键以及shift组合，输入字符\nEnter，回车键，换行\nBackspace删除光标的前一个字符\nDel : 删除键，删除光标后的一个字符\n方向键，移动光标\nHome/End : 以动光标到行首/行尾处\nPage Up / Page Down : 上/下翻页\nInsert : 切换光标为输入/替换模式，光标变成竖线/下划线\nESC : 退出输入模式，切换到命令模式\n\n底线命令模式\n在命令模式下按下:（英文冒号)就进入了底线命令模式。底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。在底线命令模式中，基本的命令有（已经省略了冒号)︰\n\nq 退出程序\nw 保存文件\nESC 退出底线命令模式\n\n\n\n\n\n\n\n\n\n\n Vim按键说明\n命令模式\n\n\n\n\n\n输入模式\n\n底线命令模式\n只需要记前几个\n\n==set nu== : 设置行号,代码中十分常用\n帐号管理\n\n\n\n\n\n\n\n\n useradd 添加用户\nuseradd -选项 用户名\n-m ： 自动创建这个用户的主目录 /home/用户名\n-g : 用户组，指定用户所属的用户组\n-G : 用户组，用户组 指定用户所属的附加组\n理解一下本质:Linux中一切皆文件，这里的添加用户说白了就是往某一个文件中写入用户的信息了! 位置：/etc/passwd\n\n\n\n\n\n\n\n\n\nuserdel 删除用户\nuserdel -r 用户名 ： 一般会将用户目录一并删除\n\n\n\n\n\n\n\n\n\nusermod 修改用户\nusermod 对应修改的内容  修改那个用户\n修改完毕后以配置文件内容为准\n\n\n\n\n\n\n\n\n\n切换用户\n\n切换用户的命令为: su username 【username是 用户名 】\n\n从普通用户切换到root用户，还可以使用命令 : sudo su\n\n在终端输入exit或logout或使用快捷方式ctrl+d，可以退回到原来用户，其实ctrl+d也是执行的exit命令\n\n在切换用户时，如果想在切换用户之后使用新用户的工作环境，可以在su和username之间加 - ，例如:【su - root】\n\n$表示普通用户\n\n#表示超级用户，也就是root用户\n\n\n\n\n\n\n\n\n\n\n\n\n\n密码设置问题\n通过root创建用户的时候要配置密码\nLinux上输入密码不会显示\npasswd 用户名 : 修改密码\n\n\n\n\n\n\n\n\n\n锁定账户\npasswd -l 用户名 : 锁定之后这个用户就不能登录了\n用户组管理属组\n每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理(开发、测试、运维)。不同Linux系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。\n\n\n\n\n\n\n\n\n\ngroupadd 添加用户组\n创建完用户组后，可以得到一个组的id，这个id是可以指定的，若不指定就是自增1\n\n\n\n\n\n\n\n\n\ngroupdel 删除用户组\n删除对应的文件\n\n\n\n\n\n\n\n\n\ngroupmod 修改用户组\ngroupmod -g id -n 新名字 原名字 \n\n\n\n\n\n\n\n\n\n切换用户组\nnewgrp 用户组\n磁盘管理\ndf : 列出文件系统整体的磁盘使用量\ndu : 检查磁盘空间使用量\n-a 可以看到隐藏文件以及子目录\n\n\n\n\n\n\n\n\n\n\n\n\n挂载外部设备\nmount  外部设备  /mnt/\n挂载到mnt目录下，来实现访问\n\n\n\n\n\n\n\n\n\n卸载外部设备\numount \n-f : 强制卸载\n进程管理\nps: 查看当前系统中正在执行的各种进程的信息\n-a 显示当前终端运行的所有的进程信息\n-u 以用户的信息显示进程\n-x 显示后台运行进程的参数\n-aux 可以查看所有的进程\n-ef 可以查看到父进程的信息\n看父进程我们一般可以通过目录树的结构来查看：pstree -p 显示父id  -u 显示用户组\n\n\n\n\nkill : 结束进程\n-(id) 结束进程id为(id)的进程\n\n\n\n其他常用命令# | 在Linux中，叫做管道符 \n# grep 查找文件中符合条件的字符串\n\n\n\nLinux链接 (了解)分为两种：硬链接、软链接\n硬链接 : A——B 假设B是A的硬链接，那么他们两个指向了同一个文件，允许一个文件拥有多个路径，用户可以通过这种机制建立硬链接到一些重要文件上，防止误删\n软链接 : 类似Windows下的快捷方式，删除了源文件，快捷方式也访问不了\n创建链接: ln 命令\ntouch可以创建文件\necho 输入字符串\n环境安装安装软件一般有三种方式：rpm、解压缩安装、yum在线安装\nJDK安装开发Java程序必要的环境\n\n下载JDK rpm包\n\n安装java环境\n# 检查当前是否存在Java环境。\n##卸载方式: rpm -qa|grep jdk  查出版本号\n##########  rpm -e --nodeps 上述版本号\n\n rpm -ivh jdk-8u301-linux-x64.rpm # rpm安装\n配置环境变量\n# 在/etc/profile文件中修改\nJAVA_HOME=/usr/java/jdk1.8.0_301-amd64\nCLASSPATH=%JAVA_HOME%/lib;=%JAVA_HOME%/jre/lib\nPATH=$JAVA_HOME/bin;$JAVA_HOME/jre/bin\nexport PATH CLASSPATH JAVA_HOME # 导出\n让配置生效\nsource /etc/profile\n\n\n\n\n\n\n\n\n\n\n发布项目\n\n打包发布\n开放防火墙端口\n重启防火墙\n\n#开启防火墙端口\nfirewa1l-cmd --zone=public --add-port=9000/tcp --permanen\n#重启防火墙\nsystemctl restart firewa11d.service\n#查番所有开启的端口，如果是阿里云，需要配置安全组规则!\nfirewa1l -cmd --list-ports\n\n\n\n\nTomcat安装\n下载Tomcat-Linux压缩包\n\n解压\ntar -Zxvf apache-tomcat-9.0.22.tar.gz\n运行！\n\n\n# 执行 ./startuo.sh\n# 停止 ./shotdown.sh\n\n\n\n\n如果对应防火墙端口已经开放，并且Aliyun服务器端口也已开放\n防火墙的配置命令#查看firewa11服务状态\nsystemctl status firewalld\n#开启、重启、关闭、firewalld.service服务\n#开启\nservice firewalld start\n#重启\nservice firewalld restart\n#关闭\nservice firewalld stop\n#查看防火墙规则\nfirewall-cmd --list-all  #查看全部信息\nfirewall-cmd --1ist-ports  #只看端口信息\n#开启端口\n开端口命令: firewall-cmd --zone=public --add-port=8080/tcp --permanent\n重启防火墙: systemctl restart firewalld.service\n命令含义:\n--zone # 作用域\n--add-port=80/tcp # 添加端口，格式为:端口I/通讯协议\n--permlanent # 永久生效，没有此参数重启后失效\n\n\n\n\n\n域名解析后,如果端口是80 - http或者443-https可以直接访问,如果是9000 8080 ,就需要通过Apcahe或者Nginx做一下反向代理即可,配置文件即可\nDocker安装官方文档：https://docs.docker.com/engine/install/centos/\n\n\n\n\n\n\n\n\n\n安装\n\n检测CentOS版本 cat /etc/redhat-release\n安装准备环境yum -y install gcc\n卸载旧版本\n安装新版本(依据官方文档步骤)\n\n拓展：VMware本地网络配置\n\n桥接模式：虚拟机可以直接访问到主机\n==注意：桥接模式一定要桥接到正确的网卡==\n以下是静态网络配置\n\n图形化界面配置网络：\nnm-connection-editor\n\n\n\n\n\n","slug":"Linux","date":"2020-12-31T16:00:00.000Z","categories_index":"","tags_index":"运维","author_index":"Aurora"},{"id":"b25d7490c19b8f0a23ec631e32700222","title":"OpenStack 部署","content":"架构Nova调用KVM(libvirt)服务提供虚拟化\nNuetron提供网络服务\nGlance 提供镜像模板\ncinder 提供存储服务\nCellometer 监控-&gt; Cinder、Nova、Glance\nSwift 对象存储\nHeat 编排服务：启动服务器集群\n存储类型：块存储、文件存储、对象存储\n\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp        0      0 0.0.0.0:9696            0.0.0.0:*               LISTEN      13960/server.log    \ntcp        0      0 0.0.0.0:6080            0.0.0.0:*               LISTEN      1167/python2        \ntcp        0      0 0.0.0.0:8774            0.0.0.0:*               LISTEN      20411/python2       \ntcp        0      0 0.0.0.0:9191            0.0.0.0:*               LISTEN      20343/python2       \ntcp        0      0 0.0.0.0:8775            0.0.0.0:*               LISTEN      1162/python2        \ntcp        0      0 0.0.0.0:25672           0.0.0.0:*               LISTEN      1172/beam.smp       \ntcp        0      0 192.168.10.43:3306      0.0.0.0:*               LISTEN      1705/mysqld         \ntcp        0      0 192.168.10.43:11211     0.0.0.0:*               LISTEN      5458/memcached      \ntcp        0      0 127.0.0.1:11211         0.0.0.0:*               LISTEN      5458/memcached      \ntcp        0      0 192.168.10.43:2379      0.0.0.0:*               LISTEN      1212/etcd           \ntcp        0      0 0.0.0.0:9292            0.0.0.0:*               LISTEN      1170/python2        \ntcp        0      0 192.168.10.43:2380      0.0.0.0:*               LISTEN      1212/etcd           \ntcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      657/rpcbind         \ntcp        0      0 0.0.0.0:4369            0.0.0.0:*               LISTEN      1/systemd           \ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1174/sshd           \ntcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1154/cupsd          \ntcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1590/master         \ntcp        0      0 127.0.0.1:6010          0.0.0.0:*               LISTEN      45180/sshd: root@pt \ntcp6       0      0 :::5000                 :::*                    LISTEN      5540/httpd          \ntcp6       0      0 :::5672                 :::*                    LISTEN      1172/beam.smp       \ntcp6       0      0 :::8778                 :::*                    LISTEN      5540/httpd          \ntcp6       0      0 ::1:11211               :::*                    LISTEN      5458/memcached      \ntcp6       0      0 :::111                  :::*                    LISTEN      657/rpcbind         \ntcp6       0      0 :::80                   :::*                    LISTEN      5540/httpd          \ntcp6       0      0 :::22                   :::*                    LISTEN      1174/sshd           \ntcp6       0      0 ::1:631                 :::*                    LISTEN      1154/cupsd          \ntcp6       0      0 ::1:25                  :::*                    LISTEN      1590/master         \ntcp6       0      0 ::1:6010                :::*                    LISTEN      45180/sshd: root@pt\n\n\n\n\n\n\n\n\n\n\n\n状态检查openstack token issue 检查KeyStone是否正常\nopenstack image list 检查glance是否正常\nopenstack compute service list 检查nova是否正常\n域、项目、用户\n","slug":"OpenStack","date":"2020-11-30T16:00:00.000Z","categories_index":"","tags_index":"云计算","author_index":"Aurora"},{"id":"67b998ca17f2f84ffc9f8907bdfa2b3c","title":"Redis 基础","content":"零、NoSQL概述Not Only SQL 不仅仅是SQL，泛指非关系型数据库，随着Web2.0互联网的诞生，传统的关系型数据库很难对付。尤其是超大规模的高并发的社区。\n关系型数据库：表格，行和列 （POI可以操作Excel）\n非关系型数据库：很多的数据类型如个人信息、社交网络、地理位置。这些数据类型的储存不需要一个固定的格式！不需要多余的操作就可以横向扩展。Map&lt;String,Object&gt;可存万事万物，使用键值对来控制\n0.1 NoSQL特点解耦！\n\n方便扩展（数据之间没有关系，很好扩展）\n大数据量高性能(Redis 一秒写8万次，读取11万次 ，NoSQL的缓存记录级，是一种细粒度的缓存，性能会比较高)\n数据类型是多样型的。不需要事先设计数据库，随去随用\n\n\n\n\n\n\n\n\n\n\n传统RDBMS和NoSQL的关系\n传统的RDBMS：\n\n结构化组织\nSQL\n数据和关系都存在单独的表 row col\n操作，数据定义语言\n严格的一致性 ACID\n基础的事务操作\n…\n\nNoSQL：\n\n不仅仅是数据\n没有固定的查询语言\n键值对存储，列存储，文档存储，图形数据库(社交关系)\n最终一致性，CAP定理和BASE理论(异地多活)  —&gt; 初级架构师\n高性能，高可用，高可扩展性\n\n\n\n\n\n\n\n\n\n\n了解\n3V + 3高\n大数据时代的3V：\n\n主要是描述问题的\n海量Volume\n多样Variety\n实时Velocity\n\n大数据时代的3高：\n\n高并发\n高可拓 \n高性能\n\n0.2 阿里巴巴演进分析\n敏捷开发、极限编程\n\n\n大型互联网应用问题：\n\n数据类型太多了\n数据源繁多，经常重构。\n数据要改造，需要大面积改造\n\n解决问题：\n\n统一数据服务层\n\n0.3 NoSQL的四大分类\n\n\n\n\n\n\n\n\nKV键值对\n\n新浪： Redis\n美团：Redis+Tair\n阿里、百度：Redis + memecache\n\n\n\n\n\n\n\n\n\n\n文档型数据库(BSON格式)\n\nMongoDB\nMongoDB是一个基于分布式文件存储的数据库，C++编写，主要用来处理大量的文档\nMongoDB是一个介于关系型数据库和非关系型数据库中间的产品，是非关系型数据库中功能最丰富，最像关系型数据库的\n\n\nConchDB\n\n\n\n\n\n\n\n\n\n\n列存储数据库\n\nHBase\n分布式文件系统\n\n\n\n\n\n\n\n\n\n\n图关系数据库\n\n不是存图形的，放到是关系，比如：朋友圈社交网络，广告推荐。\nNeo4j ， InfoGrid;\n\n\n0.4 文档内容：\nNoSQL讲解\n阿里巴巴架构演进\nNoSQL数据模型\nNoSQL四大分类\nCAP\nBASE\n五大基本数据类型\nString\nList\nSet\nHash\nZset\n\n\n三种特殊数据类型\ngeo\nhyperloglog\nbitmap\n\n\nRedis配置\nRedis持久化\nRDB\nAOF\n\n\nRedis事务操作 -包含CAP和BASE\nRedis实现订阅发布\nRedis主从复制\nRedis哨兵模式：现在公司中所有的集群都在用哨兵模式\n缓存穿透及解决方案\n缓存击穿及解决方案\n缓存雪崩及解决方案\n基础API之Jedis详解 ( Redis 底层)\nSpringBoot集成Redis操作\nRedis分析\n\n一、Redis 入门1.1 Redis概述\n\n\n\n\n\n\n\n\nRedis是什么?\nRedis ( Remote Dictionary Server )，远程字典服务\n是一个开源的使用ANSIC语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的APl。\n\n\n\n\n\n\n\n\n\nRedis能干什么\n\n内存存储、持久化，内存中是断电即失，所以持久化很重要(RDB、AOF)\n效率高，可以用于高速缓存\n发布订阅系统\n地图信息分析\n计时器、计数器（浏览量！）\n….\n\n\n\n\n\n\n\n\n\n\n特性\n\n多样的数据类型\n持久化\n集群\n事务\n…\n\n\n\n\n\n\n\n\n\n\n学习中需要用到的东西\n\nRedis官网：redis.io \nRedis中文网：redis.cn\n\n注意！Windows在Github上下载，但是已经停更很久了。\nRedis建议在Linux上学习和使用\n1.2 Windwos安装\n下载：https://github.com/dmajkic/redis\n解压到电脑上的环境目录下即可\n开启Redis，开启服务即可\n默认端口为6379\n使用Redis客户端连接Redis\n连接成功检验\n\n\n虽然Windows 版本的Redis操作十分简单，但Redis官方推荐我们使用Linux！ \n1.3 Linux安装\n官网下载安装包\n\n解压安装包，程序一般放在 /otp目录下\n\n进入解压后的文件夹，可以看到redis的配置文件：redis.conf\n\n安装基本的环境安装\nyum install gcc-c++\n\nmake\n\n检查make是否讲所需环境已经装配完成\n\n\n\n5.redis的默认安装路径： /usr/local/bin/ \n6.将redis配置备份一份，复制到bin目录下的一个自命名的文件夹内\nmkdir SkyFroopConfig\n\ncp /opt/redis-6.2.5/redis.conf SkyFroopConfig/\n\n7.启动：\n\n默认不是后台启动，修改配置文件：改为yes\n通过指定的配置文件启动服务：运行redis-server SkyFroopConfig/redis.conf\n运行redis-cli -p 端口号 运行\n测试连接\n\n8.查看Redis的服务是否开启\nps -ef|grep redis\n\n\n9.关闭Redis服务\n\n10.后面会使用单机多Redis启动集群测试！\n1.4 测试性能redis-benchmark是一个压力测试工具\nredis-benchmark 命令\n\n# 测试 100个并发连接 10万个请求\nredis-benchmark -h localhost -p 6379 -c 100 -n 100000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n如何查看分析\n\n10w并发在2.20 秒完成\n100个并发客户端\n每次只写三个字节\n只有一台服务器在处理请求(单机性能)\n每秒处理45351个请求\n\n1.5 基础知识Redis默认有16个数据库\n\n默认使用的是第0个，可以使用Select进行切换 Select [0,15]\n部分数据库操作语法SELECT [index] #选择第几个数据库\nDBSIZE # 数据库内容大小\nkeys * # 查看数据库所有的key\nflushdb # 清空当前库\nflushall # 清空所有库\n\n\n\n\n\n\n\n\n\n\n\n\nRedis是单线程的\nRedis是很快的，官方表示，Redis是基于内存操作，CPU不是Redis的性能瓶颈，Redis的瓶颈是根据机器的内存和网络带宽，可以使用单线程实现，就是用单线程了。\nRedis是 ANSI C语言写的，官方数据为100000+的APS，完全这个不比同样使用Key-Vale的Memecache差\n那么Redis是单线程为什么还这么快？\n误区：\n\n误区1：高性能的服务器一定是多线程的？\n误区2：多线程(CPU)一定比单线程效率高？ 多线程CPU上下文会切换会耗时。速度：CPU &gt; 内存 &gt; 硬盘\n\n核心：\nRedis是将所有的数据全部放在内存中的，所以说使用单线程去操作效率就是最高的。对于内存系统来说，如果没有上下文切换，效率就是最高的。多次读写都是在一个CPU上的，在内存情况下，这就是最佳方案。\n二、Redis的数据类型2.1 五大基本数据类型\n\n\n\n\n\n\n\n\n官网文档\n\n即Redis是一个开源（BSD许可)的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。它支持多种类型的数据结构，如字符串( strings )，散列( hashes )，列表( lists )，集合( sets )，有序集合( sorted sets ）与范围查询，bitmaps ，hyperloglogs和地理空间 ( geospatial）索引半径查询。Redis 内置了 复制( replication )，LUA脚本( Lua scripting )，LRU驱动事件( LRU eviction )，事务( transactions）和不同级别的磁盘持久化( persistence)，并通过Redis哨兵( Sentinel)和自动分区( Cluster )提供高可用性( high availability )。\nRedis-KeyEXISTS [value] # 是否存在某个值\nMOVE [key-value] # 移除某个值\nEXPIRE [key] [seconds] # 设置有效时间\nTTL [key] # 查看key的剩余有效时间\nTYPE [key] # 查看当前key的类型\n\n\n\nString(字符串)SET [key-value] # 设置值\nGET [key] # 获得值\nAPPEND [key] [\"String\"] # 在key字段后追加字符串,如果这个字符串不存在则会新建,相当于SET\n\nSTRLEN [key] # key字符串的长度\nINCR [key] # 自增1\nDECR [key] # 自减1\nINCRBY [key] [value] # 令key自增步长为value\n\nGETRANGE [key] [start] [end] # 取key的start到end区间的值(闭区间)，end为-1时，就是全部值\nSETRANGE [key] [offset] [value] # 从offset后改成value，替换指定位置开始的字符串\n\nSETEX [key] [seconds] [value]# 设置过期时间，过期时间为seconds\nSETNX [key-value] # 如果不存在再设置 , 在分布式锁中常常会使用，如果key不存在才会创建，否则失败\nMSET [key-value ...] # 一次性设置多个值\nMGET [key ...] # 一次获取多个值\nMSETNX [key-value ...] # 是一个原子性的操作，要么一起成功，要么一起失败 \n\n####对象如何储存\nSET user:1 &#123;name:zhangsan,age:3&#125; # 设置一个user:1对象，值为JSON字符串来保存一个对象\n\n#### 这里的key是一个巧妙的设计 : user:&#123;id&#125;:&#123;filed&#125;\nMSET user:1:name zhangsan\nMSET user:1:age 2\n\n#组合命令\nGETSET [key-value] # 先get再set.如果存在值，获取原来的值并设置新的值\n\n\n\n\n使用场景：value除了是字符串之外，还可以是数字\n\n计数器\n统计多单位的数量\n粉丝数\n对象缓存存储\n\nList(列表)在Redis里面，可以把List做成栈、队列、阻塞队列\n所有的list命令基本都是L开头的\nLPUSH [key] [element ...] # 放入一个或多个值，插入到列表的头部\nRPUSH [key] [element ...] # 放入一个或多个值，插入到列表的尾部\nLRANGE [key] [start] [stop] # 取start到stop的值，stop为-1时，取全部值\nLSET [key] [index] [value] # 将下标为index的值替换为value，如果key中的index不存在，则ERROR，可以用其修改某个值，不能插入值！\nLinsert [key] [BEFORE|AFTER] [pivot] [value] # 在值为\"pivot\"的(前面|后面)添加一个value \n\nLPOP [key] [count] # 移除从头部起第count个值，缺省count时默认第一个值\nRPOP [key] [count] # 移除从尾部起第count个值，缺省count时默认第一个值\nLREM [key] [count] [element] # 移除指定的值。移除key中的count个element值。\n\nLINDEX [key] [index] # 通过下标获得list中的某一个值\nLLEN # 获取list长度\n\nTRIM [key] [start] [stop] # 修剪key从start到stop的值，可以做一个截断操作，截取指定的区间内的值，但是这个list已经被改变了，只剩下截取的元素了\n\nRPOPLPUSH [source] [destination] # 将source的尾部第一个值，放入新的列表destination的头部\n\n\n\n\n\n\n\n\n\n\n\n\n小结\n实际上是一个链表，before Node ，left，right都可以插入值\n如果key不存在就创建新的链表\n如果key存在就新增内容\n如果移除了key，所有的value都会消失\n用途：\n\n消息队列：Lpush Rpop  \n 栈 ： Lpush Lpop\n\nSet(集合)set中的值是不能重复的\nSadd [key] [member] # set集合中插入member\n\nSrem [key] [member] # 移除集合key中的member值\nSpop [key] # 随机删除集合中的元素\nSmove [source] [destination] [element] # 将element值从集合source移动到集合destination\n\nSMEMBERS [key] #查看key的所有值\nSISMEMBER [key] [member] # 查看key中是否包含member值\nSRANDMEMBER [key] [value] # 随机抽出集合key中value个元素\n\nScard [key] # 获取集合key中的元素个数\n\n#### 微博、B站，共同关注！(并集)\n#### 数字集合类\n# - 差集\n# - 交集\n# - 并集\nSDIFF [key ...] # key1 key2 ... 的差集\nSINTER [key ...] # key1 key2 ... 的交集。 共同好友的实现\nSUNION [key ...] # key1 key2 ... 的并集\n\n\n\n\n\n\n\n\n\n\n\n用途\n共同关注的实现：将用户所有关注的人放在一个set集合中，将它的粉丝也放在一个集合中\n共同关注，共同爱好，二度好友，推荐好友(六度分隔理论)\nHash(哈希)Map集合，key-map 这个值是一个map集合\nHSET [key] [field-value] # 在哈希集合key中放入字段field及其值value\nHGET [key] [field] # 取出哈希集合key中的field字段\nHMSET [key] [field] [value] [filed value ...] # set多个key-value\nHMGET [key] [field ...] # 获取哈希集合key的多个field字段\nHGETALL # 获取全部的数据\n\nHDEL [key] [field ...] # 删除哈希集合key中的field字段\nHLEN [key] # 获取哈希集合key的字段数量\nHEXISTS [key] [field] # 判断哈希集合key中是否存在field字段\nHKEYS [key] # 只获得所有的field\nHVALS [key] # 只获得所有的value\n\n####Hash的自增 自减\nHINCRBY [key] [field] [increment] # 哈希集合key的field字段自增increment\n\nHSETNX [key] [field] [value] # 如果不存在则可以设置，如果存在则不能设置\n\n\n\n\n\n\n\n\n\n\n\n应用\n变更的数据user name age等尤其是用户信息制类的，经常变动的信息！\nHash更适合对象的存储\nString更适合字符串存储\nZset(有序集合)在Set的基础上，增加了一个值\nZadd [key] [NX|XX] [GT|LT] [CH] [INCR] [score member ...] # \nZRANGEBYSCORE [key] [min] [max] [withscores] [limit offset count] #\n#inf 代表无穷\nZRANGEBYSCORE [key] -inf +inf # 显示全部用户，从小到大\nZRANGEBYSCORE [key] -inf +inf withscores # 显示全部用户，从小到大,并附带score值\nZREVRANGE [key] [start] [stop] [withscores] # 排序\nZREVRANGE [key] 0 -1 [withscores] # 从大到小\nZrem [key] [member ...] # 移除有序集合key中的member元素\nZcard [key] # 获取有序集合key中的个数\n\nZcount [key] [min] [max] # 获取Key中指定区间的成员数量\n\n\n\n\n\n\n\n\n\n\n\n\n案例思路\nset 排序 存储班级成绩表，工资表排序\n普通消息：1.重要消息 2.带权重进行判断\n排行榜应用Top榜\n2.2 三大特殊数据类型geospatial 地理位置\n\n\n\n\n\n\n\n\n用途\nRedis的Geo：可以推算地理位置的信息，两地之间的距离，方圆几里内的人 \n\n朋友的定位\n\n附近的人\n\n打车距离计算\n\n\n可以查询一些测试数据\n# 添加地理位置，规则：两级无法直接添加，我们一般会下载城市数据，直接通过Java程序一次性导入\n# 参数: key 纬度 经度 名称\nGEOADD\n# 获取指定的城市的精度和维度\n# 参数: key element\nGEOPOS\n# m：米  km：千米  mi：英里 ft：为英尺。\n# 参数: [key] [element1] [element2] [unit]\nGEODIST\n# 以一个经纬度为中心，半径radius内的元素\n# 参数: key 经度 纬度 radius withdist(显示到中心距离的位置) withcoord(显示他人的定位信息) count\nGEORADIUS\n# 以一个元素坐标为中心，半径radius内的元素\n# 参数: key element radius withdist withcoord count\nGEORADIUSBYMEMBER\n# 了解,返回11个字符的Geohash字符串。将二位的经纬度转换为以为的字符串，如果两个字符串越接近，那么距离越近\nGEOHASH\n\n\n\n\n\n\n\n\n\n\n\n\nGEO底层实现原理其实就是Zset，我们可以使用Zset命令操作GEO\nHyperloglog\n\n\n\n\n\n\n\n\n什么是基数\nA{1,3,5,7,8,9,}  \nB{1,3,5,7,8}\n基数(不重复的元素)： = 5 \n\n\n\n\n\n\n\n\n\n简介\nRedis Hyperloglog基数统计的算法!\n网页的UV ( 一个人访问一个网站多次,但是还是算作一个人! )传统的方式，set 保存用户的id ,然后就可以统计set中的元素数量作为标准怕段\n这个方式如果保存大量的用户id ,就会比较麻烦!我们的目的是为了计数，而不是保存用户Id\nPFadd [key] [element ...] # 加入元素\nPFCOUNT [key] # 统计key中的基数数量\nPFMERGE [key1] [key2] [newkey] # 合并key1和key2为key3\n\n但Hyperloglog不会特别精确。精确率在81%左右\n原理：HyperLogLog基于概率论中伯努利试验并结合了极大似然估算方法，并做了分桶优化。\nRedis中，12KB的桶，共16384(2^14^)个桶，每个桶6bit\n64位的bit串，14bit定位桶，假设\n\n\n\n\n\n\n\n\n\n用途\n允许一定的容错\n\n网页UV 计数\n转化为一个比特串 hash函数\n分桶(大的位数组) 如，100100 00 -&gt; 0号桶  100100 11-&gt; 3号桶\n\n\n\n\n如果不允许容错，就要使用set或其他自定义的数据类型。\nUV（独立访客），需要去重：UV是网站的用户访问量，访问您网站的一台电脑客户端为一个访客\nPV：即页面浏览量，或点击量；用户每1次对网站中的每个网页访问均被记录1次。\nBitmap(位存储)\n\n\n\n\n\n\n\n\n用途\n统计用户信息：活跃、不活跃；登录、未登录；365天的打卡等，涉及到两个状态的都可以使用。\nBitmap位图，数据结构！都是操作二进制位来进行记录，只有0和1两个状态\nsetbit\ngetbit\nbitcount\n\n\n\n\n\n三、Redis事务MySQL：ACID 其中，原子性：要么同时成功，要么同时失败\nRedis的单条指令可以保证原子性，但是事务不保证原子性\nRedis事务没有隔离级别的概念 ：所有的命令在事务中，并没有直接被执行，只有发起执行命令的时候才会执行。\nRedis事务的本质：一组命令的集合 ！一个事务中的所有命令都会被序列化，在事务执行的过程中，会按照顺序执行！\n一次性、顺序性、排他性 –&gt;执行一些列的命令\n3.1 事务\n\n\n\n\n\n\n\n\n正常执行事务\nRedis的事务：\n\n开启事务( Multi )\n\n命令入队( … )\n\n执行事务( Exec )\n\n事务结束\n\n\n\n\n\n\n\n\n\n\n\n放弃事务\nDISCARD\n放弃后，事务中的队列中的命令都不会执行。\n\n\n\n\n\n\n\n\n\n编译型异常(代码有问题！命令有错) \n事务中所有的命令都不会被执行\n\n\n\n\n\n\n\n\n\n\n运行时异常(1/0)\n如果事务队列中存在语法型错误，那么执行命令的时候，其他命令可以正常执行\n，错误命令抛出异常。\n\n虽然第一条命令报错了，但是依旧正常执行成功了。\n3.2 乐观锁\n\n\n\n\n\n\n\n\nMySQL的锁\n悲观锁 ：认为什么时候都会出问题，无论做什么都会加锁。\n乐观锁 ：认为什么时候都不会出问题，所以不会上锁。更新数据的时候去判断一下，在此期间是否有人修改过数据，version字段。获取version，更新的时候比较version。\n\n\n\n\n\n\n\n\n\nRedis监控\n正常执行成功：\n\n测试多线程修改值，使用watch可以当作Redis的乐观锁操作\n\n如果修改失败，获取最新的值即可\n\n\n\n\n\n\n\n\n\n\n 面试常问\nRedis可以写乐观锁\n四、Jedis使用Java操作Redis\n\n\n\n\n\n\n\n\n\nJedis是Redis官方推荐的Java连接开发工具，使用Java操作Redis的中间件，如果使用Java操作Redis，就一定要对Jedis十分的熟悉\n1、导入依赖\n&lt;!--        Jedis-->\n&lt;dependency>\n    &lt;groupId>redis.clients&lt;/groupId>\n    &lt;artifactId>jedis&lt;/artifactId>\n    &lt;version>3.2.0&lt;/version>\n&lt;/dependency>\n&lt;!--        fastjson-->\n&lt;dependency>\n    &lt;groupId>com.alibaba&lt;/groupId>\n    &lt;artifactId>fastjson&lt;/artifactId>\n    &lt;version>1.2.76&lt;/version>\n&lt;/dependency>\n\n2、编码测试\n\n连接数据库\n操作命令\n断开连接\n\n//1. new Jedis对象\nJedis jedis = new Jedis(\"127.0.0.1\", 6379);\n//Jedis所有的命令就是之前的Redis指令\n\n\n\n4.1 常用API\nString\nList\nSet\nHash\nZset\n\n同前方指令\n4.2 通过Jedis理解事务//开启事务\nTransaction multi = jedis.multi();\n\ntry &#123;\n    multi.set(\"user1\",result);//测试命令1\n    multi.set(\"user2\",result);//测试命令2\n    multi.exec();//执行事务\n&#125;catch (Exception e)&#123;\n    multi.discard();//放弃事务\n    e.printStackTrace();\n&#125;finally &#123;\n    jedis.close();//关闭连接\n&#125;\n\n\n\n五、SpringBoot整合RedisSpringData也是和SpringBoot齐名的项目\n\n==说明：在SpringBoot2.x之后，原来使用的jedis被替换成了lettuce==\n\njedis ： 采用的直连，多个线程操作的话是不安全的，如果想要避免不安全的，使用jedis pool连接池。更像BIO模式\nlettuce：采用netty，实例可以在多个线程中进行共享，不存在线程不安全的情况，可以减少线程数量，更像Nio模式\n\n5.1 源码解析\nSpringBoot所有的配置类，都有一个自动配置类 ：RedisAutoConfiguration\n\n自动配置类都会绑定一个properties配置文件 ：RedisProperties\n\n\n@Bean\n@ConditionalOnMissingBean(name = \"redisTemplate\")//我们可以自定义一个RedisTemplate来替换这个默认的\n@ConditionalOnSingleCandidate(RedisConnectionFactory.class)\npublic RedisTemplate&lt;Object, Object> redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123;\n    //默认的RedisTemplate没有过多的设置，redis对象都是需要序列化！\n    //两个泛型都是Object的类型，后面使用需要强制转换成&lt;String,Object>\n    RedisTemplate&lt;Object, Object> template = new RedisTemplate&lt;>();\n    template.setConnectionFactory(redisConnectionFactory);\n    return template;\n&#125;\n\n@Bean\n@ConditionalOnMissingBean //由于String是Redis中最常使用的类型，所以单独提出来了一个Bean\n@ConditionalOnSingleCandidate(RedisConnectionFactory.class)\npublic StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) &#123;\n    StringRedisTemplate template = new StringRedisTemplate();\n    template.setConnectionFactory(redisConnectionFactory);\n    return template;\n&#125;\n\nConditionalOnMissingBean:当这个方法不存在时，使用这里的方法\n5.2 整合测试1、导入依赖\n&lt;dependency>\n    &lt;groupId>org.springframework.boot&lt;/groupId>\n    &lt;artifactId>spring-boot-starter-data-redis&lt;/artifactId>\n&lt;/dependency>\n\n2、配置文件\nspring:\n  redis:\n    host: 127.0.0.1\n    port: 6379\n\n3、Junit测试\n@Test\nvoid contextLoads() &#123;\n    /* redisTemplate 操作不同数据类型的API*/\n    //opsForValue 操作字符串 类似String\n    //opsForList 操作List 类似List\n    //....\n    //每一个操作对应一个数据类型 opsForXxx\n    redisTemplate.opsForValue();\n\n    /* 获取Redis连接 */\n    RedisConnection connection = redisTemplate.getConnectionFactory().getConnection();\n    //        connection.flushDb();\n    //        connection.flushAll();\n\n&#125;\n\n==在开发中很少使用这些原生的方式去编写代码，通常写一个RedisUtils写一个工具类.==\n\n\n默认的序列化方式是JDK序列化，我们可能会使用Json来序列化。\n所有对象也需要序列化，否则无法进行序列化处理。\n所以我们需要自定义一个配置类\n5.3 自定义RedisTemplatepackage com.skyfroop.config;\n\nimport com.fasterxml.jackson.annotation.JsonAutoDetect;\nimport com.fasterxml.jackson.annotation.PropertyAccessor;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;\nimport org.springframework.data.redis.serializer.StringRedisSerializer;\n\nimport java.net.UnknownHostException;\n\n/**\n *\n * @Description TODO RedisConfig 固定模板,可直接使用\n * @Create: by SkyFroop\n */\n@Configuration\npublic class RedisConfig &#123;\n\n    @Bean\n    public RedisTemplate&lt;String,Object> redisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException&#123;\n        //为了开发方便，一般直接使用&lt;String,Object>\n        RedisTemplate&lt;String, Object> template = new RedisTemplate&lt;>();\n        template.setConnectionFactory(redisConnectionFactory);\n\n        // Json序列化配置\n        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);\n        ObjectMapper objectMapper = new ObjectMapper();\n        objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY)\n                    .enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);\n        jackson2JsonRedisSerializer.setObjectMapper(objectMapper);\n        //String的序列化\n        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();\n\n        //key采用String的序列化方式\n        template.setKeySerializer(stringRedisSerializer);\n        //Hash的key采用String的序列化方式\n        template.setHashKeySerializer(stringRedisSerializer);\n        //value序列化方式采用jackson\n        template.setValueSerializer(jackson2JsonRedisSerializer);\n        //Hash的value序列化方式采用Jackson\n        template.setHashValueSerializer(jackson2JsonRedisSerializer);\n\n        template.afterPropertiesSet();// 生效配置\n\n        return template;\n    &#125;\n\n&#125;\n\n\n\n\n5.4 RedisUtil工具包(当前版本仅包含常用命令)import org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.stereotype.Component;\nimport org.springframework.util.CollectionUtils;\n\nimport java.util.Collection;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.TimeUnit;\n\n/**\n *\n * @Description TODO Redis工具类\n * @Create: Power By SkyFroop(HAN XIAOJIE)\n *\n */\n@Component\npublic final class RedisUtil &#123;\n    @Autowired\n    private RedisTemplate&lt;String,Object> redisTemplatel;\n\n    //====================================common====================================\n\n    /**\n     * 指定缓存失效时间\n     * @param key 键\n     * @param time 时间(秒)\n     * */\n    public boolean expire(String key,long time)&#123;\n        try&#123;\n            if(time > 0)&#123;\n                redisTemplatel.expire(key,time, TimeUnit.SECONDS);\n            &#125;\n            return true;\n        &#125;catch (Exception e)&#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 根据key 获取过期时间\n     * @param key 键 NOT NULL\n     * @return 时间(秒) 返回0代表永久有效\n     * */\n    public long getExpire(String key)&#123;\n        return redisTemplatel.getExpire(key,TimeUnit.SECONDS);\n    &#125;\n\n    /**\n     * 判断key是否存在\n     * @param key 键\n     * @return true 存在 false 不存在\n     * */\n    public boolean hasKey(String key)&#123;\n        try &#123;\n            return redisTemplatel.hasKey(key);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 删除缓存\n     * @param key one or more...\n     * */\n    public void del(String ...key)&#123;\n        if (key != null &amp;&amp; key.length > 0)&#123;\n            if(key.length == 1)&#123;\n                redisTemplatel.delete(key[0]);\n            &#125;else&#123;\n                redisTemplatel.delete((Collection&lt;String>) CollectionUtils.arrayToList(key));\n            &#125;\n        &#125;\n    &#125;\n\n    //====================================String====================================\n\n    /**\n     * 普通缓存获取\n     * @param key 键\n     * @return value\n     * */\n    public Object get(String key)&#123;\n        return key == null ? null : redisTemplatel.opsForValue().get(key);\n    &#125;\n\n    /**\n     * 普通缓存放入\n     * @param key 键\n     * @param value 值\n     * @return true 成功 false 失败\n     * */\n    public boolean set(String key,Object value)&#123;\n        try &#123;\n            redisTemplatel.opsForValue().set(key,value);\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 普通缓存放入并设置时间\n     * @param key 键\n     * @param value 值\n     * @param time 时间(秒) time 应大于0，如果time小于等于0将设置无暇去你\n     * @return true 成功 false 失败\n     * */\n    public boolean set(String key,Object value,long time)&#123;\n        try &#123;\n            if (time > 0)&#123;\n                redisTemplatel.opsForValue().set(key,value,time,TimeUnit.SECONDS);\n            &#125;else &#123;\n                set(key,value);\n            &#125;\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 单调递增\n     * @param key 键\n     * @param delta 步长\n     * */\n    public long incr(String key,long delta)&#123;\n        if (delta &lt; 0)&#123;\n            throw new RuntimeException(\"递增因子必须大于0\");\n        &#125;\n        return redisTemplatel.opsForValue().increment(key,delta);\n    &#125;\n\n    /**\n     * 单调递减\n     * @param key 键\n     * @param delta 步长\n     * */\n    public long decr(String key,long delta)&#123;\n        if (delta &lt; 0)&#123;\n            throw new RuntimeException(\"递减因子必须大于0\");\n        &#125;\n        return redisTemplatel.opsForValue().decrement(key,delta);\n    &#125;\n\n    //==================================== Map ====================================\n\n    /**\n     * HashGet\n     * @param key 键 NOT NULL\n     * @param item 项 NOT NULL\n     * */\n    public Object hget(String key,String item)&#123;\n        return redisTemplatel.opsForHash().get(key,item);\n    &#125;\n\n    /**\n     * 获取HashKey对应的所有键值\n     * @param key 键\n     * @return 对应的多个键值\n     * */\n    public Map&lt;Object,Object> hmget(String key)&#123;\n        return redisTemplatel.opsForHash().entries(key);\n    &#125;\n\n    /**\n     * HashSet\n     * @param key 值\n     * @param map 对应多个键值\n     * */\n    public boolean hmset(String key,Map&lt;String,Object> map)&#123;\n        try &#123;\n            redisTemplatel.opsForHash().putAll(key,map);\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * HashSet 并设置过期时间\n     * @param key 值\n     * @param map 对应多个键值\n     * @param time 时间(秒)\n     * */\n    public boolean hmset(String key,Map&lt;String,Object> map,long time)&#123;\n        try &#123;\n            redisTemplatel.opsForHash().putAll(key,map);\n            if(time>0)&#123;\n                expire(key,time);\n            &#125;\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 向一张Hash放入数据,如果不存在将创建数据\n     * @param key 键\n     * @param item 项\n     * @param value 值\n     * */\n    public boolean hset(String key,String item,Object value)&#123;\n        try &#123;\n            redisTemplatel.opsForHash().put(key,item,value);\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 向一张Hash放入数据,如果不存在将创建数据,同时设置过期时间\n     * @param key 键\n     * @param item 项\n     * @param value 值\n     * @param time 时间(秒) 注意：如果已存在的Hash表有时间,这里将会是替换原有时间\n     * */\n    public boolean hset(String key,String item,Object value,long time)&#123;\n        try &#123;\n            redisTemplatel.opsForHash().put(key,item,value);\n            if (time > 0)&#123;\n                expire(key,time);\n            &#125;\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 删除Hash中的值\n     *\n     * @param key 键 NOT NULL\n     * @param item 项 one or more NOT NULL\n     * */\n    public void hdel(String key ,Object... item)&#123;\n        redisTemplatel.opsForHash().delete(key,item);\n    &#125;\n\n    /**\n     * 判断Hash中是否有该项的值\n     * @param key 键\n     * @param item 项\n     * */\n    public boolean hHasKey(String key,String item)&#123;\n        return redisTemplatel.opsForHash().hasKey(key,item);\n    &#125;\n\n    /**\n     * Hash 单调递增 如果不存在，就会创建一个，并把新增后的值返回\n     * @param key 键\n     * @param item 项\n     * @param by 步长\n     * */\n    public double hincr(String key,String item,double by)&#123;\n        return redisTemplatel.opsForHash().increment(key,item,by);\n    &#125;\n\n    /**\n     * Hash 单调递减\n     * @param key 键\n     * @param item 项\n     * @param by 步长\n     * */\n    public double hdecr(String key,String item,double by)&#123;\n        return redisTemplatel.opsForHash().increment(key,item,-by);\n    &#125;\n\n    //==================================== Set ====================================\n\n    /**\n     * 根据key获取Set所有值\n     * @param key 键\n     * */\n    public Set&lt;Object> sGet(String key)&#123;\n        try &#123;\n            return redisTemplatel.opsForSet().members(key);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return null;\n        &#125;\n    &#125;\n\n    /**\n     * 从Set中查询是否存在value\n     * @param key 键\n     * @param value 值\n     * */\n    public boolean sHasKey(String key,Object value)&#123;\n        try &#123;\n            return redisTemplatel.opsForSet().isMember(key,value);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 将数据放入Set缓存\n     * @param key 键\n     * @param values 值 one or more\n     * @return 成功个数\n     * */\n    public long sSet(String key,Object... values)&#123;\n        try &#123;\n            return redisTemplatel.opsForSet().add(key,values);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return 0;\n        &#125;\n    &#125;\n\n    /**\n     * 将数据放入Set缓存,并设置过期时间\n     * @param key 键\n     * @param time 时间(秒)\n     * @param values 值 one or more\n     * @return 成功个数\n     * */\n    public long sSet(String key,long time,Object... values)&#123;\n        try &#123;\n            Long count = redisTemplatel.opsForSet().add(key, values);\n            if(time > 0)&#123;\n                expire(key,time);\n            &#125;\n            return count;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return 0;\n        &#125;\n    &#125;\n\n    /**\n     * 获取Set缓存长度\n     * @param key 键\n     * @return Set缓存长度\n     * */\n    public long sGetSetSize(String key)&#123;\n        try &#123;\n            return redisTemplatel.opsForSet().size(key);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return 0;\n        &#125;\n    &#125;\n\n    /**\n     * 从Set中移除值为value的项\n     * @param key 键\n     * @param values 值 one or more\n     * @return 移除的个数\n     * */\n    public long setRemove(String key,Object... values)&#123;\n        try &#123;\n            return redisTemplatel.opsForSet().remove(key, values);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return 0;\n        &#125;\n    &#125;\n\n    //==================================== List ====================================\n\n    /**\n     * 获取list缓存的长度\n     * @param key 键\n     * */\n    public long lGetListSize(String key)&#123;\n        try &#123;\n            return redisTemplatel.opsForList().size(key);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return 0;\n        &#125;\n    &#125;\n\n    /**\n     * 通过索引获取list中的值\n     * @param key 键\n     * @param index 索引\n     * @Description index >= 0时, 0为表头索引,index &lt; 0时,-1为表尾,-2为到数第二个元素...\n     * */\n    public Object lGetIndex(String key,long index)&#123;\n        try &#123;\n            return redisTemplatel.opsForList().index(key,index);\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return null;\n        &#125;\n    &#125;\n\n    /**\n     * 将List放入缓存\n     * @param key 键\n     * @param value 值\n     * */\n    public boolean lSet(String key,Object value)&#123;\n        try &#123;\n            redisTemplatel.opsForList().rightPush(key,value);\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 将List放入缓存,并设置过期时间\n     * @param key 键\n     * @param value 值\n     * @param time 时间(秒)\n     * */\n    public boolean lSet(String key,Object value,long time)&#123;\n        try &#123;\n            redisTemplatel.opsForList().rightPush(key,value);\n            if (time > 0)&#123;\n                expire(key,time);\n            &#125;\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 将List&lt;>放入缓存\n     * @param key 键\n     * @param value 值\n     * */\n    public boolean lSet(String key, List&lt;Object> value)&#123;\n        try &#123;\n            redisTemplatel.opsForList().rightPushAll(key,value);\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 将List&lt;>放放入缓存,并设置过期时间\n     * @param key 键\n     * @param value 值\n     * @param time 时间(秒)\n     * */\n    public boolean lSet(String key,List&lt;Object> value,long time)&#123;\n        try &#123;\n            redisTemplatel.opsForList().rightPush(key,value);\n            if (time > 0)&#123;\n                expire(key,time);\n            &#125;\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 根据索引修改List中的某条数据\n     *\n     * @param key 键\n     * @param index 索引\n     * @param value 值\n     * */\n    public boolean lUpdateIndex(String key,long index,Object value)&#123;\n        try &#123;\n            redisTemplatel.opsForList().set(key, index, value);\n            return true;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 移除N个值为value\n     * @param key 键\n     * @param count 移除数量\n     * @param value 值\n     * @return 移除的个数\n     * */\n    public long lRemove(String key,long count,Object value)&#123;\n        try &#123;\n            Long removeCount = redisTemplatel.opsForList().remove(key, count, value);\n            return removeCount;\n        &#125; catch (Exception e) &#123;\n            e.printStackTrace();\n            return 0;\n        &#125;\n    &#125;\n\n    //==================================== Zset ====================================\n\n\n    //====================================Geospatial====================================\n\n\n    //====================================Hyperloglog====================================\n\n\n    //====================================Bitmap====================================\n\n\n&#125;\n\n\n\n\n\n\n六、Redis.conf详解Redis启动的时候，就通过配置文件来启动。\n\n\n\n\n\n\n\n\n\n 单位\n\n\n配置文件unit单位对大小写不敏感，拥有这几种单位\n\n\n\n\n\n\n\n\n\n\n包含\n\n\n\n\n\n\n\n\n\n\n网络\nbind 127.0.0.1 -::1 # 绑定IP\nprotected-mode yes # 保护模式\nport 6379 # 端口设置\n\n\n\n\n\n\n\n\n\n\n\n\n通用( GENERAL)\ndaemonize yes # 以守护进程的方式运行，默认为no，需要修改为yes\npidfile /var/run/redis_6379.pid # 如果以后台的方式运行(守护进程)，我们就需要指定一个pid文件\n\n#日志\n# Specify the server verbosity level.\n# This can be one of:\n# debug (a lot of information, useful for development/testing)\n# verbose (many rarely useful info, but not a mess like the debug level)\n# notice (moderately verbose, what you want in production probably) 生产环境使用\n# warning (only very important / critical messages are logged)\nloglevel notice\nlogfile \"\" # 日志的输出文件名\n\ndatabases 16 # 数据库的数量，默认是16个\nalways-show-logo no # 是否显示Redis的Logo\n\n\n\n\n\n\n\n\n\n\n\n\n\n快照 ( SNAPSHOTTING )\n持久化，在规定的时间内，执行了多少次操作，则会持久化到文件(.rdb ; .aof)\nRedis是内存数据库，如果没有持久化，那么数据断电即失\n# 如果3600秒内，如果至少有1个key进行了修改，我们即使进行持久化操作\nsave 3600 1 \n#如果300秒内，如果至少有100个key进行了修改，我们即使进行持久化操作\nsave 300 100\n#如果60秒内，如果至少有10000个key进行了修改，我们即使进行持久化操作\nsave 60 10000\n\nstop-writes-on-bgsave-error yes # 持久化如果出错，是否需要继续工作\nrdbcompression yes # 是否压缩rdb资源(需要消耗一些CPU资源)\nrdbchecksum yes # 保存rdb文件的时候，进行错误的检查校验\ndir ./ # rdb文件默认的保存目录，默认是在当前文件夹下\ndbfilename dump.rdb # rdb的默认文件名\n\n\n\n\n\n\n\n\n\n\n\n\n复制 ( REPLICATION )\nreplicaof &lt;masterip> &lt;masterport> # 从机设置\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n安全 ( SECURITY )\nrequirepass 密码 # 设置密码\n##密码设置命令\nconfig set requirepass \"密码\"\n##认证\nauth 密码\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n限制 ( CLIENTS )\nmaxclients 10000 # 设置能连接上Redis的最大客户端的数量\n\n\n\n\n\n\n\n\n\n\n\n\n内存 ( MEMORY MANAGEMENT )\nmaxmemory &lt;bytes> # 最大的内存容量\nmaxmemory-policy noeviction # 内存达到上限之后的处理策略\n\t\t\t\t\t\t\t# 移除一些过期的key 也可能报错\n\n\n\n内存达到上限后的处理策略:\n\nvolatile-lru:只对设置了过期时间的key进行LRU(默认值)\nallkeys-lru :删除1ru算法的key\nvolatile-random:随机删除即将过期key\nallkeys-random:随机删除\nvolatile-ttl :刮|除即将过期的\nnoeviction :永不过期,返回错误\n\n\n\n\n\n\n\n\n\n\nAPPEND ONLY模式 (AOF)\nappendonly no # 默认不开启aof模式，默认使用的是rdb方式持久化，在大部分情况下，rdb完全够用\nappendfilename \"appendonly.aof\" # 持久化的文件的名字\n\n# appendfsync always # 每次修改都会sync\nappendfsync everysec # 每秒执行一次sync,可能会丢失这1秒的数据,默认\n# appendfsync no     # 不同步,这个时候操作系统自己同步数据，速度最快\n\n\n\n\n\n\n七、Redis持久化Redis是内存数据库，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中的数据库状态也会消失。所以Redis提供了持久化功能!\n==面试和工作，持久化都是重点==\n\nAOF :( append only file )持久化以独立日志的方式记录每次写命令，并在 Redis 重启时在重新执行 AOF 文件中的命令以达到恢复数据的目的。AOF 的主要作用是解决数据持久化的实时性。\nRDB :把当前 Redis 进程的数据生成时间点快照( point-in-time snapshot ) 保存到存储设备的过程。\n\n7.1 RDB ( Redis DateBase )在主从复制中rdb就是备用了，从机上面\n\n\n\n\n\n\n\n\n\n什么是RDB\n\n在指定的时间间隔内将内存中的数据集快照写入磁盘.也就是行话讲的Snapshot快照,它恢复时是将快照文件直接读到内存里。Redis会单独创建( fork ) 一个子进程来进行持久化.会先将数据写入到一个临时文件中。待持久化过程都结束了,再用这个临时文件替换上次持久化好的文件。整个过程中,主进程是不进行任何0操作的。这就确保了极高的性能。如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感,那RDB方式要比AOF方式更加的高效。RDB的缺点是最后- 次持久化后的数据可能丢失。\n==rdb保存的文件是dump.rdb==\n生产环境，我们会将这个文件进行备份\n\n\n\n\n\n\n\n\n\n触发机制\n\nsave 的规则满足的情况下，会自动触发rdb规则\n执行flushall命令，也会触发我们的rdb规则\n退出redis，也会产生rdb文件\n\n备份就会自动生成一个dump.rdb文件\n\n\n\n\n\n\n\n\n\n如何恢复rdb文件\n只需要将rdb文件放到Redis启动目录就可以了，redis启动时会自动检查dump.rdb回符其中的数据\n==几乎自己的默认配置就足够用了==\n\n\n\n\n\n\n\n\n\n优点\n\n适合大规模的数据恢复\n对数据的完整性不高\n\n\n\n\n\n\n\n\n\n\n缺点\n\n，需要一定的时间间隔，如果redis意外宕机，最后一次修改的数据就没有了\nfork进程的时候，会占用一定的内存空间\n\n7.2 AOF ( Append Only File )将我们的所有命令都记录下来，history，恢复的时候就把这个文件全部进行一遍\n以日志的形式来记录每个写操作,将Redis执行过的所有指令记录下来(读操作不记录) ,只许追加文件但不可以改写文件, redis启动之初会读取该文件重新构建数据,换言之. redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作\n==aof保存的文件是appendonly.aof==\n\n\n\n\n\n\n\n\n\n流程\n\n\n写入缓存：每次执行命令后，进行append操作写入AOF缓存\n同步磁盘：AOF 缓冲区根据对应的策略向硬盘进行同步操作。\nAOF重写：随着 AOF 文件越来越大，需要定期对 AOF 文件进行重写，达到压缩的目的。\n重启加载： 当 Redis 重启时，可以加载 AOF 文件进行数据恢复。\n\n7.2.1 写入缓存每次执行命令都是通过call()，call的时候会把命令写入aof缓存，也就是server.aof_buf\n调用链： call() -&gt; propogate() -&gt; feedAppendOnlyFile\nvoid call(client *c, int flags) &#123;\n    ...\n    propagate(c->cmd,c->db->id,c->argv,c->argc,propagate_flags);\n&#125;\n\nvoid propagate(struct redisCommand *cmd, int dbid, robj **argv, int argc,\n               int flags)\n&#123;\n    ...\n    if (server.aof_state != AOF_OFF &amp;&amp; flags &amp; PROPAGATE_AOF)\n        feedAppendOnlyFile(cmd,dbid,argv,argc);\n    ...\n&#125;\n\n\n\n\n\n\n\n\n\n\nfeedAppendOnlyFile()\nvoid feedAppendOnlyFile(struct redisCommand *cmd, int dictid, robj **argv, int argc) &#123;\n    // 把命令解析编码，比较复杂，\n    buf = catAppendOnlyGenericCommand(buf,argc,argv);\n    // 然后存入server.aof_buf\n    server.aof_buf = sdscatlen(server.aof_buf,buf,sdslen(buf));\n   // 如果子进程正在重写AOF，就把buf写入server.aof_rewrite_buf_blocks链表\n    if (server.child_type == CHILD_TYPE_AOF)\n        aofRewriteBufferAppend((unsigned char*)buf,sdslen(buf));\n&#125;\n\n\n解析命令\nbuf &#x3D; catAppendOnlyGenericCommand(buf,argc,argv);\n\n该函数主要工作就是解析命令，把传入的cmd和argv，argc解析成&quot;*3\\r\\n3\\r\\nSET\\r\\n5\\r\\nmykey\\r\\n$7\\r\\nmyvalue\\r\\n&quot;的样子，存储在buff里\n\n写入缓存\nserver.aof_buf = sdscatlen(server.aof_buf,buf,sdslen(buf));\n把解析好的命令写入缓存，同步给磁盘\n\n如果子进程正在重写AOF文件，则把解析好的命令写入server.aof_rewrite_buf_blocks链表\nserver.child_type表示子进程正在进行什么工作，在AOF重写(rewrite)过程中会创建子进程执行重写工作，这个在下面介绍AOF重写的时候会解释这里\n\n\n7.2.2 同步磁盘同步磁盘的操作在函数flushAppendOnlyFike()\nflushAppendOnlyFile函数的行为由redis.conf配置中的appendfsync选项的值来决定。该选项有三个可选值，分别是always,everysec和no:\n\nalways: Redis在每个事件循环都要将AOF缓冲区中的所有内容写入到AOF文件，并且同不AOF文件，所以always的效率是最差的一个，但从安全性来说也是最安全的，当发生故障停机时，AOF持久化也只会丢失一个事件循环中所产生的命令数据。\neverysec: Redis 在每个事件循环都要将AOF缓冲区中的所有内容写入到AOF\n\n如果AOF文件有错位，Redis则无法启动，此时需要修复这个AOF文件，Redis提供的修复工具:redis-check-aof --fix\n\n\n\n\n\n\n\n\n\n重写规则\naof默认就是文件的无限追加，文件会越来越大\nauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb\n\n如果AOF文件大于64M，太大了，fork一个新的进程会对我们的文件进行重写\n\n\n\n\n\n\n\n\n\n优点\n\n每一次修改都同步，文件的完整性会更加好\n每秒同步一次，可能会丢失一秒的数据\n从不同步。效率最高\n\n\n\n\n\n\n\n\n\n\n缺点\n\n相对于数据文件来说，aof远远大于rdb，修复的速度也比rdb慢\nAOF运行效率也要比RDB慢，所以Redis默认的配置就是RDB\n\n7.3 小结\nRDB持久化方式能够在指定的时间间隔内对你的数据进行快照存储\nAOF 持久化方式记录每次对服务器写的操作。当服务器重启的时候会重新执行这些命令来恢复原始的数据, AOF命令以Redis协议追加保存每次写的操作到文件末尾, Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大。\n只做缓存.如果你只希望你的数据在服务器运行的时候存在.你也可以不使用任何持久化\n同时开启两种持久化方式\n在这种情况下。当redis重启的时候会优先载入A0F文件来恢复原始的数据,因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。\nRDB的数据不实时,同时使用两者时服务器重启也只会找AOF文件,那要不要只使用AOF呢?作者建议不要,因为RDB更适合用于备份数据库( AOF在不断变化不好备份) , 快速重启,而且不会有AOF可能潜在的Bug ,留着作为一个万一的手段。\n\n\n性能建议\n因为RDB文件只用作后备用途,建议只在Slave.上持久化RDB文件,而且只要15分钟备份一次就够了,只保留save 900 1这条规则。\n如果Enable AOF . 好处是在最恶劣情况下也只会丢失不超过两秒数据.启动脚本较简单只load自己的AOF文件就可以了,代价一是带来了持续的IO ，.二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可,应该尽量减少AOF rewrite的频率,AOF重写的基础大小默认值64M太了,可以设到5G以上,默认超过原大小100%大小重写可以改到适当的数值。\n如果不Enable AOF , 仅靠Master-Slave Repllcation实现高可用性也可以,能省掉一大笔I0 ,也减少了rewrite时带来的系统波动。代价是如果Master/Slave 同时倒掉,会丢失十几分钟的数据,启动脚本也要比较两个Master/Slave中的RDB文件,载入较新的那个,微博就是这种架构。\n\n\n\n八、Redis发布订阅Redis 发布订阅(pub/sub)是一种==消息通信模式==︰发送者(pub)发送消息，订阅者(sub)接收消息。Redis客户端可以订阅任意数量的频道。订阅/发布消息图:\n\nPUBLISH [name] [message] # 发布消息到频道[name]SUBSCRIBE [name ...] # 订阅频道nameUNSUBSCRIBE [name] # 取消订阅频道name\n\n\n\n\n\n\n\n\n\n\n\n\n原理\nRedis是使用C实现的，通过分析Redis源码里的pubsut.c文件，了解发布和订阅机制的底层实现，籍此加深对Redis的理解。Redis 通过UBLISH、SUBSCRIBE 和PSUBSCRIBE等命令实现发布和订阅功能。通过SUBSCRIBE命令订阅某频道后，redis-server里维护了一个字典，字典的键就是一个个channel，而字典的值则是一个链表，链表中保存了所有订阅这个channel的客户端。SUBSCRIBE命令的关键，就是将客户端添加到给定channel的订阅链表中。通过PUBLlSH命令向订阅者发送消息，redis-server 会使用给定的频道作为键，在它所维护的channel 字典中查找记录了订阅这个频道的所有客户端的链表，遍历这个链表，将消息发布给所有订阅者。Pub/Sub从字面上理解就是发布( Publish )与订阅 (Subscribe )，在Redis中，你可以设定对某一个key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。\n微信：\n通过SUBSCRIBE命令订阅某频道后, |redis-server里维护了一个字典,字典的键就是一一个个 频道! , 而字典的值则是一个链表链表中保存了所有订阅这个channel的客户端。SUBSCRIBE 命令的关键,就是将客户端添加到给定channel的订阅链表中。\nPub/Sub从字面.上理解就是发布( Publish )与订阅( Subscribe) , 在Redis中,你可以设定对某一-个key值进行消息发布及消息订阅，当一个key值上进行了消息发布后,所有订阅它的客户端都会收到相应的消息。\n\n\n\n\n\n\n\n\n\n用途\n\n实时消息系统，比如普通的即时聊天,群聊等功能。\n实时聊天 ( 频道当作聊天室，将信息回显给所有人即可 )\n订阅，关注系统都是可以的\n\n= = = = 下述部分在《Redis高级》中详解 = = = = =九、Redis主从复制9.1 概念主从复制,是指将一台Redis服务器的数据,复制到其他的Redis服务器。前者称为主节点(master/leader) ,后者称为从节点(slave/follower) ;数据的复制是单向的,只能由主节点到从节点。Master以写为主, Slave以读为主。默认情况下,每台Redis服务器都是主节点;且一个主节点可以有多个从节点(或没有从节点) ,但一个从节点只能有一个主节点。\n\n\n\n\n\n\n\n\n\n主从复制的主要作用\n\n数据冗余:主从复制实现了数据的热备份,是持久化之外的一种数据冗余方式。\n故障恢复:当主节点出现问题时,可以由从节点提供服务,实现快速的故障恢复;实际上是- -种服务的冗余。\n负载均衡:在主从复制的基础上,配合读写分离,可以由主节点提供写服务,由从节点提供读服务(即写Redis数据时应用连接主节点,读Redis数据时应用连接从节点) , 分担服务器负载;尤其是在写少读多的场景下,通过多个从节点分担读负载,可以大大提高Redis服务器的并发量。\n高可用(高可用—&gt;集群)基石:除了上述作用以外,主从复制还是哨兵和集群能够实施的基础,因此说主从复制是Redis高可用的基础。\n\n\n\n\n\n\n\n\n\n\n一般来说,要将Redis运用于工程项目中,只使用一台Redis是万万不能的,原因如下\n\n从结构上,单个Redis服务器会发生单点故障,并且一台服务器需要处理所有的请求负载,压力较大;\n从容量上,单个Redis服务器内存容量有限,就算一台Redis服务 器内存容量为256G ,也不能将所有内存用作Redis存储内存,一般来说,==单台Redis最大使用内存不应该超过20G==。\n\n电商网站上的商品，一般都是一次上传，无数次浏览的，也就是多读少写，对于这种场景，通常使用这种架构：\n\n主从复制，读写分离。80%的情况都是进行读操作，减缓服务器的压力，架构中经常使用，最低配置：一主二从\n只要在公司中，主从复制时必须要使用的，在真实的项目中不可能单机使用Redis\n9.2 环境配置配置主机\n127.0.0.1:6379> info replication # 查看当前库的信息# Replicationrole:master # 角色 masterconnected_slaves:0 # 没有从机master_failover_state:no-failovermaster_replid:2e0dbfc18702474129a88e972702b9ef903eafddmaster_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0\n\n修改从机：\n\n端口\npid名字\nlog文件名字\ndump.db的名字\n\n\n9.3 一主二从==默认情况下，每台Redis服务器都是主节点== 所以需要修改从机配置\n一主(79) 二从(80、81)\n从机中配置：\nSLAVEOF [host] [port] # 从机的指定的主机的host和port#信息127.0.0.1:6380> INFO replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upmaster_last_io_seconds_ago:7master_sync_in_progress:0slave_repl_offset:14slave_priority:100slave_read_only:1replica_announced:1connected_slaves:0master_failover_state:no-failovermaster_replid:13242a5fc3d3f219f26d6913b64fba0fe25d5572master_replid2:0000000000000000000000000000000000000000master_repl_offset:14second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:14\n\n真实的从主配置应该在配置文件中配置，这样的话是永久的\n\n\n\n\n\n\n\n\n\n细节\n\n主机可以写，从机不能写，只能读\n如果主机断开连接，此时从机依然旧链接到主机(没有配置哨兵的情况下)，但是没有写操作，这个时候如果主机回来了，从机依旧可以读新写入的信息。\n如果是使用命令行来配置的主从，此时如果重启了从机，从机就会变成主机。再恢复为从机时，数据就可以立马从主机中获取值\n\n\n\n\n\n\n\n\n\n\n复制原理\n\nSlave启动成功连接到master后会发送一个sync命令\nMaster接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，==master将传送整个数据文件到slave， 并完成次完全同步。==\n\n\n全量复制:而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。\n增量复制: Master继续将新的所有收集到的修改命令依次传给slave，完成同步\n\n但是只要是重新连接master ,一次完全同步 (全量复制)将被自动执行。数据一定可以在从机中看到\n\n\n\n\n\n\n\n\n\n层层链路 (了解)\n上一个master 连接下一个SLAVER\n\n此时也可以完成主从复制\n如果丢失的了主节点，可以使用SLAVEOF no one让自己变成主机，其他的节点就可以手动连接到最新的主节点\n9.4 哨兵模式redis-sentinel\n\n\n\n\n\n\n\n\n\n概述\n主从切换技术的方法是:当主服务器宕机后，需要手动把一-台从服务器切换为主服务器,这就需要人工干预,费事费力,还会造成一段时间内服务不可用。这不是一种推荐的方式,更多时候,我们优先考虑哨兵模式。Redis从2.8开始正式提供了Sentinel (哨兵)架构来解决这个问题。\n能够后台监控主机是否故障,如果故障了根据投票数==自动将从库转换为主库。==\n哨兵模式是一种特殊的模式,首先Redis提供了哨兵的命令,哨兵是一个独立的进程,作为进程，它会独立运行。其原理是哨兵通过发送命令,等待Redis服务器响应,从而监控运行的多个Redis实例。\n\n然而一个哨兵进程对Redis服务器进行监控,可能会出现问题,为此,我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控,这样就形成了多哨兵模式。\n\n假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为主观下线。\n当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行ailover[故障转移]操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为客观下线。\n如果主机宕机后，新的主机选举出来后，之前的主机恢复也只能作为新主机的从机\n\n\n\n\n\n\n\n\n\n哨兵配置文件\n# sentinel monitor 被监控的名称 host port 1 \n# 数字1表示主机挂了，slave投票看让谁接替成为主机，票数最多的就会成为主机\nsentinel monitor myredis 127.0.0.1 6379 1\n\n\n\n\n\n\n\n\n\n\n\n\n优点\n\n哨兵集群，基于主从复制模式，所有的主从配置优点他都有\n主从可以切换，故障可以转移，系统的可用性更好\n哨兵模式就是主从模式的升级，手动到自动更加健壮\n\n\n\n\n\n\n\n\n\n\n缺点\n\nRedis不容易在线扩容，集群容量一旦达到上限，在线扩容就十分的麻烦\n实现哨兵模式的配置其实是很麻烦的，里面有很多选择\n\n\n\n\n\n\n\n\n\n\n哨兵模式的全部配置\n# Example sentinel.conf\n\n# 哨兵[sentine]实例运行的端口默认26379\nport 26379\n\n#哨兵sentinel的工作目录\ndir /tmp\n\n# 哨兵sentinel监控的redis主节点的ip port\n# master-name 可以自己命名的主节点名字只能由字A-z、数字0-9、\".-_\"组成。\n# quorum 配置多少个sentinel哨兵统一认为master主节点失联，那么这时客观上认为主节点失联了\n# sentinel monitor &lt;master-name> &lt;ip> &lt;redis-port> &lt;quorum>\nsentine] monitor mymaster 127.0.0.1 6379 2\n\n\n#当在Redis实例中开启了requirepass foobared 授权密码这样所有连接Redis实例的客户端都要提供密码\n#设置哨兵sentinel连接主从的密码注意必须为主从设置一样的验证密码\n# sentinel auth-pass &lt;master-name> &lt;password>\nsentine1 auth-pass mymaster MySUPER--secret-0123passwOrd\n\n\n#指定多少毫秒之后主节点没有应答哨兵sentinel此时哨兵主观上认为主节点下线   默认30秒\n# sentinel down-after-milliseconds &lt;master-name> &lt;milliseconds>\nsentinel down-after-mi liseconds mymaster 30000\n\n\n#这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行同步，\n##这个数字越小，完成failover所需的时间就越长，\n##但是如果这个数字越大，就意味着越多的slave因为replication而不可用。\n##可以通过将这个值设为1来保证每次只有一个slave处于不能处理命令请求的状态。\n# sentine1 paralle 7-syncs &lt;master-name> &lt;nums 1 aves >\nsentine1 paralle 1-syncs mymaster 1\n\n#故障转移的超时时间failover-timeout 可以用在以下这些方面:\n#1.同一个sentinel对同一 个master两次failover之间的问隔时间。\n#2.当一个slave从一 个错误的master那里同步數据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。\n#3.当想要取消一个正在进行的failover所而要的时间。\n#4.当进行failover时，配置所有s1aves指向新的master所需的最大时间。不过，即使过了这个超时，slaves 依然会被正确配置为指向master,但是就不按parallel-syncs所配置的规则来了\n#默认三分钟\n# sentinel failover-timeout &lt;master-name> &lt;milliseconds>\nsentine1 failover-timeout mymaster 180000\n\n# SCRIPTS EXECUTION\n\n#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。\n#对于脚木的运行结果有以下规则: \n#1.若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10\n#2.若脚本执行后返回2.或者比2更高的一个返回值，脚本将不会重复执行。\n#3.如果脚木在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。\n#4.一个脚木的最大执行时间为60s，如果超过这个时间，脚本将会被- 个SIGKILL信号终止，之后重新执行。\n\n#通知型脚本:当sentinel有任何警告级别的事件发生时( 比如说redis实例的主观失效和客观失效等等)，将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配置文件中配置了这个脚木路径，那么必须保证这个脚木存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。\n\n#通知脚本\n#邮件的Shell编程\n# sentinel notification-script &lt;master-name> &lt;script-path>\nsentinel notification-script mymafter /var/redis/notify.sh \n\n#客户端重新配置主节点参数脚本\n#当一个master由于failover而发生改变时，这个脚木将会被调用，通知相关的客户端关于master地址已经发生改变的信息。\n#以下多数将会在调用脚本时传给脚本:\n# &lt;master-name> &lt;role> &lt;state> &lt;from-ip> &lt;from-port> &lt;to-ip> &lt;to-port>\n#目前&lt;state>总是\"failover\",\n# &lt;role> 是\"leader\"或者“observer\"中的一个。\n#参数from-ip, from-port, to-ip, to-port是用来和旧的master和新的master (即H的sTave)通信的\n#这个脚本应该是通用的，能被多次调用，不是针对性的。\n# sentinel client-reconfig-script &lt;master-name> &lt;script-path>\nsentinel client-reconfig-script mymaster /var/redis/reconfig.sh #一般都是运维来控制\n\n\n\n\n十、Redis缓存穿透和雪崩(重要)Redis缓存的使用，极大的提升了应用程序的性能和效率,特别是数据查询方面。但同时，它也带来了一些问题。其中，最要害的问题，就是数据的一致性问题,从严格意义。上讲,这个问题无解。如果对数据的一致性要求很高，那么就不能使用缓存。另外的一些典型问题就是，缓存穿透、缓存雪崩和缓存击穿。目前，业界也都有比较流行的解决方案。\n10.1 缓存穿透\n\n\n\n\n\n\n\n\n概念\n用户想要查询一个数据,发现redis内存数据库没有,也就是缓存没有命中,于是向持久层数据库查询。发现也没有,于是本次查询失败。当用户很多的时候,缓存都没有命中，于是都去请求了持久层数据库。这会给持久层数据库造成很大的压力,这时候就相当于出现了缓存穿透。\n\n\n\n\n\n\n\n\n\n解决方案\n方法一：使用布隆过滤器\n布隆过滤器是一种数据结构,对所有可能查询的参数以Hash形式存储,在控制层先进行校验,不符合则丢弃,从而避免了对底层存储系统的查询压力;\n\n方法二：缓存空对象\n当存储层不命中后,即使返回的空对象也将其缓存起来,同时会设置一个过期时间 ,之后再访问这个数据将会从缓存中获取，保护了后端数据源;\n\n但是这种方法会存在两个问题:1、如果空值能够被缓存起来,这就意味着缓存需要更多的空间存储更多的键,因为这当中可能会有很多的空值的键;2、即使对空值设置了过期时间,还是会存在缓存层和存储层的数据会有一-段时间窗口的不一-致 ,这对于需要保持一致性的业务会有影响。\n10.2 缓存击穿量太大，缓存过期\n\n\n\n\n\n\n\n\n\n概述\n这里需要注意和缓存击穿的区别,缓存击穿,是指一个key非常热点;在不停的扛着大并发,大并发集中对这一个点进行访问,当这个key在失效的瞬间,持续的大并发就穿破缓存，直接请求数据库,就像在一个屏障上凿开了一个洞。\n当某个key在过期的瞬间,有大量的请求并发访问这类数据一般是热点数据,由于缓存过期 ,会同时访问数据库来查询最新数据，并且回写缓存,会导使数据库瞬间压力过大。\n\n\n\n\n\n\n\n\n\n解决方案\n设置热点数据永不过期\n从缓存层面来看，没有设置过期时间，所以不会出现热点key过期后产生的问题。\n加互斥锁\n分布式锁：使用分布式锁，保证对于每个key同时只有一个线程去查询后端服务，其他线程没有获得分布式锁的权限，因此只需要等待即可。这种方式将高并发的压力转移到了分布式锁，因此分布式锁的考验很大\n10.3 缓存雪崩\n\n\n\n\n\n\n\n\n概念\n缓存雪崩，是指在某一个时间段，缓存集中过期失效，Redis宕机\n产生雪崩的原因之一 ,比如在写本文的时候，马.上就要到双十二零点,很快就会迎来一波抢购 ,这波商品时间比较集中的放入了缓存,假设缓存一个小时。那么到了凌晨一点钟的时候 ,这批商品的缓存就都过期了。而对这批商品的访问查询,都落到了数据库上,对于数据库而言,就会产生周期性的压力波峰。于是所有的请求都会达到存储层,存储层的调用量会暴增,造成存储层也会挂掉的情况。\n\n其实集中过期,倒不是非常致命,比较致命的缓存雪崩,是缓存服务器某个节点宕机或断网。因为自然形成的缓存雪崩,一定是在某个时间段集中创建缓存,这个时候,数据库也是可以顶住压力的。无非就是对数据库产生周期性的压力而已。而缓存服务节点的宕机，对数据库服务器造成的压力是不可预知的,很有可能瞬间就把数据库压垮。\n\n\n\n\n\n\n\n\n\n解决方案\nRedis的高可用\n这个思想的含义是,既然redis有可能挂掉,那我多增设几台redis ,这样一台挂掉之后其他的还可以继续工作,其实就是搭建的集群。\n如 双十一：停掉一些服务(保证主要的服务可用)\n限流降级\n这个解决方案的思想是,在缓存失效后,通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存,其他线程等待。\n数据预热\n数据加热的含义就是在正式部署之前,我先把可能的数据先预先访问-遍,这样部分可能大量访问的数据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key ,设置不同的过期时间,让缓存失效的时间点尽量均匀。\n","slug":"Redis","date":"2020-05-31T16:00:00.000Z","categories_index":"","tags_index":"Java","author_index":"Aurora"},{"id":"83ffeb45e95700b79554c906e5045723","title":"UML基础","content":"一面向对象基本思想总结：\n(1)从现实世界中客观存在的事物出发来建立软件系统，强调直接以问题域（现实世界）中的事物为中心来思考问题、认识问题，并根据这些事物的本质特征，把它们抽象地表示为系统中的对象，作为系统的基本构成单位。这可以使系统直接映射问题域，保持问题域中事物及其相互关系的本来面貌(象)\n(2)用对象的属性表示事物的性质；用对象的操作表示事物的行为。（属性与操作）\n(3) 对象的属性与操作结合为一体，成为一个独立的、不可分的实体，对外屏蔽其内部细节。\n(4)对事物进行分类。把具有相同属性和相同操作的对象归为一类，类是这些对象的抽象描述，每个对象是它的类的一个实例。\n(5)复杂的对象可以用简单的对象作为其构成部分。（聚合）\n(6)通过在不同程度上运用抽象的原则,可以得到较一般的类和较特殊的类。特殊类继承一般类的属性与操作，从而简化系统的构造过程及其文档。（继承）\n(7)类具有封闭性，把内部的属性和操作隐藏起来，只有公共的操作对外是可见的。 (类的封闭性)\n(8)对象之间通过消息进行通讯，以实现对象之间的动态联系。  （消息）\n(9)通过关联表达类(一组对象)之间的静态关系。\n优点：\n\n封装的含义\n\n数据抽象（对象属性和方法的结合）\n\n信息隐藏\n\n封装的目的\n\n将对象的使用者和对象的设计者分开，使用者不必知道行为实现的细节，只需用设计者提供的消息来访问该对象。\n\n把定义和实现分开，可以大大提高软件的可维护性、可修改性。\n\n\n\n\n继承\n\n继承指子类自动获得父类中定义的数据、方法和关系，并可以添加新的成员的机制。\n\n起始类称为基类、超类、父类或者泛化类，而继承类称为派生类、子类或者特化类。\n\n继承可以帮助我们借助已知和熟悉的事物理解新的事物。\n\n继承可提高软件的可复用性\n\n继承简化了对现实世界的描述，定义子类时只需专注于自己特有的属性和操作。\n\n继承具有传递性。\n\n继承关系表示类之间的层次关系。\n\n类间具有共享特征\n\n类间具有差别或新增部分\n\n类间具有层次关系\n\n\n\n\n接口\n\n是一组没有相应方法实现的操作，非常类似于仅包含抽象方法的抽象类。\n\n是对对象行为的描述，但是并不给出对象的实现和状态。（通俗的说，接口只是说明函数应该做什么，但没有定义函数如何做。）\n\n一般只包含操作而不包含属性\n\n一个类可以实现多个接口。一个接口可以被多个类实现。\n\n使用接口比使用抽象类要安全得多，因为它可以避免许多与多重继承相关的问题。\n\n如果需要几个类共享公共基类中没有的一些特性，且希望确保每个类自己实现这些特性，就应使用接口。\n\nJava和C#等新型编程语言允许类实现多个接口，但只能继承一个通用或抽象类。\n\n\n多态性的实现方式\n\n通过接口实现多态性\n\n通过继承实现多态性\n\n通过抽象类实现的多态性\n\n\n多态性的优势\n\n面向对象技术正是利用多态提供的动态行为特征，来封装变化，适应变更，以达到系统的稳定。\n\n增加了面向对象软件系统的灵活性。\n\n进一步减少了信息冗余。\n\n提高了软件的可重用性和可扩展性。\n\n\n\n\n面向对象分析的基本原则\n\n抽象原则\n分类原则\n分类是把具有相同属性和行为的对象划分为一类，用类作为这些对象的抽象描述。\n通过不同程度的抽象可以形成一般/特殊结构。\n\n\n聚合原则\n在面向对象分析中运用聚合原则将一个复杂的事物划分为几个组成部分，形成整体/部分结构\n\n\n关联原则\n通过一个事物可以联想到另外一个事物。\n在面向对象分析中运用关联原则可以在系统模型中明确表示对象之间的静态联系。实例连接\n\n\n通信原则\n对象之间只能通过消息进行通信。\n由于封装性。用消息连接表示对象之间的动态联系。\n\n\n高层模块设计的准则\n强内聚。\n弱耦合\n良好的可拓展性和灵活性\n\n\n\n二 软件建模概念\n软件建模是把软件系统中人们关心的各个部分用模型的方式表达出来，以帮助软件产品开发的相关人员对其所开发的系统的行为和结构进行有效的说明和可视化，指导软件系统的建造，和为所建造的系统进行建档。\n建模可以帮助开发者更好的了解正在开发的系统。\n\n便于可视化系统，便于开发人员交流。\n\n允许开发人员制定系统的结构(组织)或行为(动态)，并将结构和行为联系起来，更好的理解正在开发的系统。\n\n提供指导开发人员构造系统的模板。\n\n对系统的体系结构进行可视化和控制，常用来揭示简化和复用的机会。\n\n记录开发人员的决策（文档化）。\n\n构建物理实体之前先测试，在早期就能修正一些缺陷。\n\n降低软件开发复杂度，有利于管理风险。\n\n\n软件建模的特征\n\n准确性(必须正确并恰当地描述它们所表示的系统)\n\n可理解性（模型必须尽可能简单，必须易于人们的交流）\n\n一致性（不同的图和视图表达的信息不能互相冲突）\n\n可修改性（容易修改、更新、扩展和维护）\n\n\n软件建模的原则\n\n准确的原则\n分层的原则\n实际的原则\n分治的原则\n标准的原则\n\n三 UML构成总览事务结构事物它是模型的静态部分，描述概念或物理元素。\n\n类和对象\n\n\n接口\n\n\n协作\n\n\n用况、用例\n\n\n主动类、活动类\n\n\n组件构件\n\n节点\n\n\n行为事物它是模型的动态部分，代表了跨越时间和空间的行为。\n\n交互\n\n交互（interaction）：是在特定语境中，共同完成某个任务的一组对象之间交换的消息集合 。\n消息是描述交互的手段。\n消息的表示法是一条有向直线，并在上面标有操作名。\n\n\n\n状态机\n\n描述一个对象或交互在生命周期内响应事件所经历的状态序列以及它对这些事情做出的响应\n在UML模型中状态的表示法为一个圆角矩形，并在矩形内写出状态名称及其子状态\n状态机包括一系列的对象状态、事件、由事件引起的状态之间的转换以及转换发生的同时对象所执行的动作。\n\n\n\n活动\n\n\n\n分组事物用来组织模型，使模型更加的结构化。\n注释事物和代码中的注释语句一样，是用来补充描述模型的\n事务之间的关系【基础中的重点】\n关联\n\n代表结构元素之间的某种语义上的连接\n关联关系具有方向性，单向或双向\n关联可以有名称，关联两端可以标有角色和多重性\n关联关系可以用在类图、用例图、部署图等\n\n\n\n\n依赖\n\n描述一个元素对另一个元素的依附。例如一个事物的变化会影响到另一个事物。\n依赖关系具有方向性，且是单向的\n依赖关系可以应用到类图、用例图、组件图等\n\n\n\n泛化\n\n泛化关系是一种表示特殊/一般的关系。\n泛化关系具有方向性，且是单向的。\n泛化关系可以应用到类图、用例图等\n\n\n\n实现\n\n表示一个元素实现另一个元素\n常见的实现关系\n接口和实现接口的类或组件之间的关系\n用况(用例)和实现用况(用例)的协作之间的关系\n\n\n实现关系具有方向性，且是单向的。\n\n\n\n\n聚合\n\n一种特殊形式的关联关系，表示类之间的关系是整体与部分的关系。\n聚合关系具有方向性，且是单向的。\n\n\n\n组合关系，是一种强聚合关系，用实心菱形表示，描述部分依赖于整体的存在而存在\n\n\n\n\n9种UML图\n\n四、用例图 - 静态建模4.1 概述用例是一种描述系统需求的方法，使用用例的方法来描述系统需求的过程就是用例建模\n用例方法的基本思想：用例图是从用户角度来描述系统功能的，只关心系统所能提供的服务，并不需要了解系统的内部结构和设计细节。\n用例的作用：作为整个系统开发过程中的开发依据，指导和驱动其它模型 。用例驱动的软件开发。用例图多用于业务建模和需求建模。\n4.2 元素组成\n参与者：系统外部与系统直接交互的事物，也称为活动者\n\n用例：用例是外部可见的系统功能单元，是对功能需求的描述\n\n关系（参与者与参与者之间的关系，参与者与用例之间的关系，用例之间的关系）\n\n参与者与用例之间的关系：关联\n用例之间的关系：包含、扩展、泛化\n参与者之间的关系：泛化\n\n\n\n\n​    这里应注意依赖关系的箭头方向。他是由拓展用例指向基用例\n\n扩展用例本身不是完整独立的用例，无法单独执行，扩展用例的执行必须依赖于基础用例\n4.3 用例图的描述在UML中对用例的描述并没有硬性规定，但一般情况下用例描述应包括以下几个方面：\n\n用例名称：表明用例的用途，如上面示例中的“借阅图书”、“归还图书”。\n\n标识符：[可选]编号惟一标识符一个用例，如“UC200601”。\n\n参与者：[可选]与此用例相关的参与者列表。\n\n简要说明：对该用例进行说明，描述用例作用。注意语言简要，使用自然语言。\n\n前置条件\n\n\n一个条件列表。前置条件描述了用例之前系统必须满足的条件。如果条件不满足，则用例不会被执行。\n例如： 借阅图书用例前置条件：学生出示的借书证必须是合法的借书证。\n\n后置条件\n\n后置条件在用例成功完成后得到满足，它提供了系统的部分描述。用例结束后，系统处于什么状态。\n例如：借阅图书用例后置条件：借书成功，则返回该学生借阅信息；借书失败，则返回失败的原因。\n\n扩展点\n\n如果包括扩展用例，则写出扩展用例在什么情况下使用。应该在编写事件流的同时编写。\n\n基本事件流(主事件流)\n\n描述当各项工作都正常进行时用例的工作方式。事件流描述了用户和执行用例之间交互的每一步。\n事件流是将个别用例进行合适的细化任务。可以发现原始用例图遗漏的内容。\n\n其它事件流（扩展事件流，错误事件流）\n\n在变更工作方式、出现异常或发生错误的情况下所遵循的步骤。\n例题\n\n\n\n类ClassA包含类ClassB，而且可以控制类ClassB的生命周期\n关系是 组合\nA做B的参数\n关系是 依赖\nA包含B的实例\n关系是 聚合\n4.4 用例图建模过程\n识别执行者\n\n系统外 — 必须和它交互\n系统边界 — 责任边界，非物理边界\n系统边界 — 直接与系统交互\n有意义交互 — 属于目标系统的责任\n任何事物 — 人、外系统、外部因素、时间\n\n\n识别用例\n\n执行者通过系统达到某个目标\n\n有意义的目标\n\n\n用例命名：[状语] 动词 +( [定语] 宾语)\n\n易错点1：把步骤当用例\n\n\n易错点2：警惕CRUD泛滥。一个用例背后可能隐藏着许多数据操作\n\n登录问题：\n\n\n\n\n\n五、活动图 - 动态建模用例图建模系统的功能需求，活动图则指明了系统将如何实现它的用例功能\n5.1 元素组成\n活动表示成圆角矩形\n\n分有动作状态和活动状态\n\n活动状态的内部活动可以用另一个活动图来表示\n\n区分：\n\n\n\n\n两个活动的图标之间用带箭头的直线\n\n起点和终点\n\n动作流\n\n分支与合并\n\n分叉与汇合\n\n泳道\n\n对象流\n\n\n\n5.2 活动图的用途\n活动图对表示并发行为很有用，经常用于对系统的工作流程建模，即对系统的业务过程建模\n活动图从某种意义上包含了流程图的功能\n\n活动图与流程图的区别：\n\n流程图着重描述处理过程，它的主要控制结构是顺序、分支和循环，各个处理之间有严格的顺序和时间关系；而活动图描述的则是对象活动的顺序关系所遵循的规则，它着重表现的是系统的行为，而非系统的处理过程\n\n活动图能够表示并发活动的情形，而流程图做不到\n\n活动图是面向对象的，而流程图是面向过程的\n\n活动图的应用更广，可以对业务过程、工作流、数据流和复杂算法进行建模\n\n\n六、类图   -   静态建模6.1 元素组成\n关联：类之间在概念上有实例连接关系时\n\n当模型中在相同的类中有多重关联（相同类之间的多种不同关联关系），就必须用关联名或角色（关联端名）来消除歧义\n单向关联需要单向箭头，双向关联可以是双箭头或不用\n\n\n\n依赖：其中一个事物（独立事物）的改变将影响到另一个事物（依赖事物）\n\n使用(use):声明使用一个模型元素需要用到已存在的另一个模型元素，这样才能实现使用者的功能\n调用(call):声明一个类调用其他类的操作的方法\n参数(parameter):声明一个操作和它的参数之间的关系\n发送(send):声明信号发送者和信号接受者之间的关系\n实例化(instantiate):声明一个类的方法创建了另一个类的实例\n\n\n\n泛化：指一般元素和特殊元素的分类关系，描述了一种“is a kind of” 的关系，因为子类的每个实例也都是父类的实例\n\n\n面向对象设计原则—里氏替换原则（Liskov）\n在一个软件系统中，子类应该可以替换任何基类能够出现的地方，并且经过替换以后，代码还能正常工作。\n父类出现的任何地方都可以用子类代替。\n一般元素出现的地方都可以用特殊元素来代替。\n用途：来判断在设计中的继承关系是否正确。\n\n\n\n\n聚合：一种特殊类型的关联，表示整体与部分关系的关联\n\n在关联关系上体现不出整体和部分的关系，在聚合关系上一定会体现整体和部分的关系。\n\n\n\n\n属性可见性（访问权限，可访问性）\n可见性描述了该属性对其它类是否可见，以及是否可以被其它类引用。\n\n类型：\n\n① 公有（Public） “＋”\n② 私有（Private）“－”\n③ 受保护（Protected）“＃”。\n操作\n作用域作用域  是与属性和操作相关的一个重要概念。\n\n存在两种作用域：\n（1）实例 作用域，类的不同实例对象拥有自己的属性值，类似于C++中的非静态成员。\n（2）分类 作用域，类的所有实例对象共享相同的属性值，类似于C++中的静态成员。\n\nUML 分类作用域的属性和操作名字要带下划线。\n\n\n抽象类抽象类：至少拥有一个抽象操作的类。\n\n抽象操作：在指定该操作的类中并没有该操作的实现方法，而只是显示了操作签名的那种操作。\n\n在UML图中，抽象操作的显示是在操作签名后面用特性字符串**{abstract}**，或者将操作签名用斜体字体方式显示。\n\n在UML图中，抽象类的显示是在类名后面用特性字符串**{abstract}，或者将类名用斜体**字体方式显示。\n\n\n\n接口\nUML中接口通常只包含操作不包含属性。\n\n接口不能实例化为对象。\n\n一个类可以实现一个或多个接口，一个接口可以被多个类实现。\n\n如果类实现了接口，但未实现该接口中的所有操作，那么此类必须声明为抽象的。\n\n\n\n6.2 类之间的关系识别关联关系识别策略1）认识各类对象之间的静态联系\n​    从问题域与系统责任的角度出发，考虑各类对象之间是否存在着某种静态关系，并且需要在系统中加以表示。\n​    例如：在学籍管理系统中，教师和班级之间存在着任课关系，要求在学籍管理系统中表现出来，就需要在教师、班级课程之间建立关联关系。\n2）识别关联的属性和操作\n   对于考虑中的每个关联进一步分析它是否应该具有某些属性和操作，即是否存在着简单关联不能表达的信息。例如，在教师与学生的连接中，是否需要给出优先级和使用权限等属性信息和开始对话的操作？如果需要，则可以先在关联线上附加一个关联类符号，来容纳这些属性与操作。\n3）分析关联多重性\n   对每个关联，从连接线的每一端看本端的一个对象可能与另一端的几个对象发生连接，把结果标注到连线的另一端。\n识别聚合关系策略\n物理上为整体的事物和它的部分。如汽车与发动机、人体与器官等。\n\n组织机构或与它的下级组织部门。例如，学校下设若干个系、教务处、图书馆等部门。\n\n团体（组织）与成员。例如，班级与学生、月计划表与日计划、公司与职员等。\n\n空间上的包容关系。例如，教室与桌椅、生产车间与机器设备、公共汽车与乘客。\n\n抽象事物的整体与部分。例如，学科与分支学科、法律与法律律条文、工程方案与方案细则等。\n\n具体事物和它的某个抽象方面。例如，可把人员的基本情况用对象“人员”来描述，而把他的工作职责身份、工作业绩都独立出来用一个部分对象来表示，并与对象“人员”构成聚合关系\n\n在材料上的组成关系。例如，汽车由钢、塑料和玻璃组成。\n\n\n识别泛化关系策略\n理解问题域的分类学知识\n\n问题域现行的分类方法比较正确地反映了事物的特征及各种概念的一般性与特殊性。学习理解这些知识将对认识对象及其特征、定义类、建立类之间的继承关系有很大的帮助。\n\n依据常识考虑事物的分类\n\n如果问题域没有可供参考的分类方法，可以按照一般常识从各种不同的角度考虑事物的分类从而发现继承关系。\n\n考虑类之间的语义关系\n\n如果类A与类B之间有着is a关系，那么类B所有的属性和操作是类A的属性和操作，则应考虑建立泛化关系。\n\n考察类的属性和操作\n\n从一个类中划分出一些特殊类。看一个类的属性与操作是否适合这个类的全部对象如果某些属性或操作只能适合该类的部分对象，则说明应该从这个类中划分出一些特殊类，建立泛化关系。例如：在“公司人员”类中有“股份”、“工资”两个属性，通过分析可以发现，“股份”属性只适合于公司的股东，而“工资”属性则适合于公司的职员。应在“公司人员”类下建立“股东”与“职员”两个特殊类\n识别依赖关系策略\n优先考虑关联关系和泛化关系\n\n通常，在描述语义上相互有联系的类之间的关系时，首先考虑是否存在着泛化方面的关系或结构（关联）方面的关系，并分别用对应的泛化关系或关联关系及其修饰形态进行描述。当类之间不宜于用这两种关系描述时，再考虑用依赖关系。\n\n考察类的改变\n\n如果两个类之间存在语义上的连接，其中一个类是独立的，另一个类不是独立的；并且，独立类改变了，将影响另一个不独立的类，则建立它们之间的依赖关系。\n具体而言，一个类向另一个类发送消息；一个类是另一个类的数据成员；一个类是另一个类的某个操作参数等。\n\n考察多重性\n\n虽然说，如果类A和类B之间有关联关系，那么类A和类B之间也就有依赖关系。但是在关联关系中通常都会出现多重性，即使是一对一的多重性：但在依赖关系中定不会出现多重性。\n七、顺序图 - 动态建模是一种交互图，主要描述对象之间的动态合作关系以及合作过程中的行为次序，常用来描述一个用例的行为\n\n顺序图 是一种详细表示对象之间动态交互的图形文档\n\n顺序图将交互关系表示为一 个二维图。\n\n横轴代表了在协作中各独立 的对象。\n\n纵轴是时间轴，时间沿竖线 向下延伸。\n\n沿时间方向按时间递增顺序 列出个对象所发出和接收的消息。\n\n\n7.1 元素组成\n对象\n\n命名方式：\n\n包括对象名和类名\n类名 （匿名对象）\n对象名（不关心类）\n\n\n\n对象间的排列顺序并不重要，但一般把表示参与者的对象 放在图的两侧\n\n将对象置于顺序图的顶部意味着在交互开始的时候对象就已经存在了，如果对象的位置不在顶部，那么表示对象是在交互的过程中被创建的\n\n\n对象创建和销毁\n\n\n\n\n\n\n生命线 \n\n每个对象都有自己的生命线，生命线在顺序图中表示为从对象 图标向下延伸的一条虚线，表示对象在特定时间内的存在\n\n\n\n\n消息 \n\n消息可以用于在对象间传递参数\n\n消息可以是信号（对象间的异步通信），也可以是调用（具有返回控制机制的同步调用）。\n\n消息是两个对象之间的单路通信，从源对象指向目标对象，以触发目标对象中的特定操作\n\n三种消息：\n\n调用消息：调用消息的发送者把控制传递给消息的接收者，然后停止 活动，等待消息接收者执行其某种操作后返回控制。由于发送者等待接收者，这种消息又叫做同步消息\n\n\n异步消息：消息发送后，发送者继续操作，不等待，常用于并发。\n\n\n返回消息：表示消息的返回。一般同步（过程调用）的返回不需要画出，直接隐含，而异步返回则可用它。如果异步消息有返回消息，必须明确表示出来\n\n\n\n\n反身消息：一个对象将一个消息发送给它本身，自反消息一般是同步消息\n\n\n阻止消息：消息发送者发送消息给接收者，如果接收 者无法立即接收消息，则发送者放弃这个消息\n\n\n超时消息：消息发送者发出消息给接收者并按指定时间等待。如果接收者无法在指定时间内接收消息，则发送者放弃这个消息\n\n\n消息序号：\n\n整个消息的传递过程形成了一个完整的序列，因此可以通过在每个消息的前面加上一个用冒号隔开的顺序号来表示其顺序。消息序号可采用两种方式：\n\n无层次的顺序编号\n\n嵌套编号\n\n\n\n\n\n\n\n\n\n激活 **/**（控制焦点） / 控制条\n\n激活表示该对象被占用以完成某个任务，对象执行某 个动作的时期\n\n为了表示对象是激活的，可以将该对象的 生命线拓宽成为矩形\n\n\n激活表示一个对象直接或者通过从属例程执行一个行为的时期，既可以表示行为执行的持续时间，也表示了活动和它的调用者之间的控制关系\n\n\n\n\n​    \n7.2 顺序图建模步骤\n 确定需要建模的用例:根据一个用例的用例描述，找出基本事件流和可选事件流。\n确定用例的工作流:分析事件流的先后次序。事件流的先后顺序决定了消息的发送次序。一个消息导致接收消息的对象执行一个动作。\n确定各工作流涉及的对象:根据结构模型中确定的类，确定它在用例描述中的职责。一个职责对应该对象执行一个动作。并在顺序图中按从左到右顺序进行布置\n在顺序图中添加消息和条件以便创建每个工作流\n补充说明:a. 必要的话，表达出需要创建的对象和需要撤销的对象。b. 对于循环发送或分支发送的消息，使用消息顺序项进行表达。或者，使用交互架构表达。c. 在系统分析阶段，可以只使用一般意义下的类。在系统设计阶段，可以细分出边界类、控制类、实体类。\n\n八、协作图(通信图) - 动态建模用于描述相互合作的对象间的交互关系，它描述的交互关系是对象间的消息连接关系。\n协作图（通信图）强调的是空间\n8.1 元素组成\n对象：同顺序图概念  但在协作图中，对于对象在图中的位置没有限制\n\n链 :表示对象之间的语义连接\n\n消息:\n\n消息是附在连接发送对象和接收对象的链上，使用带有标签的箭头线来表示\n\n一个链上可以附有多个消息\n\n语法格式：[前缀] [监护条件] [消息顺序项表达式] [返回值:=] 消息名（[参数列表]）\n\n\n\n\n返回消息：返回值的名字在最左，后 跟赋值号“:=”，接着是操作名和操作的参数\n\n\n\n\n8.2 协作图建模步骤\n同顺序图的建模\n\n或者：可以从顺序图直接变换过来\n\n或者：根据类图，画出对应的对象图。在链上附着消息。**\n\n在系统分析阶段，可以只使用一般意义下的类。在系统设计阶段，可以细分出边界类、控制类、实体类\n\n\n8.3 顺序图与协作图的比较联系\n都用于描述系统中对象之间的交互协作完成一项功能\n\n彼此可以相互转换\n\n\n区别\n顺序图强调的是消息的时间顺序；协作图强调的是对象的空间位置关系\n\n顺序图中有对象生命线和控制焦点；协作图中有路径，消息必须要有消息顺序号\n\n顺序图可以表示生命线的分叉；协作图可以表示多对象、主动对象\n\n\n8.4 协作图样例\n九、状态图 - 动态建模状态图用来描述对象，子系统，系统的生命周期。\n顺序图和状态图是系统动态行为的两个互补的视图\n顺序图显示了在较短的一段时间(通常是在单个用户产生的事务期间),在系统中的对象之间传递的消息,因此顺序图必须描述很多对象，即事务中所涉及的那些对象。\n状态图自始至终在一个单一对象的整个生存期中跟踪该对象，指定该对象能够接收的所有可能的消息序列,以及它对这些消息的响应。\n9.1 元素组成\n状态 ：表示一个对象在其生存期内的状况。 \n初始状态、起始状态\n中间状态组成：\n状态名\n入口动作、出口动作（entry/exit action）\n内部活动(do)：不导致当前状态发生改变的活动或动作。在一个状态中允许有多个动作\n内部事件(event):不导致当前状态发生改变的事件，会触发相应的内部动作\n延迟事件(event .. defer) : 延迟到下一状态中处理的事件\n\n\n\n\n\n\n转移（转换，迁移）：表示不同状态之间的联系。\n\n事件：事件触发状态转移。例如，一个信号、一个操作的调用、一个对象的创建或销毁、超时、某个条件的改变等。 \n\n动作：执行的行为，原子计算。（动作可以与状态相关， 也可以与转移相关）\n\n活动：一系列动作\n\n\n9.2 状态图的过程描述十、组件图 - 物理实现图组件图可以描述软件的各个组件以及它们之间的关系\n","slug":"UML","date":"2020-04-25T16:00:00.000Z","categories_index":"","tags_index":"软件工程","author_index":"Aurora"},{"id":"36e20984c48e8dcfc2b9d50a9dcddf36","title":"SQL基础","content":"零、简介重点内容已经标记，本笔记侧重操作，理论较少，仅进行简单的理论介绍，以达到了解的目的，本笔记旨在SQL的设计和操规范化进行讲解，《总结》和《设计》部分依据了Java后端开发工程师行业规约整理和总结而成，详细内容可查看《阿里巴巴Java开发手册》(华山版/泰山版/嵩山版)\nBy SkyFroop\n有道无术，术尚可求；有术无道，止于术\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n一、数据库分类1.1 关系型数据库(SQL)\nMySQL、Oracle、Sql Server、DB2、SQLlite\n通过表和表之间，行和列之间的关系进行数据的存储。\n\n1.2 非关系型数据库(NoSQL:Not Only SQL)\nRedis、MongDB\n非关系型数据库，对象存储，通过对象的自身的属性来决定\n\n1.3 DBMS(数据库管理系统)\n数据库的管理软件，科学有效的管理我们的数据。\n维护获取数据\nMySQL本身就是一种数据库管理系统，可以管理和操作数据(DB)\n\n1.4 数据库语言分类\nDDL   数据库定义语言\nDML  数据库操作语言\nDQL   数据库查询语言\nDCL   数据库控制语言\n\n二、DDL+DCL基本语句不区分大小写！如果某个字段名与关键字冲突，可以加两个 撇: ``\n2.1 数据库的列类型\n\n\n\n\n\n\n\n\n数值\n\n整数:\n\ntinyint 十分小的数据     1个字节\nsmallint 较小的数据      2个字节\nint    标准的证书            4个字节【常用】\nbigint 较大的数据         8个字节\n\n\n浮点数:\n\nfloat  单精度浮点数      4个字节\ndouble 双精度浮点数   8个字节\ndecimal  字符串形式的浮点数(精度问题！) 常用于金融计算\n\n\n\n注意：这里面的定义时后面的括号是显示字宽，不影响实际存储大小\n\n\n\n\n\n\n\n\n\n字符串\n\nchar  字符串固定大小 0–255\nvarchar  可变字符串   0–65535 【最常用】\ntinytext  微型文本   2^8-1\ntext  文本串           2^16 - 1 保存大文本\n\n这里varchar()后面的数字可以代表长度\n\n\n\n\n\n\n\n\n\n时间日期\n\ndate     YYYY-MM-DD    日期格式\ntime     HH:MM:SS        时间格式\ndatetime    YYYY-MM-DD  HH:MM:SS  【最常用】\ntimestamp  时间戳   1970.1.1到现在的毫秒数【较常用】\nyear    年份\n\n\n\n\n\n\n\n\n\n\nnull\n\n没有值\n注意：不能使用NULL进行运算\n\n2.2 数据库的字段属性(重点！)\nUnsigned : \n无符号的整数\n声明了该列不能声明为负数\n\n\nzerofill：\n0填充的\n不足的位数使用0来填充\n如int(3) –&gt;  5 - 005\n\n\n自增：\n通常理解为自增，自动在上一条的基础上+1(默认+1)\n通常用来设计唯一的主键，并且必须是整数类型\n可以自定义设计主键自增的步长\n\n\n非空：\n如果设置为Not NULL，如果不给赋值，就会报错\n如果设置为NULL，如果不赋值，默认为NULL\n\n\n默认：\n设置默认的值\n\n\n\nalibaba规范，每一个表都必须包含以下五个字段：\nid  主键\nversion  乐观锁\nis_delete  伪删除\ngmt_create  创建时间\ngmt_update  修改数据\n2.3 MySQL基本语句\n登录：mysql -u 用户名 -p 密码\n\n查看所有的数据库：show databases;\n\n切换数据库：use 数据库名;\n\n查看所有的表：show tables;\n\n显示数据库中所有的表的信息：describe student;\n\n单行注释:  –  多行注释：/**/\n\n\n偷懒使用(可以用可视化工具创建后用此语句获取语句)：\n\n查看创建数据库的语句 : Show Create Database 数据库名;\n查看数据表的定义语句: Show Create Table 表名;\nDESC 表名：显示表的结构\n\n2.4 数据库操作基本语句2.4.1 创建数据库普通创建\ncreate database 数据库名;\n\n防止数据库存在时创建\nCREATE DATABASE IF NOT EXISTS 数据库名\n\n2.4.2 删除数据库普通删除\nDrop Database 数据库名;\n\n防止不存在删除\nDrop Database If Exists 数据库名;\n\n\n\n2.4.3 创建表所有语句后面加逗号，最后一句不需要\nCREATE Table if not exits 表名(\n\t字段名1 列类型 [属性] [索引] [注释] ,\n    字段名2 列类型 [属性] [索引] [注释] , \n\t... ,\n    id int(4) NOT NULL AUTO_OMCREMENT COMMENT '学号',\n    name varchar(20) NOT NULL DEFAULT '匿名' COMMENT '姓名',\n\tPRIMARY KEY(id)\n)[表类型] [字符集设置] [注释];\n\n表中的关键字解释：\n\nDEFAULT：默认值\nNOT NULL：非空\nPRIMARY KEY(字段名) :设置该列为主键\n[表类型] [字符集设置] [注释]:常用的引擎和字符集 ENGINE=INNODB DEFAULT CHARSET=utf8\nCOMMENT：字段名的注释\nAUTO_OMCREMENT : 主键自增\n\n2.4.4 修改表修改表名：\nAlter Table 表名 Rename As 新表名;\n\n增加字段：\nAlter Table 表名 ADD 字段名 列类型 [属性] [索引] [注释];\n\n修改表的字段：\n--修改约束：\nAlter Table 表名 MODIFY 字段名 列类型 [属性] [索引] [注释];\n\n--重命名：\nAlter Table 表名 Change 字段名 新字段名;\n\n删除表的字段：\nAlter Table 表名 Drop 字段名;\n\n\n\n2.4.5 删除表Drop Table If Exists 表名\n\n\n\n2.5 数据库引擎\nINNODB：默认使用：\n\nMyISAM：早期版本使用的\n\n\n\n\n\n\nMYISAM\nINNODB\n\n\n\n事务支持\n不支持\n支持\n\n\n数据行锁定\n不支持\n支持\n\n\n外键约束\n不支持\n支持\n\n\n全文索引\n支持\n不支持\n\n\n表空间的大小\n较小\n较大\n\n\n常规的使用操作：\n\nMYISAM 好处：节约空间，速度较快\nINNODB 好处：安全性高，事务的处理，多表多用户操作\n\n在物理空间存在的位置：\n所有的数据库文件都存在data目录下，一个文件夹就对应一个数据库，本质还是文件存储\nMySQL引擎在物理文件上的区别：\n\nINNOD在数据库表中，只有一个*.frm文件 以及上级目录下的ibdata1文件\nMYISAM对应的文件\n*.frm      表结构的定义文件\n*.MYD    数据文件(data)\n*.MYI     存放索引(index)\n\n\n\n设置数据库表的字符集编码：\n如果不设置，默认字符集不支持中文，那么会出现乱码问题，因此一般设置为utf8编码：CHARSET=utf8\n或者在my.ini中配置默认的编码\n三、MySQL数据管理(DML)【含重点】3.1 外键【物理外键不建议使用】删除有外键关系的表的时候，必须要先删除引用别人的表(从表)，再删除被引用的表(主表)\n\n\n\n\n\n\n\n\n\n外键设置方法1：\nCREATE Table if not exits 表名(\n\t字段名1 列类型 [属性] [索引] [注释] ,\n    字段名2 列类型 [属性] [索引] [注释] , \n\tPRIMARY KEY(id),\n    KEY FK_外键字段名 (外键字段名),\n    CONSTRAINT FK_外键字段名 FOREIGN KEY (外键字段名) REFERENCES 另一个表(另一个表的字段名)\n)[表类型] [字符集设置] [注释];\n\n关键词解释：\n\nKEY FK_ 外键字段名中的 FK_ 是固定的\nFOREIGN KEY：为本表的某字段设置为外键链接其他表\nREFERENCES：引用，主要涉及外表内容\nCONSTRAINT 约束\n\n\n\n\n\n\n\n\n\n\n外键设置方法2【建议】：\n利用修改表\nAlter Table 表名 \nADD CONSTRAINT FK_外键字段名 FOREIGN KEY (外键字段名) REFERENCES 另一个表(另一个表的字段名);\n\n\n\n\n\n以上操作都是数据库的物理外键，数据库级别的外键，不建议使用！(避免数据库过多造成困扰)\n最佳实现：\n\n数据库就是单纯的表，只用来存数据，只有行(数据)和列(字段)\n如果想要使用多张表的数据，利用程序在应用层实现\n\n3.2 DML语言基本语句【重点】！3.2.1 添加Insert Into 表名 (字段1,字段2, ...) Values (值1,值2, ...)\n\n如果插表内所有字段可简化为:\nInsert Into 表名 Values (值1,值2, ...)\n\n​        此时字段与值一一对应，一个也不能少和对应错误\n插入多个字段:\nInsert Into 表名 (字段1,字段2, ...) Values (值1,值2, ...),(值1,值2, ...)[...]\n\n\n\n\n\n3.2.2 修改Update 表名 SET 字段名=value,... Where [条件]\n\n如果不指定条件，会改动所有的表，最好不要这样\nvalue不一定是一个具体的值，也可以是变量名\n3.2.3 删除Delete From 表名 Where [条件]\n\n危险操作：不写条件，就直接删库了\n\n\n\n\n\n\n\n\n\nTRUNCATE命令：\nTRUNCATE Table 表名\n\n\n\n作用：完全清空一个数据库表，效果与Delete不写条件类似，且不会改变表的结构和索引约束\n\n\n\n\n\n\n\n\n\nTRUNCATE和Delete From 表名比较\n\n相同点：都能删除数据，都不会删除表结构\n不同点：\nTRUNCATE 重新设置 自增列，计数器会归零\nTRUNCATE 不会影响事务\n\n\n\n拓展：DELETE删除的问题，重启数据库现象\n\nINNODB 自增列从1开始，(存在于内存中，断电即失)\nMyISAM 继续从上一个子增量开始\n\n3.2.4 查询见 四\n四、DQL查询数据【重中之重！】\n所有查询操作都依靠DQL ：Select\n简单的查询以及复杂的查询都能实现\n数据库中最核心的语言，使用频率最高\n\n查询某表全部字段：\nSelect * from 表名;\n\n\n\nSE LECT语法模板：\nSELECT [ALL|DISTINCT] --是否需要去重\n&#123;* | table.*|[table.field1[as alias1]][,...]&#125;\nFROM table_name [as table_alias]\n\t[left | right | inner join table_name2] --联合查询\n\t[WHERE ...]\t--指定结果需满足的条件\n\t[GROUP BY ...] --指定结果按照那几个字段来分组\n\t[HAVING]\t--过滤分组的记录必须满足的次要条件\n\t[ORDER BY ...]\t--指定查询记录按照一个或多个条件排序\n\t[LIMIT &#123;[offset,]row_count | row_countOFFSET offset&#125;];\t--指定查询的记录从哪条到哪条\n\nTips: []表示可选语句,{}表示必有语句\n4.1 Where 条件子语句作用:检索数据中符合条件的值\n\n\n\n\n\n\n\n\n\n逻辑运算符\n\n\n\n操作符\n含义\n范围\n结果\n\n\n\n=\n等于\n5=6\nfalse\n\n\n&lt;&gt;或!=\n不等于\n5&lt;&gt;6\ntrue\n\n\n&gt;\n大于\n\n\n\n\n&lt;\n小于\n\n\n\n\n&gt;=\n大于等于\n\n\n\n\n&lt;=\n小于等于\n\n\n\n\nAND\n和，&amp;&amp;\n5&gt;1 and 1&gt;2\nfalse\n\n\nOR\n或，||\n5&gt;1 or 1&gt;2\ntrue\n\n\nNot\n非\n\n\n\n\n比较运算符：\n\n\n\n运算符\n语法\n描述\n\n\n\nIS NULL\na IS NULL\n如果操作符为NULL，则结果为真\n\n\nIS NOT NULL\na IS NOT NULL\n如果操作符为not NULL，则结果为真\n\n\nLike\na like b\nSQL匹配，如果a匹配b，则结果为真\n\n\nBETWEEN … AND …\nBETWEEN a AND b\n在a和b之间,包括首尾\n\n\nIN\na IN (a1,a2…)\n假设a在指定集合里面，则结果为真\n\n\n4.2 查询指定字段Select * from 表名 Where [条件];\n\n\n\n别名应用(As)：Select 列名 As 列别名 From 表名 As 表别名 Where [条件];\n\n\n\n常用函数：\n字符串\n\nSELECT CONCAT('值1','值2'); -- 连接字符串\nSELECT CHAR_LENGTH(); -- 字符串长度\nSELECT LOWER(); -- 转小写\nSELECT UPPER(); -- 转大写\nSELECT INSTR('字符串','子串'); -- 返回第一次出现的子串的索引\nSELECT REPLACE('字符串','指定串','新串'); -- 替换出现的指定字符串\nSELECT SUBSTR('字符串',开始位置,结束位置); -- 返回指定的子字符串\nSELECT REVERSE(); -- 字符串反转\nLOCATE(substr,str); -- 字符串str第一次出现字串substr的位置，可以利用LOCATE() = 0判断是否存在子串\n\n\n数学运算\n\nSELECT ABS(); -- 绝对值\nSELECT CEILING(); -- 向上取整\nSELECT FLOOR();-- 向下取整\nSELECT RAND(); -- 随机数\nSELECT SIGN(); -- 判断一个数的符号，负数返回-1，正数返回1，0就是0\nTRUNCATE(X,D) /* MySQL自带的一个系统函数，X是数值，D是保留小数的位数。其作用就是按照小数位数，进行数值截取（此处的截取是按保留位数直接进行截取，没有四舍五入）规则如下：\n1）当 D 大于0，是对数值 X 的小数位数进行操作；\n2）当 D 等于0，是将数值 X 的小数部分去除，只保留整数部分；\n3）当 D 小于0，是将数值 X 的小数部分去除，并将整数部分按照 D 指定位数，用 0 替换*/\n\n\n\n\n\n时间和日期\n\nSELECT CURRENT_DATE(); -- 获取当前时间\nSELECT CURDATE(); -- 获取当前时间\nSELECT NOW(); -- 获取当前时间\nSELECT DATEDIFF(开始时间，结束时间); -- 两段时间相差多少天\nSELECT LOCALTIME(); -- 获取本地时间\nSELECT SYSDATE(); -- 获取系统时间\nSELECT DAY(LAST_DAY(NOW())); -- 获取本月的天数\nDATE_FORMAT(date,format); -- 函数用于以不同的格式显示日期/时间数据 格式缩写表：https://www.w3school.com.cn/sql/func_date_format.asp\n\n\n\n\n系统\n\nSELECT SYSTEM_USER();\nSELECT USER();\nSELECT VERSION();\n\n\n\n\n\n聚合函数【常用】\n\n\n函数名称\n描述\n\n\n\nCOUNT()\n计数\n\n\nSUM()\n求和\n\n\nAVG()\n平均值\n\n\nMAX()\n最大值\n\n\nMIN()\n最小值\n\n\n…\n…\n\n\nSELECT COUNT(指定字段) FROM 表名; -- 会忽略所有的NULL值\nSELECT COUNT(*) FROM 表名; -- 不会忽略NULL值\nSELECT COUNT(1) FROM 表名; -- 不会忽略NULL值\n-- 列名为主键时，指定列名更快，非主键时count(1)更快\n-- COUNT(1)与COUNT(*)本质相同\n\n\n\n\n\n去重操作关键词：distinct\n发现重复数据可以去重\nSelect DISTINCT * From 表名\n\n\n\n数据库的列(表达式)部分：\nSelect VERSION() --查询系统版本\nSelect 100*3-1 As 计算结果 -- 用于计算\nSelect @@auto_increment_increment --查询自增的步长\n\n数据库中的表达式：文本值，列，Null，函数，计算表达式，系统变量\nSelect 表达式 From 表\n\n\n\n\n\n4.3 模糊查询（LIKE）本质是比较运算符\nLike %代表0到任意个字符 _代表一个字符\n-- 查询多个字的\nSelect * From 表名 Where 字段名 Like '%(某字符)%'\n-- 查询单个字符\nSelect * From 表名 Where 字段名 Like '某字符_'\n\n\n\n在多条数据中查有无，用IN\n4.4 联表查询（JOIN） \n\n\n\n\n\n\n\n\n\n\nJoin对比\n\n七大联表查询：\n\n\n\n\n操作\n描述\n\n\n\nleft join\n如果表中至少有一个匹配，就返回行(交叉条件)\n\n\ninner join\n会从左表中返回所有的值，即使右表没有匹配\n\n\nright join\n会从右表中返回所有的值，即使左表没有匹配\n\n\n思路\n分析需求：，分析查询的字段来自哪些表(连接查询)\n确定使用那种连接查询(共七种)\n确定交叉点(这两个表中哪个数据时相同的)\n如果存在多张表，先查询两张表再慢慢增加\n\n举例两张表：\n表student:\n​    含字段：studentNO,studentName\n表result:\n​    含字段：studentNO,StudentResult\nSelect studentNO,studentName,SubjectNo,StudentResult\nFrom student As s\nINNER JOIN result As r\nON s.studentNO = r.studetNO\n\n注意: 这里连接语法要用ON(连接查询中判断的条件)，Where是等值查询。两者作用一样，但这里受语法限制\n多张表：\n表student:\n​    含字段：studentNO,studentName\n表result:\n​    含字段：StudentResult，studentNO,SubjectNo\n表subject:\n​    含字段：SubjectNo,SubjectName,studentNO\nSelect studentNO,studentName,SubjectNo,StudentResult\nFrom student  s\t--这里也是别名的一种方式\nRight JOIN result  r\nON s.studentNO = r.studetNO\nInner Join subject sub\nON r.SubjectNo = sub.SubjectNo\n\n\n\n4.5 自连接查询【了解】自己的表和自己的表连接\n核心：一张表拆为两张一样的表\n查询父子信息：\nSelect 当前字段名 as 父别名,当前字段名 As 子别名\nFrom 当前表名 As 父表别名,当前表名 As 子表别名\nWhere 父表别名.id = 子表别名.pid\n\n\n\n4.6 分页(Limit)和排序(Order By)4.6.1 排序\n升序：ASC\n\n降序：DESC\n\n\nSelect * From 表名\nOrder By 字段名 &#123;ASC|DESC&#125;\n\n\n\n4.6.2 分页作用：缓解数据库压力，给人更好的体验\nSelect * From 表名\nLimit 起始值,页面大小\n\n举例:\nLimit 0,5  : 1~5条\nLimit 1,5 ：2~6条\nLimit 5,5 : 第二页\n4.6.3 应用举例题目：查询 Java 第一学年，课程乘绩排名前十的学生，并且分数大于80的学生信息(学号，姓名，课程名称，分数)\n数据库结构：\n​    表student:\n​        含字段：StudentNo,StudentName\n​    表result:\n​        含字段：StudentResult，StudentNo,SubjectNo\n​    表subject:\n​        含字段：SubjectNo,StudentNo,SubjectName\nSelect s.StudentNo,StudentName,SubjectName,StudentResult\nFrom sutdent s\nINNER JOIN result r\nON s.StudentNo = r.StudentNo\nINNER JOIN subject sub\nON sub.SubjectNo = r.SubjuctNo\nWHERE SubjectName = '第一学年' AND StudentResult>=80\nORDER BY StudentResult DESC --降序\nLIMIT 0,10\n\n\n\n4.7 子查询 【难点|重点】本质：在Where语句中嵌套一个子查询语句\n此处截取SQL语句与需求，不提供数据库\n子查询包括：Where嵌套查询 和 联表查询\n  联表查询见4.4\n这里是嵌套查询：\n\n用子查询找出所有没有成绩的学生的学号、姓名及院系，专业。\n\nSelect Distinct Student.Sno,Student.Sname,Student.Sdept,Smajor \nFrom Student ,SC \nWhere Student.Sno not in(\n    Select Student.Sno \n    From Student,SC \n    Where Student.Sno = SC.Sno\n)\n\n\n\n\n查询课程名中不含“大学”字样的课程信息。\n\nSelect * From Course \nWhere Cname not in (\n    Select Cname \n    From Course \n    Where Cname Like '大学%'\n)\n\n\n查询平均分高于80分的学生\n\n表结构:\nCREATE TABLE `stu` (\n  `sno` char(4) NOT NULL,\n  `sname` char(8) NOT NULL,\n  `sex` tinyint(1) DEFAULT NULL,\n  `mno` char(2) DEFAULT NULL,\n  `birdate` datetime DEFAULT NULL,\n  `memo` text,\n  PRIMARY KEY (`sno`)\n);\nCREATE TABLE `sc` (\n  `sno` char(4) NOT NULL,\n  `cno` char(4) NOT NULL,\n  `grade` decimal(6,1) DEFAULT NULL,\n  PRIMARY KEY (`sno`,`cno`),\n  CONSTRAINT `fk_sc_sno` FOREIGN KEY (`sno`) REFERENCES `stu` (`sno`)\n);\n\n\n\n答案：\nSelect DisTinct sname \nFrom stu,sc \nWhere sc.sno = stu.sno and sc.sno in (\n    Select sno From sc \n    Group By sno \n    Having AVG(grade)>=80\n) \n\n\n\n\n\n4.8 分组过滤举例：\n查询不同课程的平均分，最高分，最低分，平均分大于80 (核心：根据不同的课程分析)\n​    表result:\n​        含字段：StudentResult，StudentNo,SubjectNo\n​    表subject:\n​        含字段：SubjectNo,StudentNo,SubjectName\nSELECT subjectName,AVG(StudentResult) AS 平均分,MAX(StudentResult) AS 最高分,MIN(StudentResult) AS 最低分\nFrom result r\nINNER Join subject sub\nON r.SubjectNo = sub.SubjectNo\nGroup By r.SubjectNo --通过什么字段分组\nHaving 平均分 > 80\n\n\n\n注意，分组过滤不能使用Where，要用Having\n4.9  数据库级别的MD5加密【拓展】什么是MD5?主要增强算法复杂度和不可逆性。MD5不可逆\n加密：\nUPDATE 表名 SET 加密字段=MD5(加密字段) Where [条件]\n\n\n\n4.10 合并表(UNION)、虚拟表、特殊关键字用法UNIONUNION 操作符用于合并两个或多个 SELECT 语句的结果集。\nUNION 内部的 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每条 SELECT 语句中的列的顺序必须相同。\nUNION语法：\nSELECT column_name(s) FROM table_name1\nUNION\nSELECT column_name(s) FROM table_name2\n\n默认地，UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。\nSQL UNION ALL 语法：\nSELECT column_name(s) FROM table_name1\nUNION ALL\nSELECT column_name(s) FROM table_name2\n\n\n\ndual表的用途及案例 (dual虚拟表)\n\n\n\n\n\n\n\n\nDUAL是为了方便那些要求所有SELECT语句都应该具有FROM和其他子句的人。MySQL可能会忽略该条款。如果没有引用表，MySQL不需要从DUAL。\n较少使用的关键字及其用法CUBE：CUBE 生成的结果集显示了所选列中值的所有组合的聚合。\nROLLUP：ROLLUP 生成的结果集显示了所选列中值的某一层次结构的聚合。\nGROUPING：当行由 CUBE 或 ROLLUP 运算符添加时，该函数将导致附加列的输出值为 1；当行不由 CUBE 或 ROLLUP 运算符添加时，该函数将导致附加列的输出值为 0。\n五、事务什么是事务：\n 要么都成功要么都失败\n\nSQL1 执行 A给B转账   A 1000  —-&gt; 200  B 200\nSQL 2执行 B收到钱  A 800 —–&gt; B 400\n\n将SQL放在一个批次中去执行\n5.1 事务原则ACID原则：\n**原子性(Atomicity)**：针对同一个事务，即SQL1和SQL2要么一起成功要么一起失败\n**一致性(Consistency)**：针对一个事务操作前与操作后的状态一致\n**持久性(Durability)**：表示事务结束后的数据不随外界原因导致数据丢失\n如果事务没有提交，服务器宕机或断电，那么重启数据库后数据状态恢复为原来的状态\n如果事务已经提交，服务器宕机或断电，重启后应该是状态已经改变后的状态\n\n\n**隔离性(Isolation)**：针对多个用户同时操作，主要是排除其他事务对本次事务的影响\n\n隔离导致的一部分问题：\n脏读：一个事务读取了另一个事务未提交的数据\n不可重复读：在一个事务内读取表中的某一行数据，多次读取结果不同。(这个不一定是错误，只是某些场合不对)\n虚读(幻读)：是指在一个事务内读取到了别的事务插入的数据，导致前后读取不一致。一般是导致多了一行\n\n事务的SQL实现\n5.2 隔离级别脏读”、“不可重复读”和“幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。\n\n数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上“串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。\n六、索引\n\n\n\n\n\n\n\n\nMySQL官方对索引的定义为:索引 (Index)）是帮助MySQL高效获取数据的数据结构。\n提取句子主干，就可以得到索引的本质:索引是数据结构。\n6.1 索引的分类\n主键索引 : PRIMARY KEY\n唯一的标识，主键不可重复，只有一个列可以作为主键\n\n\n唯一索引 ：UNIQUE KEY\n避免重复的列出现，唯一索引可以重复，多个列都可以标识为 唯一索引\n\n\n常规索引 ：KEY / INDEX\n默认的，INDEX 或 KEY关键字来设置\n\n\n全文索引 ：FULLTEXT\n在特定的数据库引擎下才有 : MyISAM\n快速定位数据\n\n\n\n6.2 索引的使用\n在创建表的时候给字段增加索引\n创建完毕后，增加索引\n\n-- 显示当前所有的索引信息\nSHOW INDEX FROM 表名;\n-- 增加一个索引\nALTER TABLE 表名 ADD 索引 INDEX 索引名(列名) -- 一般索引名与列名同名\n\n-- 分析SQL执行状况 :EXPLAIN\nEXPLAIN SELECT * From 表名; -- 非全文索引\nSELECT * FROM 表名 WHERE MATCH(字段名) AGAINST('查找值');-- 全文索引\n\n\n\n\n6.3 索引原则\n索引不是越多越好\n不要对进程变动数据加索引\n小数据量的表不需要加索引\n索引一般加在常用来查询的字段上\n\n\n\n\n\n\n\n\n\n\n索引的数据结构\nHash 类型的索引\nBtree INNODB的默认数据结构\n\n七、数据库的设计当数据库比较复杂的时候，我们就需要设计了\n糟糕的数据库设计:\n\n数据冗余，浪费空间\n\n数据库插入和删除都会麻烦、异常【屏蔽使用物理外键】\n\n\n良好的数据库设计:\n\n节省内存空间\n保证数据库的完整性\n方便我们开发系统\n\n软件开发中，关于数据库的设计：\n\n分析需求：分析业务和需要处理的数据库的需求\n概要设计：设计关系E-R图\n\n数据库设计步骤：\n\n收集信息，分析需求\n用户表：用户登录注销，用户的个人信息\n分类表：分类\n内容表：内容\n友链表 ： 友链信息\n自定义表：系统信息，某个关键的子或者一些主字段 key：value\n\n\n标识实体 把需求落地到每个字段\n标识实体之间的关系\n\n7.1 数据库三大范式为什么需要规范？\n\n信息重复\n更新异常\n插入异常\n无法正常显示信息\n\n\n删除异常\n丢失有效信息\n\n\n\n\n\n\n\n\n\n\n\n\n 三大范式：\n第一范式(1NF):要求数据库表的每一列都是补课分割的原子数据项。\n举例：\n\n家庭信息可以再分：人口数和地点\n学校信息可以再分学位和年级\n第二范式(2NF)：在第一范式的基础上，非码属性必须完全依赖于候选码(换言之：每张表只描述一件事) ，第二范式需要确保数据库中的每一列都和主键相关，而不能至于主键的某一部分相关(主要针对联合之间而言)\n举例：\n\n在上图所示的情况中，同一个订单中可能也含不同的产品，因此主键必须足”订单号”和“产品号”联合组成,但可以发现，产品数量、产品折扣、产品价格与”订单号”和“产品号”都相关，但是订单金瓶和订单时间仅与”订单号”相关，与产品号无关。这样就不满足第二范式的要求，需分成两个表，应修改为:\n\n第三范式(3NF)：在第二范式的基础上，任何非主属性不依赖于其他非主属性，第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关\n举例：\n\n表所有属性都完全依赖于学号，所以满足第二范式，但是”班主任性别”和”班主任年龄”直接依赖的是”班主任姓名”，而不是主键”学号”，所以需做如下调整：\n\n规范性  和  性能的问题：\n关联查询的表不得超过三张表\n\n考虑商业化的需求和目标(成本、用户体验)\n在规范性能的问题的时候，需要适当的考虑一下 规范性\n故意给某些表增加一些冗余的字段(从多表查询变为单表查询)\n故意增加一些计算列(从大数据量降低为小数据量的查询：或者增加索引)\n\n八、数据库连接池数据库连接—执行完毕—释放\n连接–释放  非常浪费系统资源\n池化技术：准备一些预先的资源，过来就连接预先准备好的\n最小连接数、最大连接数、等待超时\n编写连接池，实现一个接口：DataSource\n\n\n\n\n\n\n\n\n\n开源数据源实现\nDBCP\nC3P0\nDruid\n使用了这些数据库连接池之后，我们在项目中就不需要编写连接数据库的代码了\n九、总结数据库的难点主要在于查询\n\n常用查询\n\n涉及到多表操作时，使用联表查询(JOIN)，其中使用较多的就是INNER JOIN。同时不是必须使用，也可通过FROM多表查询\n涉及到需要分组统计的时候，使用GROUP BY XX字段\n根据多组字段排序时，ORDER BY 字段1 ASC/DESC , 字段2 ASC/DESC …\n\n\n查询的易错点\n\nHAVING和WHERE，他们两个的执行位置不同，WHERE要在GROUP BY之前，而HAVING要在其后，同时，聚合函数(如count(),AVG()等)是不能放进WHERE的，因为WHERE的执行顺序先于他们，因此会出现语法错误，而HAVING则可以根据聚合函数的值进行过滤筛选。\n\n判断是否为NULL的时候，是用 IS NULL 判断，并非 = 号。\n\n联表查询时也可能需要自联，两个相同的表进行连接，此时容易出现表明混乱，这时候所有的表明必须取一个别名，并且字段名需要使用表名的别名进行索引，其他类似情况同理。\n\n\n\n\n在设计表时，一般要遵循如下的默认规则：\n\n选择合适的数据类型。能定长尽量定长。定长会节省很多查询时间\n使用ENUM而不是Varchar，ENUM类型是非常快和紧凑的，在实际上，其保存的是TINYINT，但外表显示为字符串，用这个字段做一些选项列表变得非常完美。\n不要使用无法加索引的类型作为关键字段，比如test类型\n为了避免联表查询，有时候可以适当的数据冗余，比如邮箱、姓名这些不容易更改的数据\n选择合适的表引擎，有时候MyISAM适合，有时候InnoDB适合\n为保证查询性能，最好每个表都建立有auto_increment字段，建立合适的数据库索引\n最好给每个字段设定一个default值\n\n索引建立原则：合适的索引可以提高语句性能\n\n一般针对数据分散的关键字进行建立索引，比如ID、QQ、性别、状态值等建立索引没有意义\n\n字段唯一，最少，不可为NULL\n\n对大数据量建立聚集索引、避免更新操作带来的碎片\n\n尽量使用段索引，一般对int、char、date等类型的字段建立索引\n\n谨慎件里Unique类型的索引\n\n大文本字段不建立为索引，如果需要对大文本字段进行检索，可以考虑全文索引\n\n频繁更新的列不适合建立索引  \n\nOrder By中的字段、where子句中的字段。最常用的sql语句中的字段，应该建立索引\n\n只有建立索引以后，表内的行才能按照特定的顺序存储，按照需要可以是ASC或DESC\n\n\nSQL语句模板：\nSELECT [ALL|DISTINCT] --是否需要去重\n&#123;* | table.*|[table.field1[as alias1]][,...]&#125;\nFROM table_name [as table_alias]\n\t[left | right | inner join table_name2] --联合查询\n\t[WHERE ...]\t--指定结果需满足的条件\n\t[GROUP BY ...] --指定结果按照那几个字段来分组\n\t[HAVING]\t--过滤分组的记录必须满足的次要条件\n\t[ORDER BY ...]\t--指定查询记录按照一个或多个条件排序\n\t[LIMIT &#123;[offset,]row_count | row_countOFFSET offset&#125;];\t--指定查询的记录从哪条到哪条\n\n\n\nMySQL语句的执行顺序：\n(8) SELECT (9) DISTINCT (11)&lt;TOP_specification> &lt;Select_list>\n(1) FROM &lt;table_name> [as table_alias]\n(3)\t\t[left | right | inner join table_name2] --联合查询\n(2)\t\tON &lt;join_condition>\n(4) [WHERE ...]\t\n(5) [GROUP BY ...]\n(6) [WITH &#123;CUBE | ROLLUP&#125;]\n(7) [HAVING] \n(10)  [ORDER BY ...] \n\n\n\n使SQL语言更高效：\n\n能够快速缩小结果集的WHERE条件写在前面，如果有恒量条件，也尽量放在前面\n尽量避免使用GROUP BY、DISTINCT、OR、IN等语句，尽量避免使用联表查询和子查询\n不要在WHERE子句中的”=”左边进行算术或表达式运算，否则系统可能无法正确使用索引\n避免使用SELECT * ，只取需要的字段\n如果查入的数据量很大，用SELECT INTO 替代 INSERT INTO 能带来更好的性能\nWHERE字句中尽量不要使用CASE条件\n尽量不使用触发器，特别是大数据表上\n使用　UNION　ALL　操作代替OR操作，注意此时需要注意一点查询条件可以使用聚集索引，如果是非聚集索引将因此相反的效果\n当只有一行数据时使用　LIMIT　１\n\n数据库和Java字段对应：\n\nDate在JavaEntity中一般使用java.util.Date类型\n十、SQL模板SQL语句模板：\nSELECT [ALL|DISTINCT] --是否需要去重\n&#123;* | table.*|[table.field1[as alias1]][,...]&#125;\nFROM table_name [as table_alias]\n\t[left | right | inner join table_name2] --联合查询\n\t[WHERE ...]\t--指定结果需满足的条件\n\t[GROUP BY ...] --指定结果按照那几个字段来分组\n\t[HAVING]\t--过滤分组的记录必须满足的次要条件\n\t[ORDER BY ...]\t--指定查询记录按照一个或多个条件排序\n\t[LIMIT &#123;[offset,]row_count | row_countOFFSET offset&#125;];\t--指定查询的记录从哪条到哪条\n\n\n\nMySQL语句的执行顺序：\n(8) SELECT (9) DISTINCT (11)&lt;TOP_specification> &lt;Select_list>\n(1) FROM &lt;table_name> [as table_alias]\n(3)\t\t[left | right | inner join table_name2] --联合查询\n(2)\t\tON &lt;join_condition>\n(4) [WHERE ...]\t\n(5) [GROUP BY ...]\n(6) [WITH &#123;CUBE | ROLLUP&#125;]\n(7) [HAVING] \n(10)  [ORDER BY ...] \n\n","slug":"SQL","date":"2019-12-04T16:00:00.000Z","categories_index":"","tags_index":"DBA","author_index":"Aurora"}]